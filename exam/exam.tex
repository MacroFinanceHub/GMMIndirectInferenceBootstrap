\documentclass{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[a4paper,top=2cm]{geometry}
\usepackage{amssymb,amsmath,amsfonts}
\usepackage[english]{babel}
\usepackage[a4paper]{geometry}
\usepackage{enumitem}
\usepackage{booktabs}
%\parindent0mm
%\parskip1.5ex plus0.5ex minus0.5ex

\begin{document}

\title{GMM, Indirect Inference and Bootstrap}
\author{Exam}
\date{Winter 2015/2016}
\maketitle

\begin{itemize}
\item Answer \textbf{all} of the following three exercises in either German or English.

\item Hand in your solutions before March, 8 2016 at 12:00.

\item Please e-mail the solutions to mutschler@statistik.tu-dortmund.de

\item The solution files should contain your executable (and commented) R code and an additional documentation preferably as \texttt{pdf}, not \texttt{doc} or \texttt{docx}.

\item All students must work on their own, please also give your student ID number.

\item It is advised to regularly check the homepage in case of urgent updates.
\item If there are any questions, do not hesitate to contact Willi Mutschler.
\end{itemize}

\newpage
\section{Test for the Zipf index of city size distributions}

It is well known that the population size distribution of large cities can
be approximated by the Zipf distribution which is a special case of the
Pareto distribution with tail index $\alpha =1$. Suppose, $X_{1}\geq
X_{2}\geq \ldots \geq X_{n}$ is a descendingly ordered sample of city sizes.
In regional economics, the tail index $\alpha $ is often estimated from the
regression%
\begin{equation*}
\ln \left( i\right) =c-\alpha \ln X_{i}+u_{i}
\end{equation*}%
where $i$ is the rank of the city and $X_{i}$ its size, the intercept
parameter $c$ is of no interest.
\begin{enumerate}[label=(\alph*)]
  \item Comment briefly on the properties of OLS for this regression. Do you expect the ordinary $t$-test to work properly? [Maximum of 4 sentences]
\end{enumerate}
Ordered samples from the Zipf distribution (i.e. from the Pareto distribution with true tail
index $\alpha =1$) are generated by the command \texttt{x <- sort(exp(rexp(n)),decreasing=TRUE)} where \texttt{n} is the sample size.
\begin{enumerate}[label=(\alph*),resume]
\item Simulate $R=1000$ samples of size $n=20$. Perform an OLS regression and plot the distribution of $\hat{\alpha}$.
\item An important hypothesis is $H_{0}:\alpha =1$ against $H_{1}:\alpha
\neq 1$. Simulate $R=1000$ samples of size $n=20$ and plot the distribution of the test statistic
\begin{equation*}
T=\frac{\hat{\alpha}-1}{SE(\hat{\alpha})}
\end{equation*}
Show that it is not $t_{n-2}$-distributed even though $H_{0}$ is true. Also, provide economic intuition behind this hypothesis.

\item Find the critical values of a \textbf{parametric} LM-type bootstrap test of $H_{0}:\alpha =1$ with 1000 bootstrap repetitions. Compare this bootstrap approach to part (c).
\end{enumerate}

\newpage

\section{Finite sample properties for the Gamma model}
Let $y_t$ have an iid gamma distribution for $t=1,\dots T$ with shape parameter $\alpha_0=\{1,3,5\}$ and scale parameter $\beta_0=1$. Assume that $\beta_0$ is known.
\begin{enumerate}[label=(\alph*)]
\item Investigate the finite sample bias and standard error of the following estimators of $\alpha_0$, for sample sizes of $T=\{50,100,200\}$ with 1000 replications.
    \begin{enumerate}[label=(\roman*)]
      \item The maximum likelihood estimator.
      \item The GMM estimator based on the first moment of the gamma distribution.
      \item The GMM estimator based on the first two moments of the gamma distribution.
    \end{enumerate}
    \emph{Hints: \texttt{y <- rgamma(T,shape=a,scale=1)} generates $T$ draws from the gamma distribution with shape parameter $a$ and scale parameter 1. \texttt{lgamma(a)} computes the logarithm of the gamma function at a. Since we are only interested in the finite sample properties of one parameter, use an optimizer for one-dimensional problems, see the help of \texttt{optim}, \texttt{optimize}, \texttt{nlminb} or any other optimization package for details. Use \texttt{mean(y)} as the starting value.}\\
    To get a nice overview, try to output your results for each sample size in a table:
\begin{table}[h]
\centering
\caption{Bias and standard error given sample size T=\{50,100,200\}}
\begin{tabular}{ccccccccc}
\toprule
           & \multicolumn{2}{c}{$\hat{\alpha}_{ML}$} && \multicolumn{2}{c}{$\hat{\alpha}_{GMM1}$} && \multicolumn{2}{c}{$\hat{\alpha}_{GMM2}$} \\\cmidrule{2-3}\cmidrule{5-6}\cmidrule{8-9}
$\alpha_0$ & Bias               & S.D                && Bias                & S.D.                && Bias                & S.D                 \\\midrule
1          & x.xxxx             & x.xxxx             && x.xxxx              & x.xxxx              && x.xxxx              & x.xxxx              \\
3          & x.xxxx             & x.xxxx             && x.xxxx              & x.xxxx              && x.xxxx              & x.xxxx              \\
5          & x.xxxx             & x.xxxx             && x.xxxx              & x.xxxx              && x.xxxx              & x.xxxx              \\
\bottomrule
\end{tabular}
\end{table}\\
\textbf{Interpret your results.}
\item Compute the finite sample size and power of the t statistic $t=\frac{\hat{\alpha}-1}{sd(\hat{\alpha})}$ based on the three estimators in part (a) for sample size $T=200$ and 1000 replications. Use parameter values of $\alpha_0=\{1.00,1.05,1.10,1.15,1.20,1.25,1.30\}$.\\
    \emph{Hints: The size of the test is given for $\alpha_0=1$ and the power of the test is given for values $\alpha_0>1$. Count the number of times the absolute value of the computed t statistic is greater than its critical value of 1.96 (for nominal size 5\%) and divide this by the number of repetitions. Output this similar to Table 2.}
\begin{table}[h]
\centering
\caption{Size and power for sample size T=200}
\begin{tabular}{cccccc}
\toprule
$\alpha_0$    & $|t_{ML}|>1.96$ && $|t_{GMM1}|>1.96$ && $|t_{GMM2}|>1.96$ \\\midrule
1.00          & x.xxxx   && x.xxxx     && x.xxxx\\
1.05          & x.xxxx   && x.xxxx     && x.xxxx\\
1.10          & x.xxxx   && x.xxxx     && x.xxxx\\
1.15          & x.xxxx   && x.xxxx     && x.xxxx\\
1.20          & x.xxxx   && x.xxxx     && x.xxxx\\
1.25          & x.xxxx   && x.xxxx     && x.xxxx\\
1.30          & x.xxxx   && x.xxxx     && x.xxxx\\
\bottomrule
\end{tabular}
\end{table}\\
\textbf{Interpret your results.}
\item Suppose that the data-generating process is now an exponential distribution with scale parameter equal to $\alpha_0$, whereas the estimators in part (a) are still based on the gamma distribution. Redo part (a) for sample sizes of $T=\{200,400\}$ and discuss the effects of misspecification on the sampling properties of the three estimators.\\
    \emph{Hint: \texttt{y <- rexp(T,rate=1/a)} generates $T$ draws from the exponential distribution with scale parameter $a$.}
\end{enumerate}


\newpage

\section{Stochastic volatility}

An important area of the use of simulation estimation techniques in economics and finance is the estimation of continuous-time models, since, in general, maximum likelihood estimation is intractable for continuous-time processes. This is due to the fact, that to estimate such models it is necessary to have continuous data. However, in practice one observes data only at discrete points in time. The usual approach to resolve the difference in frequency between the model and data, is to discretize the model, thereby matching the frequency of the model to the data. The problem with this strategy is that discretizing a continuous-time model results in a misspecification of the functional form of the model, so that the parameters of the discrete model are, in general, not the parameters of the continuous-time model, resulting in biased estimators.

\begin{enumerate}[label=(\alph*)]
  \item Explain briefly how simulation estimation procedures such as \emph{indirect inference} circumvent this misspecification problem (maximum of 4 sentences).
\end{enumerate}

\noindent Consider the Black-Scholes model for the stock market. Assume that the price of a stock follows a geometric Brownian motion
\begin{align}
  dy_t &= \mu y_t dt + \sigma_t y_t dW_{t}^y\label{eq:price}
\end{align}
where $t>0$ denotes time and $dt$ is the infinitesimal change in time. Furthermore, $dy_t$ is the incremental change in the stock price $y_t$ and $\mu$ is the expected return of the stock. $W_t^{y}$ is a Wiener process such that $dW_t^{y} \overset{iid}{\sim} N(0,dt)$. Note that this model implies stochastic volatility, since $\sigma_t>0$ is dependent on $t$. Assume that $ln (\sigma_t^2)$ follows a mean-reverting Ornstein-Uhlenbeck process
\begin{align}
  dln(\sigma_t^2) = \alpha(\kappa -ln(\sigma_t^2))dt + \gamma dW_t^\sigma\label{eq:volat}
\end{align}
with $dln(\sigma_t^2)$ denoting the change in log volatility, $\kappa$ its expected value, $\alpha>0$ a sensitivity parameter and $\gamma>0$ a diffusion parameter. $W_t^\sigma$ is a Wiener process, such that $dW_t^{\sigma} \overset{iid}{\sim} N(0,dt)$. Furthermore, $dW_t^y$ and $dW_t^\sigma$ are assumed to be independent.

We would like to estimate the parameters $\theta=(\mu,\alpha,\kappa,\gamma)$ of this continuous-time model with stochastic volatility. Note that $ln(y_t)$ is observed discretely, whereas the volatility series $ln(\sigma_t^2)$ is latent, i.e. unobservable.

Equations \eqref{eq:price} and \eqref{eq:volat} can be exactly discretized ($dt\approx \Delta t$ is a small number) with
    \begin{align}
      ln(y_t) &= a_y + b_y ln(y_{t-1}) +  c_y u^y_t\label{eq:discy}\\
      ln(\sigma_t^2) &= a_\sigma + b_\sigma ln(\sigma_{t-1}^2) + c_\sigma u_t^\sigma\label{eq:discsigma}
    \end{align}
    where
    \begin{align*}
    a_y &=\left(\mu-\frac{1}{2}\sigma_t^2\right)\Delta t&
    b_y &= 1&
    c_y &= \sqrt{\sigma_t^2\Delta t}\\
    a_\sigma&=\kappa\left(1-b_\sigma\right)&
    b_\sigma&=e^{-\alpha\Delta t}&
    c_\sigma&=\gamma\sqrt{\Delta t} \sqrt{\frac{1-b_\sigma^2}{2\alpha}}
    \end{align*}
    and $u_t^y$ and $u_t^\sigma$ are independent standard normally distributed random variables.

\begin{enumerate}[resume,label=(\alph*)]
\item Write an R function that generates $T$ discrete values for $ln(y_t)$. The function should:
    \begin{enumerate}[label=(\roman*)]
    \item Simulate $n=\frac{T}{\Delta t}$ values for $ln(\sigma_t^2)$ and $ln(y_t)$ using equations $\eqref{eq:discy}$ and $\eqref{eq:discsigma}$.\\
        \emph{Hint: To speed things up you can use the \texttt{filter} function.}
    \item Select every $\frac{1}{\Delta t}$-th value of $ln(y_t)$ and output this as a vector.
    \item Set $\Delta t=0.1$ and $T=100$. Generate a sample with $\mu=1$, $\alpha=0.9$, $\kappa=0$ and $\gamma=1$. Save this as \texttt{lnytrue}. Comment on your choice of starting values.
    \end{enumerate}
\newpage
\item Continuous-time models are sometimes estimated by discretizing them in a crude way. The discretized version of \eqref{eq:price} and \eqref{eq:volat} is
    \begin{align}
        ln(y_t) - ln(y_{t-1}) &= \mu + \sigma_t u_t^y\label{eq:auxy}\\
        ln(\sigma_t^2) - ln(\sigma_{t-1}^2) &= \alpha(\kappa-ln(\sigma_{t-1}^2)) + \gamma u_t^\sigma\label{eq:auxsigma}
    \end{align}
    with $u_t^y$ and $u_t^\sigma$ are independent standard normally distributed random variables.    We need to derive indirect estimators for the parameters $\theta=(\mu,\alpha,\kappa,\gamma)$ given the auxiliary model in equations \eqref{eq:auxy} and \eqref{eq:auxsigma} that can be computed fast. Note, however, that we only have data for $y_t$, but not for $\sigma_t$. Therefore, do the following steps:
    \begin{enumerate}[label=(\roman*)]
      \item Derive an unbiased estimator $\hat{\mu}$ for $\mu$ in \eqref{eq:auxy} given data $y_t$.
      \item Define $r_t=ln(y_t)-ln(y_{t-1})-\mu$ and show that equations \eqref{eq:auxy} and \eqref{eq:auxsigma} can be manipulated to derive an expression
      \begin{align}
      ln(r_t^2)= \delta_0 + \delta_1 ln(r_{t-1}^2) + \varepsilon_t \label{eq:r}
    \end{align}
    which is independent of $\sigma_t$. Furthermore, note that $\delta_0$ and $\delta_1$ are functions of $\alpha$ and $\kappa$, whereas $\varepsilon_t$ is a function of $\gamma$ and the error terms in \eqref{eq:auxy} and \eqref{eq:auxsigma}.
    \item Derive estimators for $\delta_0$, $\delta_1$ and the standard deviation of $\varepsilon_t$, $sd(\varepsilon_t)$, in equation \eqref{eq:r}, given data $y_t$ and the unbiased estimator $\hat{\mu}$ for $\mu$ in (i), that can be computed fast. Note that this implicitly defines estimators for $\alpha$, $\kappa$ and $\gamma$.
    \end{enumerate}
\item Load the dataset you generated in (b). Estimate the parameters $\theta=(\mu,\alpha,\kappa,\gamma)$ by indirect inference with the auxiliary model \eqref{eq:auxy}, \eqref{eq:auxsigma} and \eqref{eq:r}. Your moment conditions are based on $\hat{\beta}=(\hat{\mu},\hat{\delta}_0,\hat{\delta}_1,\widehat{sd(\varepsilon_t)})$ for the true dataset and the average of $\hat{\beta}_h=(\hat{\mu}_h,\hat{\delta}_{0,h},\hat{\delta}_{1,h},\widehat{sd(\varepsilon_t)}_h)$ for $h=1,\dots,H$ simulated datasets. The indirect inference estimator is then defined as
    $$\hat{\theta} = \underset{\theta}{argmin}\left\{ \left(\hat{\beta}-\overline{\hat{\beta}_h}\right)'W \left(\hat{\beta}-\overline{\hat{\beta}_h}\right) \right\}$$
    with $\overline{\hat{\beta}_h}=\frac{1}{H}\sum_{h=1}^{H}\hat{\beta}_h$. Set $H=10$ and use the identity matrix as the weight matrix $W$.
\item Generate $R=100$ new samples each with $\mu=1$, $\alpha=0.9$, $\kappa=0$ and $\gamma=1$. Repeat step (d) for each sample and save the estimators in a matrix. Compute the mean, standard deviation and root-mean-squared-error of your estimates.
\item Comment on how your results would change if you used the optimal weight matrix instead of the identity matrix (maximum of 4 sentences).
\end{enumerate}





%\section{Macroeconomic desasters}
%
%Barro and Jin, \textquotedblleft On the Size Distribution of Macroeconomic
%Desasters\textquotedblright , \emph{Econometrica}, 79 (2011) 1567-1589,
%assume that extreme macroeconomic shocks follow a single power law or a
%double power law distribution. The password protected pdf of the article is
%downloadable from the course site. Let $0<b\leq 1$ denote the fraction of
%output contraction. Barro and Jin consider the transformed disaster size%
%\begin{equation}
%z=\frac{1}{1-b}.  \label{e1}
%\end{equation}%
%The density of the double power law is%
%\begin{equation*}
%f_{z}(x)=\left\{
%\begin{array}{ll}
%0 & \text{if }x<z_{0} \\
%Bx^{-\beta -1} & \text{if }z_{0}\leq x<\delta \\
%Ax^{-\alpha -1} & \text{if }\delta <x%
%\end{array}%
%\right.
%\end{equation*}%
%where $\alpha ,\beta ,A,B>0$ are parameters, $z_{0}$ is a known threshold,
%and $\delta >z_{0}$ is a parameter.
%
%\begin{enumerate}
%\item Show that the conditions%
%\begin{eqnarray*}
%\int_{z_{0}}^{\infty }f_{z}(x)dx &=&1 \\
%\lim_{\varepsilon \rightarrow 0}f_{z}(\delta -\varepsilon ) &=&f_{z}(\delta )
%\end{eqnarray*}%
%with $\varepsilon >0$ imply%
%\begin{eqnarray*}
%B &=&A\delta ^{\beta -\alpha } \\
%\frac{1}{A} &=&\frac{\delta ^{\beta -\alpha }}{\beta }\left( z_{0}^{-\beta
%}-\delta ^{-\beta }\right) +\frac{\delta ^{-\alpha }}{\alpha }.
%\end{eqnarray*}
%
%\item Derive the log-likelihood function of the parameters $\alpha ,\beta $
%and $\delta $ (please derive it and do not simply state the log-likelihood
%function, which would be too easy given that it is printed in the article).
%
%\item Load the dataset \texttt{gdpdesaster.csv}. It contains the GDP
%desaster data used by Barro and Jin, i.e. 157 GDP contractions $b$ of more
%than 9.5\%. Transform the desaster size according to (\ref{e1}). In R, write
%a function that computes the log-likelihood as a function of the parameters $%
%\alpha $ and $\beta $ for a given value of $\delta $. Set $z_{0}=1.105$ and $%
%\delta =1.4$, and numerically compute the maximum likelihood estimates $\hat{%
%\alpha}$ and $\hat{\beta}$ as well as the maximized value of the
%loglikelihood function. Hint: Remember that the optimization command \texttt{%
%optim} minimizes rather than maximizes the objective function.
%
%\item Define a grid $1.300,1.301,1.302,\ldots ,1.600$ for the parameter $%
%\delta $ and compute the maximized value of the loglikelihood function for
%each grid value. Plot the log-likelihood against the grid to show that the
%log-likelihood function has several local minima. Find the maximum
%likelihood estimate $\hat{\delta}$.
%
%\item Compute the asymptotic standard errors for $\hat{\alpha},\hat{\beta},%
%\hat{\delta}$. Hint: Define another function that computes the
%log-likelihood as a function of the parameter vector $(\alpha ,\beta ,\delta
%)$.\newpage
%\end{enumerate}
%
%\section{Estimation of observed Evans bubbles}
%
%Evans, \textquotedblleft Pitfalls in Testing for Explosive Bubbles in Asset
%Prices\textquotedblright , \emph{American Economic Review}, 81 (1991)
%922-930, considers periodically collapsing bubbles. Evans bubbles are
%stochastic processes described by%
%\begin{equation}
%B_{t}=\left\{
%\begin{array}{ll}
%\left( 1+r\right) B_{t-1}u_{t} & \text{\quad if }B_{t-1}<\alpha \\
%\delta +\frac{1+r}{\pi }\left( B_{t-1}-\frac{\delta }{1+r}\right) u_{t} &
%\quad \text{if }B_{t-1}\geq \alpha \text{ and }D_{t}=1 \\
%\delta u_{t} & \quad \text{if }B_{t-1}\geq \alpha \text{ and }D_{t}=0%
%\end{array}%
%\right.  \label{bubble}
%\end{equation}%
%where $r$ is the expected growth rate, $\alpha $ is a threshold value, $%
%\delta $ is the expected bubble value immediately after a crash, $\pi $ is a
%probability, $u_{t}=\exp \left( y_{t}-\sigma ^{2}/2\right) $ where $%
%y_{t}\sim N(0,\sigma ^{2})$ with $\sigma =0.05$, and $D_{t}$ is a Bernoulli
%distributed random variable with $D_{t}=1$ (bubble continues) with
%probability $\pi $ and $D_{t}=0$ (bubble bursts) with probability $1-\pi $.
%Let $0<\delta <\left( 1+r\right) \alpha $.
%
%\begin{enumerate}
%\item Show that Evans bubbles are rational in the sense that%
%\begin{equation*}
%E(B_{t}|B_{t-1})=(1+r)B_{t-1}.
%\end{equation*}%
%Hint: $E(u_{t})=1$.
%
%\item Set $\alpha =1$, $\delta =0.5$, $\pi =0.95,\sigma =0.05$, $r=0.05$ and
%simulate a path $B_{1},\ldots ,B_{5000}$ (with starting value $B_{0}=\delta $%
%). Compute and plot the cumulative mean%
%\begin{equation*}
%\bar{X}_{i}=\frac{1}{i}\sum_{j=1}^{i}B_{i}.
%\end{equation*}%
%Explain why the plot indicates that the bubble has (very) heavy tails.
%
%\item Load the dataset \texttt{evans.csv}. It contains an artificial path of
%length $T=1000$ of a bubble process (of course, the bubble is only one
%component of stock prices and cannot be observed separately in real
%applications). Suppose the parameter $r=0.05$ and the starting value $%
%B_{0}=\delta $ are known (but $\delta $ is not). In R, write a function that
%computes the log-likelihood for $(\alpha ,\delta ,\pi ,\sigma )$. Hints: If $%
%X\sim LN(\mu ,\sigma )$ then $aX\sim LN(\mu +\ln a,\sigma )$. In R, the
%density function of the $LN(\mu ,\sigma )$ distribution at point $x$ can be
%computed by \texttt{dlnorm(x,mu,sigma)}.
%
%\item Now consider one-dimensional sections of the log-likelihood function.
%Set $\alpha _{0}=1$, $\delta _{0}=0.5$, $\pi _{0}=0.95$, and $\sigma
%_{0}=0.05$. Define a grid $g=(0.80,0.81,\ldots ,1.50)$. For each grid
%element $g_{i}$ compute the log-likelihood function at $(\alpha
%_{0}g_{i},\delta _{0},\pi _{0},\sigma _{0})$ and plot it against the grid.
%Do the same for $(\alpha _{0},\delta _{0}g_{i},\pi _{0},\sigma _{0})$ and $%
%(\alpha _{0},\delta _{0},\pi _{0},\sigma _{0}g_{i})$. As to $\pi _{0}$ use
%the grid $0.900,0.901,\ldots 0.999$. Explain why standard numerical methods
%to maximize the log-likelihood function are likely to fail.
%
%\item Assume that the parameter $\alpha =0.98$ is known. Compute the maximum
%likelihood estimates $\hat{\delta}$, $\hat{\pi}$, and $\hat{\sigma}$.
%
%\item Compute the bootstrap $0.95$-confidence intervals for $\delta ,\pi ,$
%and $\sigma $. Use the percentile method and set the number of bootstrap
%replications to $B=1000$.\newpage
%\end{enumerate}
%
%\section{Small sample properties of GMM estimators}
%
%This exercise is based on the article \textquotedblleft Small-Sample Bias in
%GMM Estimation of Covariance Structures\textquotedblright\ by J.G.\ Altonji
%and L.M.\ Segal, \emph{Journal of Business and Economic Statistics}, 14
%(1996) 353-366. Altonji and Segal investigate the finite-sample properties
%of GMM estimators. They estimate a highly simplified artificial example. The
%connection to GMM estimation is not obviously visible, but exists.
%
%Let $Y_{it}$, $i=1,\ldots ,N$, $t=1,\ldots ,T$, be a panel sample. The
%number of individuals is $N=10$, the number of periods is $T=50$. The sample
%elements $Y_{it}$ are i.i.d. (both over individuals and time) with
%\begin{equation}
%Y_{it}=\frac{X_{it}-1.648721}{2.161197}  \label{lnstd}
%\end{equation}%
%where $X_{it}$ are i.i.d. lognormally distributed with parameters $\mu =0$
%and $\sigma =1$. Location and scale in (\ref{lnstd}) are chosen such that $%
%E(Y_{it})=0$ and $Var(Y_{it})=1$.\footnote{%
%The matrix of observations is generated by \texttt{y \TEXTsymbol{<}-
%matrix((rlnorm(N*TT)-1.648721)/2.161197,N,TT)} where \texttt{N=10} and
%\texttt{TT=50}. Row $i$ contains the observations of individual $i$.}
%
%The unknown parameter to be estimated is the variance $\theta $ of $Y_{it}$
%(the true value of which is, of course, 1). Altonji and Segal compare two
%GMM estimators of $\theta $. Both are based on the $N$ estimators of the $N$
%individual variances $\sigma _{i}^{2}$,
%\begin{equation*}
%\hat{\sigma}_{i}^{2}=\frac{1}{T-1}\sum_{t=1}^{T}\left( Y_{it}-\bar{Y}%
%_{i}\right) ^{2}.
%\end{equation*}%
%The $N$ variance estimators are collected in a vector $\hat{\sigma}^{2}$.
%Define a vector $\iota =(1,\ldots ,1)^{\prime }$ of length $N$. The first
%GMM estimator $\hat{\theta}_{1}$ of the total variance $\theta $ is simply
%the unweighted mean of the $N$ individual variance estimators (the GMM
%weighting matrix is the identity matrix),%
%\begin{equation*}
%\hat{\theta}_{1}=\frac{1}{N}\sum_{i=1}^{N}\hat{\sigma}_{i}^{2}=\left( \iota
%^{\prime }\iota \right) ^{-1}\iota ^{\prime }\hat{\sigma}^{2}.
%\end{equation*}%
%The second GMM estimator $\hat{\theta}_{2}$ is based on the estimated
%asymptotically optimal weight matrix,%
%\begin{equation*}
%\hat{\theta}_{2}=\left( \iota ^{\prime }\hat{\Omega}^{-1}\iota \right)
%^{-1}\iota ^{\prime }\hat{\Omega}^{-1}\hat{\sigma}^{2}.
%\end{equation*}%
%The estimated weight matrix $\hat{\Omega}$ contains the variances and
%covariances of the individual variance estimators $\hat{\sigma}_{i}^{2}$.
%The element $(i,j)$ of $\hat{\Omega}$ is (Altonji and Segal use a slightly
%different normalization)%
%\begin{equation*}
%\hat{\Omega}_{ij}=\widehat{Cov}(\hat{\sigma}_{i}^{2},\hat{\sigma}_{j}^{2})=%
%\frac{\left[ \frac{1}{T-1}\sum_{t}\left( Y_{it}-\bar{Y}_{i}\right)
%^{2}\left( Y_{jt}-\bar{Y}_{j}\right) ^{2}\right] -\hat{\sigma}_{i}^{2}\hat{%
%\sigma}_{j}^{2}}{T}.
%\end{equation*}
%
%\begin{enumerate}
%\item Write an R program to estimate the finite-sample distribution of $\hat{%
%\theta}_{1}$ and $\hat{\theta}_{2}$ by simulations and plot both
%distribution functions.
%
%\item Compute the mean squared error of $\hat{\theta}_{1}$ and $\hat{\theta}%
%_{2}$ (i.e. the mean squared deviation from the true value 1).
%
%\item Load the dataset \texttt{smallgmm.csv} into the dataframe \texttt{x}
%and convert it into a matrix using the command \texttt{y \TEXTsymbol{<}-
%matrix(x[,1],10,50)}. Compute $\hat{\theta}_{2}$ for this dataset. Use the
%nonparametric bootstrap\footnote{%
%Resampling can be done by \texttt{ystar \TEXTsymbol{<}-
%matrix(sample(y,size=500,replace=TRUE),10,50)}.} to estimate the bias and
%standard error of $\hat{\theta}_{2}$.\newpage
%\end{enumerate}
%
%\section{Estimation of the Cox-Ingersoll-Ross model}
%
%Cox, Ingersoll and Ross, \textquotedblleft A Theory of the Term Structure of
%Interest Rates\textquotedblright , \emph{Econometrica} 53 (1985) 385-407,
%suggest a continuous-time model for the short-term interest rate. The
%stochastic process is described by the stochastic differential equation%
%\begin{equation}
%dX_{t}=\left( \theta _{1}-\theta _{2}X_{t}\right) dt+\theta _{3}\sqrt{X_{t}}%
%dW_{t}  \label{sdecir}
%\end{equation}%
%where $W_{t}$ is a standard Wiener process and $X_{0}>0$.
%
%\begin{enumerate}
%\item Activate the R package \texttt{sde}. Generate and plot a single path
%on the time interval $[0,200]$ of an Cox-Ingersoll-Ross process with
%parameters $\theta _{1}=0.03$, $\theta _{2}=0.5$, and $\theta _{3}=0.08$ and
%starting value $X_{0}=0.06$ using the command \texttt{sde.sim}. Set the
%number of steps to $N=200$.
%
%\item Continuous-time models are sometimes estimated by discretizing them in
%a crude way. The discretized version of (\ref{sdecir}) is, of course,%
%\begin{equation}
%X_{t}=X_{t-1}+\left( \theta _{1}-\theta _{2}X_{t-1}\right) +\theta _{3}\sqrt{%
%X_{t-1}}\varepsilon _{t}  \label{dissde}
%\end{equation}%
%with $\varepsilon _{t}\sim N(0,1)$ and starting value $X_{0}=\theta
%_{1}/\theta _{2}$. Find estimators for the parameters $\theta _{1},\theta
%_{2},\theta _{3}$ in (\ref{dissde}) that can be computed fast (e.g. least
%squares estimators).
%
%\item Load the dataset \texttt{cirpath.csv}. The process is not observed
%continuously. The dataset only contains observations of $X_{t}$ at discrete
%time points $t=1,\ldots ,200$. Estimate the parameters $\theta _{1},\theta
%_{2}$, and $\theta _{3}$ by indirect inference with the auxiliary model (\ref%
%{dissde}). Assume that the (unobserved) starting value is $X_{0}=\theta
%_{1}/\theta _{2}$.
%\end{enumerate}

\end{document}
