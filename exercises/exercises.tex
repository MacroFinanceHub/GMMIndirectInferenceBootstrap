\documentclass{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\usepackage{answers} % Lösungen werden in Datei ans.tex gespeichert, aber nicht angezeigt. Ganz unten im Dokument kann man die Lösungen auch einbinden.
\usepackage[nosolutionfiles]{answers} %Lösungen werden direkt bei Aufgabe angezeigt
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\Newassociation{solution}{Solution}{ans}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[a4paper,bottom=1.5in,top=1.5in]{geometry}
\usepackage{hyperref}
\hypersetup{
colorlinks=true,pdfstartview={FitH},plainpages = false,linkcolor = black }
\usepackage{fancyhdr}
    \pagestyle{fancy}
    \renewcommand{\sectionmark}[1]{\markboth{#1}{}}
    \lhead{\small GMM, Indirect Inference, Bootstrap -- Exercise Book}
    \rhead{\small \leftmark}
    \rfoot{\thepage}
    \cfoot{}
\usepackage{amssymb,amsmath,amsfonts}

\parindent0mm
\parskip1.5ex plus0.5ex minus0.5ex

\begin{document}

\title{Exercise Book}
\author{Mark Trede and Willi Mutschler}
\date{Version: November 16, 2015}
\maketitle\thispagestyle{empty}
\newpage
\Opensolutionfile{ans}[ans]
\renewcommand{\contentsname}{Overview of Exercises}
\tableofcontents\newpage

\setcounter{page}{1}
\section{A short introduction to R\label{introductionR}}

\begin{solution}
\textbf{Quick overview of R}

R, also called GNU S, is a strongly functional language and environment. It provides a wide variety of statistical (linear and nonlinear modeling, classical statistical tests, time-series analysis, classification, clustering, simulation, optimization...) and graphical techniques, and is highly extensible. To get a flavor of the possibilities just try the
following commands:

\begin{itemize}
\item \texttt{example(lm)}
\item \texttt{demo(graphics)}
\item \texttt{demo(lm.glm)}
\item \texttt{library(tcltk); demo(tkcanvas)}
\item \texttt{RSiteSearch("GMM\textquotedblright)}
\end{itemize}

It is open-source and can be run under Windows and any UNIX System (MAC, Linux). It is a programming language, so we type commands (\quotedblbase we ask\textquotedblright ) and R executes them (\quotedblbase R answers\textquotedblright ).

R also has a rudimental GUI (Graphical User Interface), which can be used to organize the workspace, load packages and list and manipulate some objects. For instance look at \texttt{Verschiedenes}. There are buttons for the following functions:
\begin{itemize}
  \item \texttt{objects()} or \texttt{ls()} lists all variables and
      objects
  \item \texttt{search()} lists all packages that are currently in use
  \item \texttt{rm(x,y,z)} removes the variables x,y and z
  \item \texttt{rm(list=ls())} removes everything (be careful!)
\end{itemize}

There are better GUIs like SciViewsR, JGR or RCommander. We, however, won't use them, but will instead type everything in the command window or even better in a script editor. TinnR, Notepad++ (in combination with NPPtoR) or Eclipse (in combination with StatET) are highly recommendable. In our opinion, one of the best editors is RStudio, so we will make use of its functionality! If you need assistance installing those editors, please contact us.

There are a couple of good books and manuals for R, for instance:
\begin{itemize}
    \item Behr, Andreas (2011) - Einf\"{u}hrung in die Statistik mit R
    \item Crawley, Michael J. (2007) - The R Book
    \item Farnsworth, Grant V. (2008) - Econometrics in R
    \item Verzani, John (2005) - Using R for Introductory Statistics
    \item The R Development Core Team (2010): An Introduction to R
    \item The command RSiteSearch("whatever-you-want-to-find") helps to find functions and references on the internet.
\end{itemize}
\end{solution}


\subsection{Starting and quitting}

Start R. The window you see is the \textquotedblleft R Console\textquotedblright\ and we will call it the command window in the following. Inside the command window, compute \texttt{$1+1$}, \texttt{$2-1$}, \texttt{$3/2$}, \texttt{$2*4$} and \texttt{$2^{10}$}. Quit R using the command \texttt{q()}, without saving the workspace.

\begin{solution}
\textbf{Starting and quitting}

During the start R searches for two files in the current working directory: .RData and .Rhistory. Those files contain your workspace and your history. You can create them by saving your workspace and saving your history (simply use the menu bar: \texttt{Datei}). The basic mathematical operators are \texttt{+,-,*,/,\symbol{94}.} Please try the following code:

\begin{verbatim}
#############################
### Starting and quitting ###
#############################
1+1
2-1
3/2
2*4
2^10
q()
# In order to save your workspace, you can also use
save.image(file.choose())
#In order to save your history, i.e. the commands you've executed, use
savehistory(file.choose())
#To load your workspace and history use
load(file.choose())
loadhistory(file.choose())
#better: use the GUI!!!
\end{verbatim}

There are two very handy key shortcuts:
\begin{itemize}
    \item $\uparrow $ and $\downarrow $ scroll through the history,i.e.
        all the commands you have typed so far.
    \item Tab ($\leftrightarrows $) completes commands, functions and
        variable names
\end{itemize}
\end{solution}

\subsection{Scripts}

Restart R. In the menu, choose \textquotedblleft Datei\textquotedblright , then \textquotedblleft Neues Skript\textquotedblright . A new window opens. Type the following four lines:

\texttt{a <- 3}\newline
\texttt{b <- 4}\newline
\texttt{c <- a+b}\newline
\texttt{print(c)}
\texttt{pi}
\texttt{Pi}
\texttt{PI}

Mark the lines and press Strg+R (or Ctrl+R). Save your script under any name (preferably with the extension \texttt{.R}) on your hard disk or USB flash drive. Quit R, restart, open the script, and execute it.

\begin{solution}
\textbf{Scripts}

Scripts are a great way to organize your code, since it can be very impractical to scroll through all the commands and execute them one by one. So please use an editor!

\emph{Variables:} Variables are placeholders for any content you can imagine: numbers, vectors, matrices, text, dates, times, tables, data, etc. In order to assign a value or a content into a variable, use \texttt{x <- 5.6}. Note: \texttt{x <-5,6} produces an error. Try the following code:

\begin{verbatim}
###############
### Scripts ###
###############
a <- 3
b <- 4
c <- a+b
#there are three ways to print the contents of the variable
(c <- a+b) #Parentheses around a command prints its output
print(c)   	#The command print()
c	#Just type the name of the variable
pi
Pi
PI
\end{verbatim}

\emph{Upper and lower cases, dot and comma:} R differentiates between upper and lower cases, try: \texttt{pi}, \texttt{Pi} und \texttt{PI}. Keep that in mind when calling functions and commands, and also when naming variables. The decimal point is the dot.
\end{solution}

\subsection{Working directory}

In R, you can obtain the current working directory using the command \texttt{getwd()} (\textquotedblleft get working directory\textquotedblright ). This is the directory where R saves files, and where it looks for files to read.

\begin{enumerate}
\item Find out where your working directory is.

\item You can change the working directory using the command \texttt{setwd("x:/path")} where \texttt{x:} is the drive (e.g. hard disk) and \texttt{path} the complete path. Note that the path does not contain the backslash (\textquotedblleft \texttt{\textbackslash}\textquotedblright ) which is usually used in Windows, but the slash (\textquotedblleft \texttt{/}\textquotedblright ). Change your working directory as you like. Check if the change was successful.
\end{enumerate}

\begin{solution}
\textbf{Working directory}

To change the working directory you can also use the GUI: \texttt{(Datei -- Verzeichnis wechseln)}.

\emph{Text and strings:} You have to use quotation marks and put your text in between those. Missing quotation marks are the most common error you get. Try the following code:
\begin{verbatim}
#########################
### Working directory ###
#########################
getwd()
setwd("c:/") #important: Windows uses the backslash \, R uses the slash /
getwd()
\end{verbatim}
\end{solution}

\subsection{Help and comments}

\begin{enumerate}
\item A very important command in R is the question mark (\texttt{?}) followed by any name of a function. This way you can start the help function, giving you details about any R command. Read the help page for the command \texttt{mean}.

\item The hash sign (\texttt{\#}) is the comment sign. Everything following the comment sign is ignored (until the end of line). Insert some comments into your script and re-execute it.
\end{enumerate}

\begin{solution}
\textbf{Help and comments}

\emph{Help}
\begin{itemize}
\item It is very important to get used to the extensive help functions in R.
\item The most important one is the question mark: \texttt{?function} (example: \texttt{?mean}).
\item If you can't remember the exact name of the function, try
   \begin{itemize}
     \item The TAB-button $\leftrightarrows$.
     \item \texttt{??mean} searches for aliases, concepts or titles that include \texttt{mean}.
     \item \texttt{apropos("\emph{mean}")} lists all objects that contain \texttt{mean}.
     \item \texttt{example("\emph{mean}")} gives you an example.
     \item \texttt{find("\emph{mean}")} gives you the name of the package containing the function mean.
   \end{itemize}
\end{itemize}

\emph{Comments:} You can put comments in your script using \#. This is extremely important in order to keep an organized and understandable code. Please get used to comment what you are doing. Please have a look at the following code:
\begin{verbatim}
#########################
### Help and comments ###
#########################
?mean
??mean
apropos("mean")
example("mean")
find("mean")
\end{verbatim}
\end{solution}

\subsection{Packages\label{packages}}

An advantage of R is the large number of packages available on CRAN. Packages increase the functionality of R. If your computer is connected to the internet, you can install new packages by choosing the menu items \textquotedblleft Pakete\textquotedblright , \textquotedblleft Installiere Paket(e)\ldots \textquotedblright .

\begin{enumerate}
\item Install the package \texttt{xlsx}. Then activate the package using the command \texttt{library(xlsx)}. Typing \texttt{library(help=xlsx)} will give you more information about the new commands.
\item Install the package \texttt{AER}. We will need it for the next exercises.
\end{enumerate}

\begin{solution}
\textbf{Packages}

There are several methods to install new packages:
\begin{enumerate}
  \item Using the GUI:
    \begin{itemize}
     \item \texttt{Pakete -- Installiere Pakete}, choose a close-by mirror.
     \item Highlight AER and xlsx (using the CTRL button you can highlight several items).
     \end{itemize}
  \item Using the command window:
     \begin{itemize}
       \item \texttt{install.packages("xlsx")}
     \end{itemize}
\end{enumerate}
     After you downloaded the packages you can load them either by typing: \newline \texttt{library(xlsx)} and \texttt{library(AER)} or using the GUI: \texttt{Pakete -- Lade Paket}.\newline Please have a look at the help files: \texttt{library(help=xlsx)} and \texttt{library(help=AER)}. Here's the code:
\begin{verbatim}
################
### Packages ###
################
#If you want to specify a folder for the installation files of the package,
#use: .libPaths("X:/Your Folder")
install.packages("xlsx")
#very important: JAVA needs to be installed on your computer otherwise you get an error
library(xlsx)
library(help=xlsx)
library(AER)
\end{verbatim}
\end{solution}

\section{Importing data into R\label{importingdata}}

When learning a new computer language, the most basic standard problem is how to import data. In this exercise you
will learn a number of ways to read datasets. You can type the commands either in the command window or, preferably, write and save a script and
then execute it.

\subsection{Reading text files}

Download the file \texttt{bsp1.txt} from the course page and save it in the directory \texttt{c:/temp} (of course, you may use other directories).
Change the working directory to \texttt{c:/temp}. Use the command

\texttt{bsp1 <- read.csv("bsp1.txt")}

to import data into object \texttt{bsp1} and type \texttt{print(bsp1)}, or simply \texttt{bsp1}, to see the dataset.

\begin{solution}
\textbf{Reading text files}

The output should look like this:
\begin{verbatim}
   Groesse Alter Tore Gehalt
1      168    21    0    1.9
2      186    20    0    1.6
3      158    21    4    3.3
4      170    20    6    1.6
(...)
\end{verbatim}
CSV (\emph{Comma-Separated Values}) is a format to describe structured data in a textfile (\texttt{.txt} or \texttt{.csv}). Each item is separated by a comma, semicolon or a tabstop. Often you can find the names of the variables in the first line (\texttt{header=TRUE} oder \texttt{header=FALSE}). The command \texttt{read.csv()} is used to import the data. You can change the separation symbol with \texttt{sep=}. The default value is the comma: \texttt{bsp1 <- read.csv("bsp1.txt",sep=",")}.

If you don't want to write the exact name of the file, simply use \texttt{file=file.choose()}. The code looks like this:
\begin{verbatim}
##########################
### Reading text files ###
##########################
bsp1 <- read.csv(file.choose())
\end{verbatim}
\end{solution}

\subsection{Reading excel files}

Download the excel file \texttt{bsp2.xlsx} from the course page and save it. Reading excel files is rather uncomfortable in R.

\begin{enumerate}
\item Open the file from Excel and save it as \texttt{bsp2.csv}. In contrast to the English version, the German version of Excel does not write a decimal point, but a comma, and entries are not separated by commas, but semicolons.\footnote{Please always check, if your Excel version uses the German or the English format.} If your data are saved in the German format, you can read the data using one of the two following commands

    \texttt{bsp2 <- read.csv("bsp2.csv",dec=",",sep=";")}


    \texttt{bsp2 <- read.csv2("bsp2.csv")}.

    Import the dataset and have a look at it using \texttt{print(bsp2)}.

\item If you insist to read Excel files, the best way to do it is by means of the package \texttt{xlsx}. Activate the package using \texttt{library(xlsx)}. Read the help text of the command \texttt{read.xlsx}. Load the file \texttt{bsp2.xlsx} using the command \texttt{read.xlsx} and print the data.
\end{enumerate}

\begin{solution}
\textbf{Reading excel files}

\begin{verbatim}
###########################
### Reading excel files ###
###########################
#specify the symbol for the decimal point and separator
bsp2 <- read.csv(file.choose(), dec=",",sep=";")
bsp2
bsp2 <- read.csv(file.choose()) # if you use the "English" decimal point
bsp2 <- read.csv2(file.choose()) # if you use the "German" decimal point
bsp2

#use the package xlsx
library(xlsx)
?read.xlsx
bsp2 <- read.xlsx(file.choose(),sheetIndex=1)
bsp2
\end{verbatim}
\end{solution}

\subsection{Other data formats}

\begin{enumerate}
\item There is a large number of packages to make foreign data formats readable in R. The most important package is \texttt{foreign}, which can be used to read SPSS and Stata files (but not Excel). Install and activate the package \texttt{foreign} and read the corresponding help with \texttt{library(help=foreign)}. Load \texttt{bsp3.dta} into the object $\mathtt{bsp3}$ and print it.

\item R also has got its own data format. You can save objects using the command \texttt{save} and then re-load them with \texttt{load}. Save the object \texttt{bsp3} in the file \texttt{bsp3.Rdata}, quit R, restart, and load the file \texttt{bsp3.Rdata}. Print \texttt{bsp3}.

\item Try \texttt{scandat <- scan()} and insert some data. Edit your data with \texttt{edit(scandat)}.
\end{enumerate}

\begin{solution}
\textbf{Other data formats}

Please see the following code.
\begin{verbatim}
##########################
### Other data formats ###
##########################
install.packages("foreign")
library(foreign)
library(help=foreign)
bsp3 <- read.dta(file.choose())
bsp3

#R's data format
save(bsp3, file=file.choose())
rm(bsp3)
bsp3
load(file.choose())
bsp3

#Editing data
                    # you can input as many things as you like,
scandat <- scan()   # copy&paste works as well (very handy)
data.entry(scandat) # edits your data
edit(scandat)       # edits your data
\end{verbatim}
\end{solution}

\subsection{Missing values and trimming}

\texttt{NA} stands for a missing value. NaN stands for Not a Number (example \texttt{0/0}). Missing values can produce errors in some functions and you should either remove them (trimming) or replace them with a 0. Create a vector \texttt{y <- c(1:3,NA,NA,4:2)} and (i) trim or (ii) replace them with 0.

\begin{solution}
\textbf{Missing values and trimming}

Please see the following code.
\begin{verbatim}
###################################
### Missing values and trimming ###
###################################
0/0
y <- c(1:3,NA,NA,4:2)
y
mean(y)
is.na(y)        # a query to get NAs
which(is.na(y)) # another way to get the positions of the NAs
y[-4]           # removes the 4th entry

y <- c(1:3,NA,NA,4:2)
y[which(is.na(y))]=0 ;y # overwrites NA with 0, note: y changes!

y <- c(1:3,NA,NA,4:2)
y[-which(is.na(y))]; y # removes the NA, note: y has not changed!
\end{verbatim}
\end{solution}

\section{Describing data in R\label{beschreiben}}

Imported datasets are usually stored as dataframe objects. A dataframe is almost the same as a matrix. Each row is an observation, and each column is a variable. Create a new script for the following exercises to be able to repeat the commands.

\subsection{Head and tail\label{indices}}

On the internet site of the course you will find the file \texttt{indices.csv}. It contains the daily index values of the two indices DAX and FTSE 100 from 8/9/2005 to 8/9/2010.\footnote{Since working with calendar dates is a bit cumbersome in R, the information about the dates has been omitted.} Load the data into R and save them as dataframe \texttt{indices}. Large dataframes cannot be printed nicely. A good way to learn about the structure of the dataframe is the command \texttt{head(indices)}. Try it (by the way, you can also use \texttt{tail(indices)}). If you are only interested in the variable names of the dataframe, just type \texttt{names(indices)}. For a thorough insight try also \texttt{str(indices)}, \texttt{class(indices)} and \texttt{attributes(indices)}.

\begin{solution}
\textbf{Head and tail}

Please have a look at the following code:
\begin{verbatim}
#####################
### Head and tail ###
#####################
indices <- read.csv2(file.choose())
head(indices)
tail(indices)
names(indices)
str(indices)        # gives you the structure and an overview of the object
class(indices)      # gives you the type of the object
attributes(indices) # gives you a very good overview of the attributes of the object
\end{verbatim}
\end{solution}

\subsection{Attaching dataframes}

If you use the \texttt{attach} command, the columns of the dataframe are accessible by the column names as ordinary variables. Now you can directly access the two variables \texttt{dax} and \texttt{ftse}. Type \texttt{attach(indices)}. \emph{Note: The help page for \texttt{attach} notes that attach can lead to confusion: The possibilities for creating errors when using attach are numerous.} Therefore we are going to avoid it and use the \texttt{\$} sign to attach variables, i.e. \texttt{dax <- indices$dax; ftse <- indices$ftse}. Another way is to directly access the columns of the dataframe, i.e. \texttt{dax <- indices[,1]; ftse <- indices[,2]}. Save the DAX series into \texttt{dax} and the FTSE series into \texttt{ftse}.

\begin{solution}
\textbf{Attaching dataframes}

Please have a look at the following code:
\begin{verbatim}
############################
### Attaching dataframes ###
############################
#To access data there are at least 3 ways to do it
#1) With the dollar sign
dax <- indices$dax; ftse <- indices$ftse
print(dax)
#2) with braskets
dax <- indices[,1]; ftse <- indices[,2]
print(dax)
#3) with the attach command
attach(indices)
print(dax)
\end{verbatim}
\end{solution}

\subsection{Simple plots}

Type \texttt{plot(dax)} to create a graph showing the time series of the DAX index. Create a new graph of the time series of the logarithm of the DAX index.

\begin{solution}
\textbf{Simple plots }

\begin{verbatim}
####################
### Simple plots ###
####################
plot(dax)
plot(log(dax)) #compare the y-axis!
\end{verbatim}
The graphs look the same, the y-axis, however, has changed. The range is smaller using logs.
\end{solution}

\subsection{Stock returns}

\begin{enumerate}\setlength{\itemsep}{-1pt}
\item Save the number of observations into the variable \texttt{n}. Hint: The command \texttt{dim(x)} returns the number of rows and columns of \texttt{x} as a vector.
\item Generate a new variable containing the daily returns of the DAX index:

    \texttt{rdax <- log(dax[2:n]/dax[1:(n-1)])}

     and plot them. Define and plot the returns of the FTSE in a similar way (\texttt{rftse}).

\item Activate the package \texttt{MASS}. Use the command \texttt{truehist} to draw the histogram of the DAX returns.

\item For the DAX returns, compute the mean (\texttt{mean}), the standard deviation (\texttt{sd}), the variance (\texttt{var}), the median, the 1\%-and the 99\% quantiles (\texttt{quantile}), and the range (\texttt{range}).

\item Sometimes boxplots are a nice way to present a dataset. Type \texttt{boxplot(rdax,rftse}).

\item Plot the DAX returns against the FTSE returns using \texttt{plot(rdax,rftse}).

\item Compute the correlation between the DAX returns and the FTSE returns (\texttt{cor}).

\item Compute the correlation of the DAX returns with its lagged (by one day) return.
\end{enumerate}

\begin{solution}
\textbf{Stock returns}
Please see the following code:
\begin{verbatim}
#####################
### Stock returns ###
#####################
dim(indices)
n <- dim(indices)[1]; n
head(dax)
head(dax[2:n])     # vector containing all elements of dax except the first one

tail(dax)
tail(dax[1:(n-1)]) # vector containing all elements of dax except the last one
rdax <- log(dax[2:n]/dax[1:(n-1)])
head(rdax)
length(rdax)
rftse <- log(ftse[2:n]/ftse[1:(n-1)])
	
plot(rdax)
library(MASS)
truehist(rdax)
	
mean(rdax)
var(rdax)
sd(rdax)
median(rdax)
quantile(rdax,probs=c(0.01,0.99))
range(rdax)
	
boxplot(rdax)
boxplot(rdax,rftse)

plot(rdax,rftse)
	
cor(rdax,rftse)

m <- length(rdax)
#Attention dim(rdax) doesn't work (dim(rdax)=NULL), so we have to use the length of the vector.
#Note: A vector has the dimension of NULL!
cor(rdax[2:m],rdax[1:(m-1)])
\end{verbatim}

  The plots, the histogram as well as the boxplot show the well-known stylized facts about stock returns:
  \begin{itemize}
  \item The mean is around 0, but positive (positive expected return).
  \item The standard deviation is a measure of risk.
  \item Compared to a normal distribution, one can see that the daily returns are not perfectly symmetric around the mean (weak asymmetry).
  \begin{itemize}
  \item Large negative returns are more often than large positive ones.
  \item Large positive returns are in absolute terms greater than large negative returns.
  \end{itemize}
  \item There's more mass in the tails of the distribution (fat tails).
  \item The center is, compared to a normal distribution, higher (peakedness).
  \end{itemize}
  \item The computed statistics support the evidence for those stylized facts.
\end{solution}

\section{Graphics with R \label{graphics}}

A strength of R is its flexible way to create graphics. The following exercises illustrate that. Please write scripts for these exercises.

\begin{solution}
\textbf{More on graphics }

\texttt{plot()} is a very powerful command. Among other things it creates a graphical window, a Cartesian coordinate system and it plots the data. Most graphic functions expect x- and y-coordinates. You can load these from variables, enter them manually or use the function \texttt{locator(n)}, where you can simply click in the plot. The parameter \texttt{n} indicates the number of times you have to click for coordinates. Thus, \texttt{points(locator(4))} expects four clicks and after that it puts four points in the graph.

You can add other graphics like \texttt{points()}, \texttt{lines()},\texttt{legend()},... Please note that in order to use those functions you first have to call the \texttt{plot()} function. Thus, when plotting several graphics, write a script and execute all commands each time you change something.

There are several parameters that are pretty common for all graphical functions. Among others: \texttt{xlab=, ylab=, main=, col=, pch=, type=, ylim=, xlim=, lty=, lwd=} \dots

\texttt{par()} creates a new windows for several graphics. It doesn't draw a coordinate system, however, so you have to use \texttt{plot()} again.
\end{solution}

\subsection{School data}

\begin{enumerate}
\item Download the dataset \texttt{caschool.csv} into the object \texttt{caschool}. This dataset is discussed in great detail in the textbook of Stock and Watson. The codebook (\texttt{caschool.pdf}) is downloadable from the internet site of this course. Draw a scatterplot of the variable \texttt{testscr} against \texttt{str}.

\item Re-create the same plot with nicer and more informative axis labels (the axes options in the \texttt{plot} command are \texttt{xlab} and \texttt{ylab}).

\item Re-create the plot again and add a title (using the \texttt{main} option of the \texttt{plot} command).

\item The \texttt{col}-option can be used to change the colors of the points or lines. Try it. A list of all available color names is \texttt{colors()}. One can even color different parts of the plot differently, but we omit that here.

\item The command \texttt{points} adds one or more points into an existing plot. Add the point (mean of \texttt{str}, mean of \texttt{testscore}) to your last plot in red color. If you want to change the point symbol, you can use the option \texttt{pch}, see also \texttt{?points}.

\item The \texttt{text} command inserts text into an existing plot. Label the red point with the text \textquotedblleft mean\textquotedblright . The easiest way to position the text is by means of the mouse. Use the command \texttt{locator}, e.g. as in \texttt{text(locator(1),"mean")}.

\item \label{streudiagr}One can partition the window into an array of small windows. You can prepare a partition using \texttt{par(mfrow=c(n,m))} where $n\times m$ is the number of plots ($n$ rows and $m$ columns). Prepare a window for four scatterplots ($2\times 2$). Plot the scatterplots of \texttt{testscr} against (a) the teacher-student ratio \texttt{str}, (b) the percentage of English language learners \texttt{el\_pct}, (c) the percentage qualifying for reduced price lunch \texttt{meal\_pct}, (d) the percentage qualifying for income assistance \texttt{calw\_pct}.
\end{enumerate}

\begin{solution}
\textbf{School data }

Please see the following code:
\begin{verbatim}
###################
### School data ###
###################
#1)
caschool  <- read.csv(file.choose())
head(caschool)
tail(caschool)
names(caschool)
str(caschool)
str <- caschool$str #Note that you have now overwritten the str() function!
testscr <- caschool$testscr
plot(str,testscr)

#2)
plot(str,testscr,xlab="Student Teacher Ratio", ylab="Test Score")

#3)
plot(str,testscr,xlab="Student Teacher Ratio", ylab="Test Score",main="CA Test Score Data")

#4)
print(colors())
plot(str,testscr,xlab="Student Teacher Ratio", ylab="Test Score",main="CA Test Score Data",
                                                                                col="violet")

#5)
?points #read the pch settings
plot(str,testscr,xlab="Student Teacher Ratio", ylab="Test Score",main="CA Test Score Data")
points(mean(str),mean(testscr), col= "red", pch=23)

#6)
#either
plot(str,testscr,xlab="Student Teacher Ratio", ylab="Test Score",main="CA Test Score Data")
points(mean(str),mean(testscr), col= "red", pch=19)
text(mean(str)-1,mean(testscr),"MEAN", col="red")
#or
plot(str,testscr,xlab="Student Teacher Ratio", ylab="Test Score",main="CA Test Score Data")
points(mean(str),mean(testscr), col= "red", pch=19)
text(locator(1),"MEAN", col="red") #locator(n) asks for n positions

#7)
el_pct <- caschool$el_pct
meal_pct <- caschool$meal_pct
calw_pct <- caschool$calw_pct
par(mfrow=c(2,2))
plot(testscr, str, xlab = "Test Score", ylab="Student-teacher-ratio")
plot(testscr, el_pct,xlab = "Test Score", ylab="Percentage English learners")
plot(testscr, meal_pct,xlab = "Test Score", ylab="Percentage reduced price lunch")
plot(testscr, calw_pct,xlab = "Test Score", ylab="Percentage income assistance")
\end{verbatim}
\end{solution}

\subsection{Index returns}

\begin{enumerate}
\item Download the dataset \texttt{indices.csv}. Generate a new variable with starting value 100 that represents the relative time series of the DAX index. Plot the time series of this normalized DAX index using the command \texttt{plot} with the options \texttt{type="{}l"} (for \textquotedblleft line\textquotedblright ) and \texttt{col="blue"}.

\item Add the normalized FTSE index to the last plot. Use the command \texttt{lines} with the color option \texttt{col="{}red"}.

\item Use the \texttt{legend} command to add a legend explaining the meaning of the two colored lines. You may use the command \texttt{locator} to find a suitable position for the legend.
\end{enumerate}

\begin{solution}
\textbf{Index returns}

Please see the following code:
\begin{verbatim}
#####################
### Index returns ###
#####################
#1)
indices <- read.csv2(file.choose())
head(indices)
tail(indices)
names(indices)
str(indices)

dax <- indices$dax	
dax_norm <- 100*dax/dax[1]
par(mfrow=c(3,1))
plot(dax)
plot(dax_norm)
plot(dax,dax_norm) #Perfect linear correlation
cor(dax,dax_norm)  #Perfect linear correlation

#2)
par(mfrow=c(1,1))
ftse <- indices$ftse
ftse_norm <- 100*ftse/ftse[1]
plot(dax_norm,type="l", col="blue")
lines(ftse_norm, col="red")

#3)
plot(dax_norm,type="l", col="blue")
lines(ftse_norm, col="red")
legend(locator(1), legend=c("Normalized Dax", "Normalized FTSE"), fill = c("blue","red"))
\end{verbatim}
\end{solution}

\section{Programming with R\label{miscellaneous}}

\subsection{Using functions}

\begin{enumerate}
\item Functions are called by its name followed by the arguments in parentheses. The syntax is

\texttt{function(Argument\ 1=arg1,\ Argument\ 2=arg2,\ \dots )}

You can specify arguments either by regarding the order they need to be called (\texttt{log(10,10)}) or by specifying the argument itself (\texttt{log(10,base=10})).

You can call several functions at a time using ";". Whenever you encounter the symbol $+$ instead of $>$, you have forgotten to close parentheses. Just type \texttt{")"} or hit ESC.

Try:

\texttt{sqrt(2); sin(pi); exp(1); log(10)}; \texttt{log(10,10);log(10,base=10)}; \newline
\texttt{sqrt(2} (without closing the bracket!)

\item The concatenation function c() creates vectors. You can pick the i-th item of a vector using square brackets. Try:

\texttt{simpsons <- c(\textit{"Homer"},\textit{"Marge"},\textit{%
"Bart"},\textit{"Lisa"},\textit{"Maggie"})}\newline
\texttt{x <- c(1,2,3,4,5,6,7,8,9,10)}\newline
\texttt{x <- c(1:10)}\newline
\texttt{length(simpsons); sum(x); mean(x)}\newline
\texttt{simpsons[3]}

\item Consider the vector \texttt{x <- 0:10}. Use the function \texttt{sum()} to calculate the sum of all values that are smaller than 5, i.e. 0+1+2+3+4 = 10.
\end{enumerate}

\begin{solution}
\textbf{Using functions}

Have a look at the following code:
\begin{verbatim}
#######################
### Using functions ###
#######################
#1)
log(10,10)
log(10,base=10)

sqrt(2); sin(pi); exp(1); log(10)
log(10,10);log(10,base=10)
sqrt(2

#2)
simpsons <- c("Homer","Marge","Bart","Lisa","Maggie")
x <- c(1,2,3,4,5,6,7,8,9,10)
x <- c(1:10)
length(simpsons); sum(x); mean(x)
simpsons[3]
simpsons[-3]

#3)
x <- 0:10
sum(x<5)
x<5

x*(x<5)
sum(x*(x<5)) #simple

x[x<5]
sum(x[x<5]) # more elegant
\end{verbatim}
\texttt{sum(x<5)} is equal to 5, because \texttt{x<5} gives you a vector consisting of 1 and 0. These numbers indicate if the value in \texttt{x} is smaller than 5, i.e. TRUE(1) if it's smaller and FALSE(0) if not. \texttt{sum(x<5)} counts these 0's and 1's. The right solution is \texttt{sum(x*(x<5))} or \texttt{sum(x[x<5])}. Try to understand why!

\end{solution}

\subsection{Sequences and other vectors}

In R, sequences are generated by the \texttt{seq}-command. An abbreviated form for integers is \texttt{from:to}. To generate a vector with repeated elements use the command \texttt{rep}.

\begin{enumerate}\setlength{\itemsep}{0pt}
\item Generate the vectors $x=(1,2,\ldots ,100)$ and $y=(2,4,6,\ldots ,1000)$.

\item Generate an equi-spaced grid from $-4$ to $4$ with $500$ grid points.

\item Generate a vector of $n=100$ missing values (\texttt{NA}).

\item Generate the vector $x=(0,1,2,0,1,2,\ldots ,0,1,2)$ of length 300.

\item Generate the vector $x=(0,\ldots ,0,1,0,\ldots ,0)$ of length 100 with
the 1 at position 40.
\end{enumerate}

\begin{solution}
\textbf{Sequences and other vectors}

\begin{verbatim}
###################################
### Sequences and other vectors ###
###################################
?seq
?rep

# x=(1,2,...,100)
x <- 1:100; x

# x=(2,4,...,1000)
x <- 2*(1:500)
x <- seq(2,1000,by=2); x

# equi-spaced grid from -4 to 4 with 500 grid points
x <- seq(-4,4,length.out=500); x
length(x)
x[2]-x[1]
x[2]-x[1] == x[300]-x[299]

# 100 missing values
x <- rep(NA,100); x

# x=(0,1,2,0,1,2,...,0,1,2) with length 300
x <- rep(c(0,1,2),100); x
length(x)

# vector with 0 except a 1 at position 40, length 100
# several ways to do this
x <- c(rep(0,39),1,rep(0,60)); x ;length(x)
x <- rep(0,100); x[40] <- 1; x; length(x)
\end{verbatim}
\end{solution}

\subsection{Random numbers}

There are random number generators for a large number of distributions. The general syntax is

\texttt{rNAME(n,parameters)}

where \texttt{NAME} is an abbreviation of the distribution name (e.g. \texttt{norm}, \texttt{lnorm}, \texttt{binom}, etc.), \texttt{n} is the number of values to be drawn, and \texttt{parameters} are the parameter(s) of the distribution.

\begin{enumerate}
\item Activate the \texttt{MASS} package. Generate a vector \texttt{x} of $n=10000$ random numbers drawn from the standard normal distribution and plot the histogram.

\item Generate a vector \texttt{r} of $n=500$ random numbers drawn from the $t$-distribution with 3 degrees of freedom (see \texttt{?rt}). Execute \texttt{plot(r)}.

\item Cumulate the vector \texttt{r} using the command \texttt{cumsum}. Plot the cumulated series.
\end{enumerate}

\begin{solution}
\textbf{Random numbers }

\begin{verbatim}
######################
### Random numbers ###
######################
library(MASS)
#1)
?rnorm
x <- rnorm(10000); x
truehist(x)

#2)
?rt
r <- rt(500,3); r
plot(r)
mean(r) # expectation of a student t-distribution is E(r) = 0
var(r)  # variance of a student t-distribution is Var(r) = (k)/(k-2)
        # with k degrees of freedom. Here: Var(r)=3
truehist(r)

#3)
x <- 1:10
x; cumsum(x)
plot(cumsum(r)) # we se a random walk!
\end{verbatim}
\end{solution}

\subsection{Loops}

In general, one should try to avoid loops in R as they often slow down the computations considerably. In this course, we will ignore this advice for didactical reasons. The type of loop that is used most often, is the \texttt{for}-loop. Unfortunately, the help function does not work for the loop commands, please type \texttt{?Control} to read the help text. The syntax of the \texttt{for}-loop is

\texttt{for(~[var] in [sequence]) \{ [commands] \}}

where \texttt{[var]} is an index variable and \texttt{[sequence]} is a vector of values to be assigned to the index variable. In our applications, we often need to store the results computed within the loop in a result vector. In this case, it is advisable to initiate an empty vector before the loop starts:

\texttt{Z <- rep(NA,100)}

\texttt{for(i in 1:100) \{ [compute something with result x]; Z[i] <- x \}}

\begin{enumerate}\setlength{\itemsep}{-1pt}
\item Generate a vector \texttt{r} of $n=500$ random numbers drawn from the $t$-distribution with 3 degrees of freedom. Use a \texttt{for}-loop to compute the moving average of \texttt{r} within a window of length 21.

\item Write a program using a \texttt{for}-loop over $r=1,\ldots ,10000$ to perform the following steps for every $r$: Generate a sample of size $n=100$ from the lognormal distribution $LN(0,1)$. Find the maximum and store it. After the loop is performed, plot the histogram of the maxima.
\end{enumerate}

\begin{solution}
\textbf{Loops}

\begin{verbatim}
#############
### Loops ###
#############
#1)
?Control
r <- rt(500,3)
Z <- rep(NA,length(r))

for (i in 10:length(r)) {
    Z[i] <- mean(r[-10:10 + i])
}

plot(r)
lines(Z, col="red")
abline(h=mean(r), col="blue")

#2)
??"Log normal"
?Lognormal
Z <- rep(NA,10000)
for (r in 1:10000) {
	Z[r] <- max(rlnorm(100))
}

library(MASS)
truehist(Z)
truehist(rlnorm(10000))
\end{verbatim}
\end{solution}

\subsection{Functions}

Functions are very powerful in R. Their general syntax is

\texttt{f <- function(arg1,arg2,...) \{ [commands to compute output var]; return(var)\} }

where the arguments can be scalars, vectors, matrices etc. For example, the following function computes and returns $x^{2}+2y^{2}$.

\texttt{fexmpl <- function(x,y) \{ z <- x\symbol{94}2+2*y\symbol{94}2; return(z)\}}

Once the function has been defined it can be used like any other internal R function.

\begin{enumerate}\setlength{\itemsep}{-1pt}
\item Define a function $f(x)=x^{2}+\sin (x)$ where $x$ can either be a scalar or a vector. Define a grid of length 500 on the interval $[-3,3]$ and plot the function.

\item Define a function that computes the empirical raw moment of order $p$ for a sample $x_{1},\ldots ,x_{n}$, i.e. $m_{p}=\frac{1}{n}\sum_{i=1}^{n}x_{i}^{p}$.
\end{enumerate}

\begin{solution}
\textbf{Functions}

\begin{verbatim}
#################
### Functions ###
#################
#1)
fexmpl1 <- function(x) {
    z <- x^2 + sin(x)
    return(z)
}
str(fexmpl1)

x <- seq(-3,3,length.out=500)
plot(x,fexmpl1(x))

#2)
mp <- function(x,p) {
	n <- length(x)
	m <- 1/n * sum(x^p)
	return(m)
}

x <- c(1,2,3)
mp(x,1)
mp(x,2)
\end{verbatim}
\end{solution}

\subsection{Numerical optimization}

There are two commands for numerical optimization: \texttt{optimize} for univariate optimization and \texttt{optim} for multivariate optimization.

\begin{enumerate}\setlength{\itemsep}{-1pt}
\item Numerically find the minimum of the function $f(x)=x^{2}+\sin (x)$. \emph{Hint: It lies between -1 and 0.}

\item Numerically find the minimum of the function $f(x,y)=x^{2}+\sin (x)+y^{2}-2\cos (y)$. First get a view of the function using the following commands
\begin{verbatim}
f <- function(x,y) x^2+sin(x)+y^2-2*cos(y)
x <- seq(-5,5,by=.2);y <- seq(-5,5,by=.2);z <- outer(x,y,f)
persp(x,y,z,phi=-45,theta=45,col="yellow",shade=.65 ,ticktype="detailed")
\end{verbatim}
You can try to edit phi and theta to get a better view.
\end{enumerate}

\begin{solution}
\textbf{Numerical optimization}

\begin{verbatim}
##############################
### Numerical optimization ###
##############################
?optimize
?optim
#1)
optimize(fexmpl1, upper=0, lower=-1)

#2)
f <- function(x,y) x^2+sin(x)+y^2-2*cos(y)
x <- seq(-5,5,by=.2)
y <- seq(-5,5,by=.2)
z <- outer(x,y,f)
persp(x,y,z,phi=-45,theta=45,col="yellow",shade=.65 ,ticktype="detailed")
#the Minimum appears to be at (-.5,0)

#note: When using optim for multidimensional optimization,
#the argument in your definition of the function must be a single vector
fx <- function(x) x[1]^2+sin(x[1])+x[2]^2-2*cos(x[2])
optim(c(-.5,0),fx) # does work!
optim(c(-.5,0),f) #does not work
\end{verbatim}
\end{solution}

\section{Probability theory\label{repetition1}}

\subsection{Moments\label{moments}}

\begin{enumerate}
\item Show that the moments of the standard normal distribution $N(0,1)$ are
$\mu _{r}=0$ for odd orders $r$, and $\mu _{r}=\prod_{i=1}^{r/2}\left(
2i-1\right) $ for even orders $r$.

\item Let $X\sim N(\mu ,\sigma ^{2})$ and $Y=\exp (X)$. Derive the
expectation of $Y$.

\item The distribution function of the Pareto distribution with parameters $%
K>0$ and $\alpha >0$ is%
\begin{equation*}
F_{X}(x)=1-\left( \frac{K}{x}\right) ^{\alpha }.
\end{equation*}%
where $x\geq K$. Derive the density $f_{X}$ and the moment of order $%
p<\alpha $. Do moments of order $p\geq \alpha $ exist?
\end{enumerate}

\begin{solution}
\textbf{Moments}

\begin{enumerate}
\item Since the density function of $N(0,1)$ is symmetric around 0, all
    odd moments are obviously 0. Proof:
    \begin{equation*}
      E[X^r] = E[(-X)^r] = E[(-1)^r X^r] = E[-(X^r)]=-E[X^r]\Leftrightarrow E[X^r] = 0
    \end{equation*}

    We use without proof $\mu _{2}=1$ (variance). As to moments of even
    orders $r\geq 4$,
\begin{equation*}
\mu _{r}=\int_{-\infty }^{\infty }x^{r}\frac{1}{\sqrt{2\pi }}e^{-x^{2}/2}dx
\end{equation*}%
which can be integrated by parts to
\begin{equation*}
\mu _{r}=\left. \frac{1}{r+1}x^{r+1}\frac{1}{\sqrt{2\pi }}%
e^{-x^{2}/2}\right\vert _{-\infty }^{\infty }-\int_{-\infty }^{\infty }\frac{%
1}{r+1}x^{r+1}\frac{1}{\sqrt{2\pi }}e^{-x^{2}/2}\left( -x\right) dx.
\end{equation*}%
Since the exponential function goes to zero faster than any power of $x$
goes to infinity, the first summand vanishes, and%
\begin{equation*}
\mu _{r}=\frac{1}{r+1}\underbrace{\int_{-\infty }^{\infty }x^{r+2}\frac{1}{\sqrt{2\pi }}e^{-x^{2}/2}dx}_{\mu_{r+2}}
\end{equation*}%
or%
\begin{equation*}
\mu _{r+2}=\left( r+1\right) \mu _{r}.
\end{equation*}%
Odd moments are thus the product of odd numbers. Another way to write
this product is given by $\mu _{r}=\prod_{i=1}^{r/2}\left( 2i-1\right) $.

\item The expectation is%
\begin{eqnarray*}
E(Y) &=&E\left( \exp \left( X\right) \right)  \\
&=&\int_{-\infty }^{\infty }\exp \left( x\right) \frac{1}{\sqrt{2\pi }\sigma
}\exp \left( -\frac{1}{2}\left( \frac{x-\mu }{\sigma }\right) ^{2}\right) dx
\\
&=&\int_{-\infty }^{\infty }\frac{1}{\sqrt{2\pi }\sigma }\exp \left( -\frac{1%
}{2}\left( \frac{x-\mu }{\sigma }\right) ^{2}-\frac{1}{2}\left( -2x\right)
\right) dx.
\end{eqnarray*}%
The term inside the exponential function can be written as%
\begin{eqnarray*}
&&-\frac{1}{2}\left[ \left( \frac{x-\mu }{\sigma }\right) ^{2}-\frac{%
2x\sigma ^{2}}{\sigma ^{2}}\right]  \\
&=&-\frac{1}{2}\left( \frac{x^{2}-2\left( \mu +\sigma ^{2}\right) x+\mu ^{2}%
}{\sigma ^{2}}\right)  \\
&=&-\frac{1}{2}\left( \frac{\left[ x^{2}-2\left( \mu +\sigma ^{2}\right)
x+\left( \mu +\sigma ^{2}\right) ^{2}\right] -\left( \mu +\sigma ^{2}\right)
^{2}+\mu ^{2}}{\sigma ^{2}}\right)  \\
&=&-\frac{1}{2}\left( \frac{\left[ x-\left( \mu +\sigma ^{2}\right)
\right]^2 -\mu^2 -2\mu \sigma ^{2}-\sigma ^{4}+\mu^2}{%
\sigma ^{2}}\right)  \\
&=&-\frac{1}{2}\left(\frac{\left[x-(\mu+\sigma^2)\right]^2}{\sigma^2} - 2\mu - \sigma^2 \right)
\end{eqnarray*}%
Hence,%
\begin{eqnarray*}
E(Y) &=&\int_{-\infty }^{\infty }\frac{1}{\sqrt{2\pi }\sigma }\exp \left( -%
\frac{1}{2}\left(\frac{\left[x-(\mu+\sigma^2)\right]^2}{\sigma^2}\right) + \mu +\frac{\sigma^2}{2} \right) dx \\
&=&e^{\mu +\sigma ^{2}/2}\int_{-\infty }^{\infty }\frac{1}{\sqrt{2\pi }%
\sigma }\exp \left( -\frac{1}{2}\left(\frac{\left[x-(\mu+\sigma^2)\right]^2}{\sigma^2}\right)\right)dx
\end{eqnarray*}%
The integrand is simply the density of a normal distribution with mean
$\mu +\sigma ^{2}$ and variance $\sigma ^{2}$. The integral over the
density is unity, and thus%
\begin{equation*}
E(Y)=e^{\mu +\sigma ^{2}/2}.
\end{equation*}

\item The density of the Pareto distribution is%
\begin{equation*}
f_{X}(x)=\frac{dF(x)}{dx}=\alpha K^\alpha x^{-\alpha -1}
\end{equation*}%
for $x\geq K$ (and zero elsewhere). The moment of order $%
p<\alpha $ is%
\begin{eqnarray*}
E\left( X^{p}\right)  &=&\int_{-\infty }^{\infty }x^{p}f_{X}(x)dx \\
&=&\int_{K}^{\infty }x^{p}\alpha K^\alpha x^{-\alpha -1}dx \quad\text{since } x\geq K\\
&=&\alpha K^\alpha\int_{K}^{\infty }x^{p-\alpha -1}dx \\
&=&\alpha K^\alpha\cdot \left[ \frac{1}{p-\alpha }x^{p-\alpha }\right]
_{K}^{\infty }.
\end{eqnarray*}%
Since $p<\alpha $ the expression within the square brackets goes to zero as $%
x\rightarrow \infty $, and therefore%
\begin{eqnarray*}
E\left( X^{p}\right)  &=&\alpha K^\alpha\cdot \frac{1}{\alpha -p} K^{p-\alpha } \\
&=&\frac{\alpha }{\alpha -p}K^{p}.
\end{eqnarray*}%
Moments of order $p\geq \alpha $ do not exist as the integral does not
converge.
\end{enumerate}
\end{solution}
\newpage

\section{Multiple linear regression\label{repetition2}}

Linear models are estimated in R by the command \texttt{lm}. This command
has an unusual syntax and returns a rather complex object (called \texttt{lm}
object). Be prepared: it takes some time to get used to that. We start with
the simple linear regression model that is used in the textbook by Stock and
Watson. The codebook \texttt{caschool.pdf} for the dataset can be downloaded
from the course site.

\subsection{Student teacher ratio (I)}

\begin{enumerate}
\item Load the dataset \texttt{caschool.csv} into the object \texttt{caschool} and make \texttt{testscr} as well as \texttt{str} accessible. Perform the following
commands:\smallskip \newline
\texttt{regr <- lm(testscr\symbol{126}str)}\newline
\texttt{print(regr)}

Create the scatterplot of \texttt{testscr} against \texttt{str} and then
type \texttt{abline(regr)}. The color (\texttt{col}), the line type (\texttt{%
lty}), and the line width (\texttt{lwd}) can easily be changed by the
options of the plot command. Try it.

\item The student teacher ratio \texttt{str} in the school district Antelope
is 19.33 an. Predict the variable \texttt{testscore} for the district
Antelope using the \texttt{predict} command. To do so, type \texttt{%
predict(regr,newdata=data.frame(str=19.33))}. Add the predicted value to the
plot (in blue color).

\item Among other things, \texttt{lm} objects also contain the residuals of
the regression. You can extract them using the function \texttt{residuals}
with the \texttt{lm}-object as argument. Compute the sum of the residuals.

\item Create a plot showing the residuals. Add the horizontal axis using the
command \texttt{abline(h=0)} (the \texttt{h} is for horizontal).

\item Plot the residuals against the variable \texttt{str}.

\item Load the \texttt{AER} package. Execute the commands \texttt{print(summary(regr))} and

\texttt{print(coeftest(regr,vcov=vcovHC))}. Interpret the outputs.

\item Test the hypothesis $H_{0}:\beta =-1$. Write down each step of the
test procedure. \emph{Hint: You can also make use of \texttt{linearHypothesis} function of the car package.}

\item Give a 95\% confidence interval for $\beta $.
\end{enumerate}

\begin{solution}
\textbf{Student teacher ratio (I)}

\begin{verbatim}
###################################
#### Student teacher ratio (I) ####
###################################
#1)
caschool <- read.csv(file.choose())
View(caschool)
testscr <-caschool$testscr
str <- caschool$str

regr <- lm(testscr~str)
print(regr)
str(regr) #a very complex object! You can access those things using the $ sign.

plot(str,testscr, xlab="Student-Teacher-Ratio", ylab="Testscore", main="Scatterplot", pch=20)
abline(regr, col="red",lwd=3, lty=2)

#2)
prediction <- predict(regr, newdata=data.frame(str=19.33))
plot(str,testscr, xlab="Student-Teacher-Ratio", ylab="Testscore", main="Scatterplot", pch=20)
abline(regr, col="red",lwd=3, lty=2)
#add a point
points(19.33, prediction, col="blue", pch=20)
#lines connects two points with a straight line.
#Note: abline needs a slope and an intercept to draw a straight line
lines(c(19.33, 19.33), c(0,prediction),lty=3, col="blue")
lines(c(0, 19.33), c(prediction,prediction),lty=3, col="blue")

#3)
#Either use the function residuals()
sum(residuals(regr))
#Or extract them from the object regr with the dollar sign
sum(regr$residuals)

#4)
plot(residuals(regr))
abline(h=0)

#5)
plot(regr$residuals,str)

#6)
print(summary(regr)) # with this command you assume homoscedasticity

library(AER)
print(coeftest(regr,vcov=vcovHC)) #with this command you assume heteroscedasticity

#7)
# Access the variables
BETA <- coeftest(regr,vcov=vcovHC)[2,1]
SDBETA <- coeftest(regr,vcov=vcovHC)[2,2]
#t-Test
H0 <- -1
t <- (BETA- (H0))/SDBETA
t
#the critical values are 1.96 (5%) and 2.58 (1%)
abs(t) > 1.96; abs(t) > 2.58
#H0 can be rejected with a significance level of 5%!
#The estimate is significantly different from -1.

#There is also a function for linear hypothesis testing  (F-Test)
library(car)
linearHypothesis(regr,c("str=-1"))

#8)
lower_limit <- BETA - 1.96*SDBETA
upper_limit <- BETA + 1.96*SDBETA

names(lower_limit)="lower limit"; names(upper_limit)="upper limit"
print(c(lower_limit, upper_limit))
\end{verbatim}
\begin{enumerate}
  \item In the scatterplot you can see that the points scatter
      heterogeneously around the regression line, since there are several
      outliers. The assumption of homoscedasticity does not hold.
  \item Note: \texttt{lines} connects points with a straight line.
      \texttt{abline}, however, needs a slope and an intercept to draw a
      straight line.
  \item The \$ sign is very handy to access data and variables from
      complex objects. Get used to using the \$.
  \item The plot shows that the residuals are heteroscedastic and
      most likely autocorrelated (not i.i.d.).
  \item According to the plot, there is no obvious linear correlation
      between the exogenous variable and the residuals.
  \item This is the standard output of a regression. In order to control
      for heteroscedasticity, use \texttt{coeftest()} and specify the
      variance-covariance matrix appropriately (e.g.
      \texttt{vcov=vcovHC}).
  \item This is a standard t-Test. You can either compute it by hand or
      use \texttt{linearHypothesis()}.
  \item The critical value for a 95\% confidence interval is 1.96.
\end{enumerate}
\end{solution}



\subsection{Capital asset pricing model}

Load the dataset \texttt{capm.csv} and make the variables accessible. The variable \texttt{rdai} contains the daily returns (in
\%) of Daimler from 9/9/2009 to 8/9/2010, the variable \texttt{rdax}
contains the DAX returns. The CAPM implies that the intercept of the simple
linear regression%
\begin{equation*}
r_{DAI,t}=\alpha +\beta r_{DAX,t}+u_{t}
\end{equation*}%
is zero.

\begin{enumerate}
\item Estimate the model and test the null hypothesis $H_{0}:\alpha =0$.

\item The coefficient $\beta $ is a measure of the systemic risk. Give a
95\% confidence interval for $\beta $.
\end{enumerate}

\begin{solution}
\textbf{Capital asset pricing model}

Please see the following code:
\begin{verbatim}
#####################################
#### Capital asset pricing model ####
#####################################
#1)
capm <- read.csv2(file.choose())
head(capm)
rdai <- capm$rdai
rdax <- capm$rdax
regr <- lm(rdai~rdax)
regr
library(car)
linearHypothesis(regr,c("(Intercept) = 0")) #cannot be rejected

#2)
summary(regr)
BETA <- summary(regr)$coefficients[2,1]
SDBETA <- summary(regr)$coefficients[2,2]
lower_limit <- BETA - 1.96*SDBETA
upper_limit <- BETA + 1.96*SDBETA
names(lower_limit)="lower limit"; names(upper_limit)="upper limit"
print(c(lower_limit, upper_limit))
\end{verbatim}
\end{solution}


\subsection{Student teacher ratio (II)}

The \texttt{lm}-command is also used to perform multiple linear regressions.
The syntax is close to the simple linear models. Put the endogenous variable
to the left of the tilde. On the right of the tilde you list the exogenous
variables, separated by plus signs. It looks like this: \texttt{lm(y\symbol{%
126}x1+x2+x3)}.

\begin{enumerate}
\item Load the dataset \texttt{caschool.csv} into the object \texttt{caschool} and
make \texttt{testscr}, \texttt{str}, \texttt{el\_pct} and \texttt{expn\_stu} accessible. Perform the following
commands:\smallskip \newline
\texttt{regr <- lm(testscr\symbol{126}str+el\_pct)}\newline
\texttt{print(regr)}

Explain the output.

\item Regress \texttt{testscr} on \texttt{str}, assign the residuals of the
regression into the variable \texttt{r1} and plot them. Now regress \texttt{%
testscr} on \texttt{str}, \texttt{el\_pct} and \texttt{expn\_stu}, put the
residuals into the variable \texttt{r2} and add them to the plot. Compute
the sum of squared residuals for both regressions.

\item Consider the regression of \texttt{testscr} on \texttt{str}, \texttt{%
el\_pct} and \texttt{expn\_stu}. Using the \texttt{predict}-command, predict
the value of \texttt{testscr} for a school district with an average class
size (\texttt{str}) of 25 students, a percentage of English learners (%
\texttt{el\_pct}) of 60\% and an average expenditures per student (\texttt{%
expn\_stu}) of 4000\$. How would the result change if the average class size
was reduced to 17?

\item Reconsider the regression of \texttt{testscr} on \texttt{str}, \texttt{%
el\_pct} and \texttt{expn\_stu}. Let \texttt{regr} be the object containing
the regression results. Execute the commands \texttt{print(summary(regr))}
and \texttt{print(coeftest(regr,vcov=vcovHC))}. Interpret the output.

\item Test the null hypothesis that the coefficients on \texttt{str} and
\texttt{expn\_stu} both equal $0$ and the coefficient on \texttt{el\_pct}
equals $-0.7$. \emph{Hint: Use the linearHypothesis function of the car package.}
\end{enumerate}

\begin{solution}
\textbf{Student teacher ratio (II)}

\begin{verbatim}
####################################
#### Student teacher ratio (II) ####
####################################
library(AER)
#1)
caschool <- read.csv(file.choose())
head(caschool)
testscr <- caschool$testscr
str <- caschool$str
el_pct <- caschool$el_pct
expn_stu <- caschool$expn_stu
regr <- lm(testscr~str+el_pct)
regr

#2)
simple <- lm(testscr~str)
r1 <- residuals(simple)
plot(r1)

multiple <- lm(testscr~str + el_pct + expn_stu)
r2 <- residuals(multiple)
plot(r2)

plot(r1,ylab="")
points(r2,col="red")

sum(r1^2)
sum(r2^2)
sum(r1^2) > sum(r2^2)

#3)
multiple <- lm(testscr~str + el_pct + expn_stu)
predict(multiple, newdata=data.frame(str=25, el_pct=0.6, expn_stu=4000))
predict(multiple, newdata=data.frame(str=17, el_pct=0.6, expn_stu=4000))

#4)
regr <- lm(testscr~str + el_pct + expn_stu)
summary(regr)
library(AER)
coeftest(regr, vcov=vcovHC)

#5)
library(car)
linearHypothesis(regr,c("str=0","expn_stu=0","el_pct=-.7"))
\end{verbatim}
\end{solution}

\subsection{Omitted variable bias\label{omitted}}

Load the dataset \texttt{omitted.csv} into the object \texttt{omitted} and make it
accessible. There are five variables: \texttt{y}, \texttt{%
x1}, \texttt{x2}, \texttt{x3} and \texttt{x4}. The sample has been generated
in R; the sample size is $n=500$. The true regression surface is%
\begin{equation*}
Y=1+2X_{1}+3X_{2}+4X_{3}+5X_{4}.
\end{equation*}%
The exogenous variable $X_{1}$ is uncorrelated with $X_{2},X_{3},X_{4}$. The
variables $X_{2}$ and $X_{3}$ are positively correlated, so are $X_{3}$ and $%
X_{4}$. The variables $X_{2}$ and $X_{4}$ are uncorrelated.

\begin{enumerate}
\item Estimate the intercept and the slope coefficients for $%
X_{1},X_{2},X_{3},X_{4}$ from the dataset (the estimates should be close to
the true values 1,2,3,4,5).

\item Estimate a regression of $Y$ on $X_{2},X_{3}$ and $X_{4}$. Explain why
the estimates are still close to the true values.

\item Estimate a regression of $Y$ on $X_{1},X_{2}$ and $X_{3}$. Which
coefficients are still estimated accurately? And why?
\end{enumerate}

\begin{solution}
\textbf{Omitted variable bias}

Please see the following code:
\begin{verbatim}
###############################
#### Omitted variable bias ####
###############################
library(car)
omitted <- read.csv2(file.choose())
head(omitted)
y <- omitted$y
x1 <- omitted$x1
x2 <- omitted$x2
x3 <- omitted$x3
x4 <- omitted$x4

cor(omitted)

#1
regra <-lm(y ~ x1+x2+x3+x4)
summary(regra)
linearHypothesis(regra,c("x1=2","x2=3","x3=4","x4=5"))
#2
regrb <- lm(y~x2+x3+x4)
summary(regrb)
linearHypothesis(regrb,c("x2=3","x3=4","x4=5"))
#3
regrc <- lm(y~x1+x2+x3)
summary(regrc)
linearHypothesis(regrc,c("x1=2","x2=3","x3=4"))
\end{verbatim}

Multicollinearity: Strong correlation between the exogenous
      variables. The coefficients, however, are unbiased if the model is
      specified correctly, i.e. if no variables are omitted. Otherwise you get an omitted variable bias.
      Furthermore the estimators are not efficient, they have high standard errors. Also a ceteribus
      paribus interpretation of a single coefficient is not valid.
\end{solution}

\subsection{Asymptotic normality}

\begin{enumerate}
\item Consider the multiple linear regression model $y=X\beta +u$. In R,
generate the matrix $X$ by executing the following commands:

\texttt{library(MASS)}

\texttt{set.seed(123)}

\texttt{X <-
cbind(1,mvrnorm(n=100,c(5,10),matrix(c(1,0.9,0.9,1),2,2)))}

The true coefficient vector is
\begin{equation*}
\beta =\left(
\begin{array}{c}
3 \\
2 \\
-1%
\end{array}%
\right)
\end{equation*}%
and the error terms are i.i.d. uniformly distributed on the interval $[-1,1]$%
. Hence, the assumption of normally distributed error terms is violated.

\item Write an R program that generates $R=10000$ random samples of size $%
n=100$ each (the easiest way to do so is to use a \texttt{for} loop).
Generate an empty vector \texttt{V <- rep(NA,10000)}. For each
sample $i=1,\ldots ,R$, compute the OLS estimate $\hat{\beta}$ of $\beta $
and store the second component of $\hat{\beta}$ in the $i$-th element of the
vector $\mathtt{V}$.

\item Plot the histogram of $\mathtt{V}$.

\item Compute the mean $m$ and standard deviation $s$ of \texttt{V} and add
the density of $N(m,s)$ to the plot. \emph{Hint: You can use \texttt{curve(dnorm(x,mean=m,sd=s),add=T)} to add the Gaussian density with mean m and std. deviation s to the plot.}

\item Move the command that generates $X$ into the loop (without the seed
command). Now there is a new, random $X$ for each sample. Is the normal
approximation still valid?

\item Try if the approximation is worse for sample size $n=10$ (you will
have to shorten $X$ in this case).
\end{enumerate}

\begin{solution}
\textbf{Asymptotic normality}
The code might look like this:
\begin{verbatim}
##############################
#### Asymptotic Normality ####
##############################
library(MASS)
beta_true <- c(3,2,-1)
TT <- 100
R <- 10000
set.seed(123)
X_fix <- cbind(1,mvrnorm(n=TT,c(5,10),matrix(c(1,0.9,0.9,1),2,2)))

V1 <- rep(NA,R); V2 <- rep(NA,R);
for (i in 1:R) {
  u <- runif(TT,-1,1)
  X_rand <- cbind(1,mvrnorm(n=TT,c(5,10),matrix(c(1,0.9,0.9,1),2,2)))
  y1 <- X_fix%*%beta_true + u
  y2 <- X_rand%*%beta_true + u
  beta_hat1 <- solve(t(X_fix)%*%X_fix)%*%t(X_fix)%*%y1
  beta_hat2 <- solve(t(X_rand)%*%X_rand)%*%t(X_rand)%*%y2
  V1[i] <- beta_hat1[2]
  V2[i] <- beta_hat2[2]
}
truehist(V1)
m1 <- mean(V1)
s1 <- sd(V1)
curve(dnorm(x,mean=m1,sd=s1),add=T)

truehist(V2)
m2 <- mean(V2)
s2 <- sd(V2)
curve(dnorm(x,mean=m2,sd=s2),add=T)
\end{verbatim}
\end{solution}

\subsection{Pitfalls in the linear regression model (I)}

A simple linear regression is very easily performed by any statistical
program. However, there are many mistakes and misinterpretations that can be
made. A critical inspection of your regression results is crucial. Three of
the more common mistakes are illustrated in the following.

Load the dataset \texttt{gehaelter.csv} into the object \texttt{gehaelter} and make
the variables accessible. The dataset contains
observations on 100 graduates about their length of study (\texttt{dauer}),
their initial salary (\texttt{gehalt}) and their major (\texttt{fach},
1=chemistry, 2=economics).\footnote{%
The data are fictional and have been generated by a computer algorithm.}

\begin{enumerate}
\item Draw the scatterplot of salary against length of study.

\item Perform a linear regression of salary on length of study and add the
estimated regression line to the scatterplot. What is the effect of the
length of study on the salary?

\item Repeat 1. and 2. separately for chemistry and economics graduates.
What is the effect of the length of study on salary in each group?

\item Re-draw the scatterplot with economists colored in blue and chemists
colored in red.
\end{enumerate}

\begin{solution}
\textbf{Pitfalls in the linear regression model (I)}

Please have a look at the following code:
\begin{verbatim}
#####################################################
#### Pitfalls in the linear regression model (I) ####
#####################################################
gehaelter <- read.csv(file.choose())
names(gehaelter)
head(gehaelter)
str(x)

dauer <- gehaelter$dauer
gehalt <- gehaelter$gehalt
fach <- gehaelter$fach

#1
plot(dauer, gehalt, xlab="Duration", ylab="Salary", main = "Scatterplot")

#2
model <- lm(gehalt~dauer)
model
plot(dauer, gehalt, xlab="Duration", ylab="Salary", main = "Scatterplot")
abline(model)

#3
chem <- lm(gehalt~dauer, data=x[fach==1,])
chem
econ <- lm(gehalt~dauer, data=x[fach==2,])
econ

plot(dauer, gehalt, xlab="Duration", ylab="Salary", main = "Scatter")
points(x[fach==1,2],x[fach==1,1],col="blue") #mark chem students
points(x[fach==2,2],x[fach==2,1], col="red") #mark econ students
abline(model)
abline(chem, col="blue")
abline(econ, col="red")
legend("topright", legend=c("Total","Chemistry", "Economics"), fill = c("black","blue","red"))
\end{verbatim}
\end{solution}

\subsection{Pitfalls in the linear regression model (II)}

Load the dataset \texttt{storch.csv}. It contains observations on the stork
population (eyries) in Lower Saxony and the number of births in Germany from
1958 to 2004.

\begin{enumerate}
\item Plot the scatterplot of the number of births against the number of
storks and perform a linear regression. What is the effect of the number of
storks on the number of births?

\item Repeat the exercise with the number of out-of-wedlock births.
\end{enumerate}

\begin{solution}
\textbf{Pitfalls in the linear regression model (II)}

\begin{verbatim}
######################################################
#### Pitfalls in the linear regression model (II) ####
######################################################
storch <- read.csv2(file.choose())
head(storch)
names(storch)
str(storch)

Horstpaare <- storch$Horstpaare
Geburten <- storch$Geburten
nichtehelich<- storch$nichtehelich

#1)
plot(Horstpaare, Geburten, xlab="Number storks", ylab="Number of births")
regr <- lm(Geburten ~ Horstpaare)
regr
abline(regr)

#2)
regr1 <- lm(nichtehelich ~ Horstpaare)
regr1
plot(Horstpaare, nichtehelich, xlab="Number storks", ylab="Number of out-of-wedlock births")
abline(regr1)
\end{verbatim}
\end{solution}

\subsection{Pitfalls in the linear regression model (III)}

Load the \texttt{indices.csv }(this dataset has been used before, see
exercise \ref{indices}). Execute the following commands:\smallskip

\begin{enumerate}
\item \texttt{n <- dim(indices)[1]}\newline
\texttt{kdax <- dax[6:n]}\newline
\texttt{kftselag <- ftse[1:(n-5)]}

There are two new variables: the DAX index \texttt{kdax} and the FTSE-100
index lagged by five trading days (\texttt{kftselag}).

\item Regress the DAX index \texttt{kdax} on the lagged FTSE index \texttt{%
kftselag}. Interpret the estimated coefficients.

\item Test the null hypothesis that the DAX index does not depend on the
lagged FTSE index (significance level 0.05).
\end{enumerate}

\begin{solution}
\textbf{Pitfalls in the linear regression model (III)}

Please have a look at the following code:
\begin{verbatim}
#######################################################
#### Pitfalls in the linear regression model (III) ####
#######################################################
indices <- read.csv2(file.choose())
head(indices)
names(indices)
str(indices)
dax <- indices$dax
ftse <- indices$ftse

n <- dim(indices)[1]
kdax <- dax[6:n]
kftselag <- ftse[1:(n-5)]

plot(kftselag,kdax)
obj <- lm(kdax ~ kftselag)
obj
summary(obj)
library(AER)
coeftest(obj, vcov=vcovHC)
\end{verbatim}
Be careful with time series data! The assumption of iid in the linear regression model is not valid anymore!
\end{solution}
\newpage


\section{Multivariate random variables\label{multivariate}}

The package \texttt{MASS} includes a command to generate i.i.d. draws from
the multivariate normal distribution. Type \texttt{library(MASS)} to
activate it.

\subsection{Joint distributions}

Consider the bivariate density%
\begin{equation*}
f(x,y)=40\cdot \left( x-0.5\right) ^{2}\cdot y^{3}\cdot \left( 3-2x-y\right)
\end{equation*}%
for $(x,y)\in \lbrack 0,1]\times \lbrack 0,1]$ and $f(x,y)=0$ else.

\begin{enumerate}
\item Show that $f(x,y)$ is really a density function.

\item Derive the marginal densities $f_{X}(x)$ and $f_{Y}(y)$ and plot them.

\item Derive the conditional density of $X$ given $Y=y$ and plot it for $%
y=0.01$ and $y=0.95$.

\item Are $X$ and $Y$ independent?
\end{enumerate}

\begin{solution}
\textbf{Joint distributions}

\begin{enumerate}
\item If $f(x,y)$ is a density function, then the double integral over
    the support must be equal to 1. So, first integrate for x and then
    for y:
\begin{align*}
\int_0^1 f(x,y) dx = \frac{10}{3} y^3 (2-y)\\
\int_0^1 \frac{10}{3} y^3 (2-y) dy = 1
\end{align*}
\item The marginal densities are given by
\begin{align*}
f_Y(y) = \int_0^1 f(x,y) dx = \frac{10}{3} y^3 (2-y)\\
f_X(x) = \int_0^1 f(x,y) dy = -20 x^3 + 42x^2-27x+55
\end{align*}
\item The conditional density of $X$ given $Y=y$ is given by
\begin{align*}
f_X(x|y) =\frac{f(x,y)}{f_Y(y)} = 12(x-0.5)^2 \frac{3-2x-y}{2-y}
\end{align*}
\item X and Y are dependent, because:
\begin{align*}
f(x,y) \neq f_X(x)\cdot f_Y(y)
\end{align*}
\end{enumerate}
\end{solution}

\subsection{Gaussianity or else?}

\label{gaussianity}

Load the dataset \texttt{gaussian.csv} into the object \texttt{gaussian}. Each
column of the dataframe \texttt{gaussian} is a variable (V1, V2, V3, V4).

\begin{enumerate}
\item Split the screen into $2\times 2$ (see exercise \ref{streudiagr}).
Plot the histogram for each variable and add the density of the standard
normal distribution to each histogram. Are the variables normally
distributed?

\item Compute the correlation matrix. Are the variables correlated?

\item Plot the $4\times 4$ matrix of scatterplots (use the command \texttt{%
pairs}). Are the variables independent?

\item Compute the sum $Y=V1+V2+V3+V4$ and plot the histogram of $Y.$ Is the
sum normally distributed?
\end{enumerate}

\begin{solution}
\textbf{Gaussianity or else?}

The program might look like this:
\begin{verbatim}
##############################
#### Gaussianity or else? ####
##############################
#1)
gaussian <- read.csv(file.choose())
View(gaussian)

library(MASS)
x <- seq(-4, 4, length=100)
par(mfrow=c(2,2))

truehist(gaussian$V1); lines(x,dnorm(x))
truehist(gaussian$V2); lines(x,dnorm(x))
truehist(gaussian$V3); lines(x,dnorm(x))
truehist(gaussian$V4); lines(x,dnorm(x))

#2)
cor(gaussian)

#3)
pairs(gaussian)

#4)
Y <- gaussian$V1+gaussian$V2+gaussian$V3+gaussian$V4
par(mfrow=c(1,1))
truehist(Y); lines(x,dnorm(x, mean=mean(Y),sd=sd(Y)))
\end{verbatim}
\begin{enumerate}
  \item Each variable seems to be normally distributed.
  \item There is no (or just a very small) correlation between the
      variables.
  \item The scatterplots show that the variables are NOT independent,
      even though they are normally distributed and uncorrelated. If one
      variable is (in absolute terms) very big, it is very likely that
      the other variables are (in absolute terms) big as well. This is
      dependence!
  \item Compared to a normal distribution the sum has more mass in the
      center of the distribution. Because of the dependence the sum is
      per se not normally distributed.
\end{enumerate}
This exercise illustrates that for the sum of normally distributed variables
being also normally distributed requires the assumption of
\textbf{independence}, not just uncorrelatedness; two separately (not
jointly) normally distributed random variables can be uncorrelated without
being independent, in which case their sum can be non-normally distributed.
\end{solution}

\subsection{Gaussian and uncorrelated, but dependent}

Let $X\sim N(0,1)$ and define%
\begin{equation*}
Y=U\cdot X
\end{equation*}%
where%
\begin{equation*}
U=\left\{
\begin{array}{cc}
-1 & \quad \text{with probability 0.5} \\
1 & \quad \text{with probability 0.5}%
\end{array}%
\right.
\end{equation*}

\begin{enumerate}
\item Determine the distribution of $Y$.

\item Derive the covariance between $X$ and $Y$.

\item Generate a random sample of size $n=1000$ from $(X,Y)^{\prime }$ and
show the scatterplot.

\item For the sample, compute the sum $X+Y$ and plot its histogram.
\end{enumerate}

\begin{solution}
\textbf{Gaussian and uncorrelated, but dependent}

\begin{enumerate}
  \item The distribution of Y is the same as X, because
  \begin{align*}
  &Pr(Y \leq x) = Pr(X\leq x)\cdot Pr(U=1) + Pr(-X\leq x)\cdot Pr(U=-1)\\
  &=\Phi(x) \cdot \frac{1}{2}+ \Phi(x) \cdot \frac{1}{2} = \Phi(x)
  \end{align*}
  since $X$ and $-X$ have the same distribution and $\Phi$ ist the
  distribution function of the normal distribution.
  \item X and Y are uncorrelated, since the covariance is given by
  \begin{align*}
  Cov(X,Y) = E(X\cdot Y) - \underbrace{E(X)}_{=0}\cdot E(Y) = E[E(X \cdot Y|U)] = E[X^2]\cdot \frac{1}{2} + E[-X^2]\cdot \frac{1}{2} = 0
  \end{align*}
  \item The program might look like this
\begin{verbatim}
##################################################
#### Gaussian and uncorrelated, but dependent ####
##################################################
library(MASS)
X <- rnorm(1000)
U <- sample(c(-1, 1), 1000, replace = TRUE)
Y <- U*X
truehist(X)
truehist(Y)
cov(X,Y)
plot(X,Y)
\end{verbatim}
Even though X and Y are normal and uncorrelated, they are not
independent, since it is very likely that if X is large, Y is in absolute
terms also large. In fact: $|Y|=|X|$.
\item The additional code might look like this:
\begin{verbatim}
Z <- X+Y
mean(Z)
par(mfrow=c(1,1))
truehist(Z)
\end{verbatim}
Because X and Y are dependent, the sum is not normally distributed (see
also ex. \emph{Gaussianity or else}).
\end{enumerate}

\end{solution}


\newpage
\section{Stochastic convergence and limit theorems\label{convergence}}

\subsection{Law of large numbers}

Let $X_{1},X_{2},\ldots $ be an i.i.d. sequence of arbitrarily distributed
random variables with finite variance $\sigma ^{2}$. Define the sequence of
random variables%
\begin{equation*}
\bar{X}_{n}=\frac{1}{n}\sum_{i=1}^{n}X_{i}.
\end{equation*}

\begin{enumerate}\setlength{\itemsep}{-2pt}
\item Write an R program to illustrate the law of large numbers.

\item Now suppose that the sequence $X_{1},X_{2},\ldots $ is an $AR(1)$
process:
$$\left( X_{i}-\mu \right) =\rho \left( X_{i-1}-\mu \right) +\varepsilon _{i}$$
where $\varepsilon _{i}\sim iid(0,\sigma _{\varepsilon }^{2})$ is not
necessarily normally distributed and $|\rho |<1$. Show that the law of large
numbers still holds despite the intertemporal dependence.
\end{enumerate}

\begin{solution}
\textbf{Law of large numbers}

The programs might look like this:
\begin{verbatim}
##############################
#### Law of large numbers ####
##############################
#1)
z <- rep(NA,1000); u <- rep(NA,1000); g <- rep(NA,1000)
for (i in 1:1000) {
  z[i] <- mean(rnorm(i,mean=10,sd=2))
  u[i] <- mean(runif(i,min=0,max=1)) #expectation is (a+b)/2
  g[i] <- mean(rgeom(i,prob=0.2)) #expectation is (1-p)/p
}

par(mfrow=c(1,2))

truehist(z,main="Law of large numbers")
plot(z, main="for the normal distribution"); abline(h=10,lwd=2,col="red")

truehist(u,main="Law of large numbers")
plot(u,main="for the uniform distribution"); abline(h=0.5,lwd=2,col="red")

truehist(g,main="Law of large numbers")
plot(g,main="for the geometric distribution"); abline(h=4,lwd=2,col="red")

#2)
z <- rep(NA,1000)
rho=0.8
mu=2
for (i in 1:1000) {
  z[i] <- mean(filter((1-rho)*mu+rnorm(i,sd=2),rho,method="recursive",init=(1-rho)*mu))
  #try and use a different distribution
}
par(mfrow=c(1,2),pty="s")
truehist(z,main="Law of large numbers")
plot(z, main="for intertemporal dependence AR(1)"); abline(h=mu,lwd=2,col="red")
\end{verbatim}
\end{solution}

\subsection{Law of large numbers for the variance}

Let $X_{1},X_{2},\ldots $ be an i.i.d. sequence of arbitrarily distributed
random variables with mean $\mu $, variance $\sigma ^{2}$, and finite
kurtosis, i.e. $E(X_{i}^{4})<\infty $. Define the sequence of random
variables%
\begin{equation*}
S_{n}^{2}=\frac{1}{n}\sum_{i=1}^{n}\left( X_{i}-\bar{X}\right) ^{2}, \quad \text{where } \bar{X}=n^{-1}\sum_{i=1}^{n}X_{i}.
\end{equation*}

\begin{enumerate}\setlength{\itemsep}{-2pt}
\item Write an R program to illustrate that $S_{n}^{2}\rightarrow \sigma
^{2} $ in probability.

\item Now draw the samples from a $t$-distribution with 3 degrees of
freedom, i.e. $X_{i}\overset{iid}{\sim }t_{3}$. The kurtosis of the $t_{3}$%
-distribution is infinite. Use your R program to show that $S_{n}^{2}$ does
no longer converge to $\sigma ^{2}$ in probability.
\end{enumerate}

\begin{solution}
\textbf{Law of large numbers for the variance}

The program might look like this:
\begin{verbatim}
###############################################
#### Law of large numbers for the variance ####
###############################################
#1)
library(MASS)
zn <- rep(NA,1000); zu <- rep(NA,1000); zg <- rep(NA,1000)
for (i in 1:1000) {
  rn <- rnorm(i,mean=10,sd=2) ; variance_n <- 2^2
  ru <- runif(i,min=0,max=6) ; variance_u <- 3
  rg <- rgeom(i,prob=0.2); variance_g <-20
  zn[i] <- sum((rn-mean(rn))^2)/i
  zu[i] <- sum((ru-mean(ru))^2)/i
  zg[i] <- sum((rg-mean(rg))^2)/i
}

par(mfrow=c(1,2))
truehist(zn,main="Law of large numbers")
plot(zn, main="for the variance (normal distrib.)"); abline(h=variance_n,lwd=2,col="red")

truehist(zn,main="Law of large numbers")
plot(zu, main="for the variance (uniform distrib.)"); abline(h=variance_u,lwd=2,col="red")

truehist(zn,main="Law of large numbers")
plot(zg, main="for the variance (geometr. distrib.)"); abline(h=variance_g,lwd=2,col="red")

#2)
zt <- rep(NA,1000)
for (i in 1:1000) {
  xt <- rt(i,df=3)
  zt[i] <- sum((xt-mean(xt))^2)/i; variance_t <-3/(3-2)
}

par(mfrow=c(1,2))

truehist(zt,main="Law of large numbers")
plot(zt, main="for the variance (t-distrib. df=3)");abline(h=variance,lwd=2,col="red")
\end{verbatim}
\end{solution}

\subsection{Central limit theorem}

Let $X_{1},X_{2},\ldots $ be an i.i.d. sequence of arbitrarily distributed
random variables with mean $\mu $ and finite variance $\sigma ^{2}$. Define
the sequences of random variables%
\begin{eqnarray*}
Y_{n} =\sum_{i=1}^{n}X_{i}, \qquad Z_{n} =\sqrt{n}\frac{\left( \frac{1}{n}Y_{n}\right) -\mu }{\sigma }.
\end{eqnarray*}

\begin{enumerate}\setlength{\itemsep}{-1pt}
\item Write an R program to illustrate the central limit theorem.

\item Show that the central limit theorem still holds if we replace the
standard deviation $\sigma $ in the denominator of $Z_{n}$ by the estimated
standard deviation
\begin{equation*}
S_{n}=\sqrt{\frac{1}{n}\sum_{i=1}^{n}\left( X_{i}-\bar{X}\right) ^{2}}.
\end{equation*}

\item Now let $X_{1},X_{2},\ldots $ be an i.i.d. sequence of $t$-distributed
random variables with 1.5 degrees of freedom. Show that the convergence in
distribution breaks down.
\end{enumerate}

\begin{solution}
\textbf{Central limit theorem}

The program might look like this:
\begin{verbatim}
###############################
#### Central limit theorem ####
###############################
#1) Illustration of the central limit theorem for the uniform distribution
#    with sigma as the standard deviation
library(MASS)
N <- 10000 # how many times to draw individual X_i's, note that i = 1,2,..,n
expect <- 0.5
vari <- 1/12
par(mfrow = c(1,2),pty = "s")
for(n in 1:20) {
  X <- runif(N*n) #N*n random numbers
  M <- matrix(X, N, n) #matrix of N rows and n columns filled with random numbers
  Yn <- rowSums(M) # calculate the sum of n random numbers for N rows
  Zn <- (Yn/n - expect)*sqrt(n/vari) #standardization
  #display the sequence of random variables
  truehist(Zn, xlim = c(-4,4),ylim = c(0,0.5), main = paste("n =", toString(n),sep =" "))
  coord <- par("usr")
  # par("usr") gives you a vector of the form c(x1, x2, y1, y2)
  # giving the extremes of the coordinates of the plotting region
  x <- seq(coord[1], coord[2], length.out = 500)
  lines(x, dnorm(x), col = "red")
  qqnorm(Zn, ylim = c(-4,4), xlim = c(-4,4), pch = ".", col = "blue")
  abline(0, 1, col = "red")
  Sys.sleep(1)
}

#2) Illustration of the central limit theorem for the uniform distribution
#     with the estimated standard deviation
library(MASS)
N <- 10000 # how many times to draw individual X_i's, note that i = 1,2,..,n
expect <- 0.5
par(mfrow = c(1,2),pty = "s")
#hint: it needs to start with n=2, for n=1 you get vari=0 and you cannot divide by 0
for(n in 2:20) {
  X <- runif(N*n) #N*n random numbers
  M <- matrix(X, N, n) #matrix of N rows and n columns filled with random numbers
  vari <- 1/n * rowSums((M-rowMeans(M))^2)
  Yn <- rowSums(M) # calculate the sum of n random numbers for N rows
  Zn <- (Yn/n - expect)*sqrt(n/vari) #standardization
  #display the sequence of random variables
  truehist(Zn, xlim = c(-4,4),ylim = c(0,0.5), main = paste("n =", toString(n),sep =" "))
  coord <- par("usr")
  x <- seq(coord[1], coord[2], length.out = 500)
  lines(x, dnorm(x), col = "red")
  qqnorm(Zn, ylim = c(-4,4), xlim = c(-4,4), pch = ".", col = "blue")
  abline(0, 1, col = "red")
  Sys.sleep(1)
}

#3)
library(MASS)
N <- 10000 # how many times to draw individual X_i's, note that i = 1,2,..,n
expect <- 0
par(mfrow = c(1,2),pty = "s")
#hint: it needs to start with n=2, for n=1 you get vari=0 and you cannot divide by 0
for(n in 2:20) {
  X <- rt(N*n,df=1.5)
  M <- matrix(X, N, n)
  vari <- 1/n * rowSums((M-rowMeans(M))^2)
  Yn <- rowSums(M)
  Zn <- (Yn/n - expect)*sqrt(n/vari)
  truehist(Zn, xlim = c(-4,4),ylim = c(0,0.5), main = paste("n =", toString(n),sep =" "))
  coord <- par("usr")
  x <- seq(coord[1], coord[2], length.out = 500)
  lines(x, dnorm(x), col = "red")
  qqnorm(Zn, ylim = c(-4,4), xlim = c(-4,4), pch = ".", col = "blue")
  abline(0, 1, col = "red")
  Sys.sleep(1)
}
\end{verbatim}
\end{solution}


\subsection{Central limit theorem for dependent data}
Suppose that the sequence $X_{1},X_{2},\ldots $ is an $AR(1)$ process, i.e.
$$\left( X_{i}-\mu \right) =\rho \left( X_{i-1}-\mu \right) +\varepsilon _{i}$$
where $\varepsilon _{i}\sim iid(0,\sigma _{\varepsilon }^{2})$ is not
necessarily normally distributed and $|\rho |<1$.

\begin{enumerate}
  \item Show that $X_i$ has mean equal to $\mu $ and finite variance equal to $\sigma_\varepsilon^2/(1-\rho^2)$.
  \item To derive the asymptotic distribution of the sample mean, do the following steps:
  \begin{enumerate}
    \item Derive the asymptotic distribution of $\frac{1}{\sqrt{n} } \sum_{i=1}^n \varepsilon_i$
    \item Show that
    \begin{align*}
    \frac{1}{\sqrt{n}} \sum_{i=1}^n \varepsilon_i = \sqrt{n}\left[(1-\rho)\left(\frac{1}{n}Y_n-\mu\right) + \rho\left(\frac{X_n - X_0}{n}\right)\right]
    \end{align*}
      with $Y_{n} =\sum_{i=1}^{n}X_{i}$.
    \item Show that
    \begin{align*}
        \textsl{plim}\left[\frac{\rho}{1-\rho}\left(\frac{X_n - X_0}{\sqrt{n}}\right)\right] = 0
    \end{align*}
    \emph{Hint: Use Tchebychev's Inequality.}
    \item Put your results of (a),(b) and (c) together and derive the asymptotic distribution of the sample mean. That is, show that
    \begin{align*}
    Z_{n} =\sqrt{n}\frac{\left( \frac{1}{n}Y_{n}\right) -\mu }{\sigma} \overset{d}{\rightarrow} U \sim N(0,1)
    \end{align*}
    for $\sigma=\sqrt{\sigma_\varepsilon^2/(1-\rho)^2}$.
  \end{enumerate}
  \item Write an R program to demonstrate the central limit theorem for the AR(1) process.
\end{enumerate}

\begin{solution}
\textbf{Central limit theorem for dependent data}

\begin{enumerate}
  \item First let's derive the expectation and variance of the AR(1) process with $|\rho|<1$. For this, we use recursive substitution techniques given a starting value $X_0$:
    \begin{align*}
      X_i = (1-\rho)(1+\rho+\rho^2+\dots+\rho^N)\mu + \varepsilon_i + \rho \varepsilon_{i-1} + \rho^2 \varepsilon_{i-2} + \dots + \rho^N \varepsilon_{i-N} + \rho^{N+1} X_0
    \end{align*}
    Note that $\lim_{N\rightarrow \infty} \rho^{N+1} = 0$ and $\lim_{N\rightarrow \infty} \sum_{j=0}^\infty \rho^j = \frac{1}{1-\rho}$, since $|\rho|<1$. The AR(1) process with $|\rho|<1$ can therefore be equally represented by
    \begin{align*}
    X_i = \mu + \sum_{j=1}^\infty \rho^j \varepsilon_{i-j}
    \end{align*}
    Its expectation and variance are then equal to
    \begin{align*}
    E(X_i) &= \mu + \sum_{j=1}^\infty \rho^j E(\varepsilon_{i-j}) = \mu\\
    Var(X_i) &= \sum_{j=1}^\infty (\rho^j)^2 var(\varepsilon_{i-j}) = \sum_{j=1}^\infty (\rho^2)^j \sigma_\varepsilon^2 = \frac{\sigma_\varepsilon^2}{1-\rho^2}
    \end{align*}

  \item Asymptotic distribution for mean
  \begin{enumerate}
    \item Due to our assumptions on $\varepsilon_i$, we can use the central limit theorem such that
        \begin{equation*}
            \sqrt{n} \left(\frac{1}{n} \sum_{i=1}^n \varepsilon_i \right) = \frac{1}{\sqrt{n}} \sum_{i=1}^n \varepsilon_i  \overset{d}{\rightarrow} U_\varepsilon \sim N(0,\sigma_\varepsilon^2)
        \end{equation*}
    \item Let's have a look at $\frac{1}{\sqrt{n}} \sum_{i=1}^n \varepsilon_i$:
        \begin{align*}
        \frac{1}{\sqrt{n}} \sum_{i=1}^n \varepsilon_i
        &= \frac{1}{\sqrt{n}} \sum_{i=1}^n \left[(X_i-\mu)-\rho(X_{i-1}-\mu)\right]\\
        &= \frac{1}{\sqrt{n}} \left[\sum_{i=1}^n (X_i-\mu)- \rho\sum_{i=1}^n(X_{i-1}-\mu)\right]\\
        &= \frac{1}{\sqrt{n}} \left[\sum_{i=1}^n (X_i-\mu)- \rho\left[\sum_{i=1}^n(X_{i}-\mu)-(X_n - X_0)\right]\right]\\
        &= \sqrt{n} \left[\frac{1}{n}\sum_{i=1}^n (X_i-\mu)- \rho\left[\frac{1}{n}\sum_{i=1}^n(X_{i}-\mu)-\left(\frac{X_n - X_0}{n}\right)\right]\right]\\
        &= \sqrt{n} \left[\frac{1}{n}Y_n-\mu- \rho\left[\frac{1}{n}Y_n-\mu-\left(\frac{X_n - X_0}{n}\right)\right]\right]\\
        &= \sqrt{n}\left[(1-\rho)\left(\frac{1}{n}Y_n-\mu\right) + \rho\left(\frac{X_n - X_0}{n}\right)\right]
        \end{align*}

    \item Using the definition of the probability limit, we have to show that for any $\delta>0$
        \begin{align*}
          \lim_{n\rightarrow \infty} P\left(\left|\frac{\rho}{1-\rho}\left(\frac{X_n - X_0}{\sqrt{n}}\right)\right|> \delta\right) = 0
        \end{align*}
        By Tchebychev's Inequality we have
        \begin{align*}
        P\left(\left|\frac{\rho}{1-\rho}\left(\frac{X_n - X_0}{\sqrt{n}}\right)\right|> \delta\right) \leq \frac{1}{\delta^2} var\left[\frac{\rho}{1-\rho}\left(\frac{X_n - X_0}{\sqrt{n}}\right)\right]
        \end{align*}
        Let's have a look at $var\left[\frac{\rho}{1-\rho}\left(\frac{X_n - X_0}{\sqrt{n}}\right)\right]$:
        \begin{align*}
        var\left[\frac{\rho}{1-\rho}\left(\frac{X_n - X_0}{\sqrt{n}}\right)\right]
        &= \frac{1}{n}\left(\frac{\rho}{1-\rho}\right)^2 var(X_n - X_0)\\
        &= \frac{1}{n}\left(\frac{\rho}{1-\rho}\right)^2 \left(var(X_n) + var(X_0) - 2 cov(X_n,X_0)\right]\\
        &= \frac{1}{n}\left(\frac{\rho}{1-\rho}\right)^2 \left[\frac{\sigma_\varepsilon^2}{1-\rho^2} + \frac{\sigma_\varepsilon^2}{1-\rho^2} - 2 corr(X_n,X_0)\sqrt{\frac{\sigma_\varepsilon^2}{1-\rho^2}}\sqrt{\frac{\sigma_\varepsilon^2}{1-\rho^2}} \right]\\
        &\leq \frac{1}{n}\left(\frac{\rho}{1-\rho}\right)^2 4 \left(\frac{\sigma_\varepsilon^2}{1-\rho^2}\right)
        \end{align*}
        since $corr(X_n,X_0) \geq -1$.

        Thus for any $\delta>0$, we have
        \begin{align*}
        P\left(\left|\frac{\rho}{1-\rho}\left(\frac{X_n - X_0}{\sqrt{n}}\right)\right|> \delta\right) \leq \frac{1}{\delta^2} \frac{1}{n}\left(\frac{\rho}{1-\rho}\right)^2 4 \left(\frac{\sigma_\varepsilon^2}{1-\rho^2}\right)
        \end{align*}
        In the limit
        \begin{align*}
        \lim_{n\rightarrow \infty} P\left(\left|\frac{\rho}{1-\rho}\left(\frac{X_n - X_0}{\sqrt{n}}\right)\right|> \delta\right) = 0.
        \end{align*}

    \item Now, let's go back to
            \begin{align*}
            \frac{1}{\sqrt{n}} \sum_{i=1}^n \varepsilon_i = \sqrt{n}\left[(1-\rho)\left(\frac{1}{n}Y_n-\mu\right) + \rho\left(\frac{X_n - X_0}{n}\right)\right]
            \end{align*}
            Let's divide by $(1-\rho)$
        \begin{align*}
        \frac{\frac{1}{\sqrt{n}} \sum_{i=1}^n \varepsilon_i}{1-\rho} & = \sqrt{n}\left[\frac{1}{n}Y_n-\mu\right] + \frac{\rho}{1-\rho}\left(\frac{X_n - X_0}{\sqrt{n}}\right)
        \end{align*}

        For the left-hand-side we have
        \begin{align*}
        \frac{\frac{1}{\sqrt{n}} \sum_{i=1}^n \varepsilon_i}{1-\rho} \overset{d}{\rightarrow} \tilde{U_\varepsilon} \sim N\left(0,\frac{\sigma_\varepsilon^2}{(1-\rho)^2}\right)
        \end{align*}

        Since $\textsl{plim}\left[\frac{\rho}{1-\rho}\left(\frac{X_n - X_0}{\sqrt{n}}\right)\right] = 0$, we have
        \begin{align*}
        \sqrt{n}\left[\frac{1}{n}Y_n-\mu\right] \overset{d}{\rightarrow} \tilde{U} \sim N\left(0,\frac{\sigma_\varepsilon^2}{(1-\rho)^2}\right)
        \end{align*}
        and we're done. That is, set $\sigma^2 = \frac{\sigma_\varepsilon^2}{(1-\rho)^2}$, then
        \begin{align*}
        Z_n = \sqrt{n}\frac{\left(\frac{1}{n}Y_n\right)-\mu}{\sigma} \overset{d}{\rightarrow} U \sim N(0,1)
        \end{align*}
  \end{enumerate}
  \item The program might look like this:

\begin{verbatim}
################################################
### Central limit theorem for dependent data ###
################################################
# AR(1) process
library(MASS)
n <- 10000
N <- 5000 # how many times to draw individual X_i's, note that i = 1,2,..,n
mu <- 0
rho=0.8
sigma_eps <- 0.5 #this is the standard deviation of the random term in the AR(1) process
var_x <- sigma^2/(1-rho^2) # analytical variance of an AR(1)-process
factr <- var_x*(1+rho)/(1-rho) # this is the required adjustment
X <- matrix(NA,N,n)
for (j in 1:N) {
  X[j,] <- filter((1-rho)*mu+rnorm(n,sd=sigma),rho,method="recursive",init=(1-rho)*mu)
}
Yn <- rowSums(X)
Zn <- sqrt(n)*(Yn/n - mu)/sqrt(factr) #standardization
truehist(Zn, main = paste("n =", toString(n),sep =" "))
coord <- par("usr")
x <- seq(coord[1], coord[2], length.out = 500)
lines(x, dnorm(x), col = "red")
\end{verbatim}
\end{enumerate}



\end{solution}



\subsection{Delta method\label{deltamethod}}

A very important linear transformation is a first order Taylor
approximation. First, we consider the univariate case. Suppose $X_1,X_2,\dots$ are iid with common mean $\mu$ and variance $\sigma^{2}$. Define $\bar{X}_n=(X_1+\dots+X_n)/n$.

\begin{enumerate}
\item  Derive the asymptotic distribution of $$\sqrt{n}(f(\bar{X_n})-f(\mu)),$$ where $f$ is differentiable (at least at $\mu $) and its derivative is continuous.
\item Now turn to the multivariate case and let $X_1,X_2,\dots$ be random iid vectors of length $K$ with common mean vector $\mu$ and covariance matrix $\Sigma$. Define $Y=f(\bar{X}_n)$ where $f$ is a scalar valued differentiable function.\footnote{%
Of course, one could also consider vector-valued functions.} Denote the gradient of $f$ as $D_{f}$ and suppose it is continuous. Write down the first order Taylor approximation of $f$ around $\mu $ and determine the approximate distribution of $Y.$
\end{enumerate}

\begin{solution}
\textbf{Delta method}

\begin{enumerate}
  \item According to a central limit theorem, we have
  $$\sqrt{n}\left(\bar{X}_n-\mu\right) \overset{d}{\rightarrow} U \sim N(0,\sigma^2).$$ The first order Taylor expansion of $f(\bar{X}_n)$ around $\mu $ is%
\begin{equation*}
f\left( \bar{X}_n \right) \approx f\left( \mu \right) +f^{\prime }\left( \mu_n  \right) \left(
\bar{X}_n-\mu \right),
\end{equation*}
where $\mu_n$ is an intermediate point between X and $\mu$. Since $|\mu_n-\mu|\leq |\bar{X}_n-\mu|$ and $X_n \overset{p}{\rightarrow} \mu$, $\mu_n \overset{p}{\rightarrow}\mu$. Since $f'$ is continuous we have $f'(\mu_n) \overset{p}{\rightarrow} f'(\mu)$.

We see that $f(\bar{X}_n)$ is just a linear transformation of $\bar{X}_n$. Since $\bar{X}_n$ is asymptotically normal, so is $f(\bar{X}_n)$. The asymptotic mean is equal to
\begin{equation*}
E\left( f(\bar{X}_n)\right) = f\left( \mu \right) +f^{\prime }\left( \mu
\right) \left( E\left(X\right)-\mu \right) = f\left( \mu \right)
\end{equation*}%
and the asymptotic variance is%
\begin{eqnarray*}
Var\left( f(\bar{X}_n)\right) &=&Var\left( f\left( \mu \right) +f^{\prime }\left(
\mu \right) \left( X-\mu \right) \right) \\
&=&\left[ f^{\prime }\left( \mu \right) \right] ^{2}Var\left( X\right)= \left[ f^{\prime }\left( \mu \right) \right] ^{2}\sigma ^{2}.
\end{eqnarray*}
So we have
$$\sqrt{n}\left(f(\bar{X}_n)-f(\mu)\right)=f'(\mu_n)\left[\sqrt{n}\left(\bar{X}_n-\mu\right)\right] \overset{d}{\rightarrow} f'(\mu)U \sim N(0,[f'(\mu)]^2\sigma^2)$$
\item The first order Taylor expansion of $f$ around $\mu $ is%
\begin{equation*}
Y \approx f\left( \mu \right) +D_f\left( \mu \right) \left(
\bar{X_n}-\mu \right)
\end{equation*}
Because Y is a linear transformation of $\bar{X_n}$, and $\bar{X_n}$ is asymptotically normal, so is $Y$. The asymptotic mean of $Y$ is%
\begin{equation*}
E\left( Y\right) = f\left( \mu \right)
\end{equation*}%
and the asymptotic variance is%
\begin{eqnarray*}
Var\left( Y\right) &=&Var\left( f\left( \mu \right) +D_f\left(\mu\right) \left( \bar{X_n}-\mu \right) \right) \\
&=&D_f\left( \mu \right) Var\left( \bar{X_n}\right)D_f\left( \mu \right)' \\
&\rightarrow & D_f\left( \mu \right)\Sigma D_f\left( \mu \right)'.
\end{eqnarray*}
\end{enumerate}

\end{solution}


\subsection{Limits of maxima (I)}

Let $X_{1},X_{2},\ldots $ be an i.i.d. sequence of standard normally
distributed random variables. Define the random variable%
\begin{equation*}
M_{n}=\max_{i=1,\ldots ,n}X_{i}
\end{equation*}%
and its normalized version $R_{n}=(M_{n}-d_{n})/c_{n}\ $where
\begin{eqnarray*}
d_{n} &=&\sqrt{2\ln n}-\frac{\ln \left( 4\pi \right) +\ln \ln n}{2\sqrt{%
\left( 2\ln n\right) }} \\
c_{n} &=&\left( 2\ln n\right) ^{-1/2}.
\end{eqnarray*}

\begin{enumerate}
\item Write an R program to illustrate that $R_{n}$ converges in
distribution.

\item The limit distribution of $R_{n}$ is the Gumbel distribution. Add the
Gumbel density $\exp \left( -x-e^{-x}\right) $ to a histogram of $R_{n}$.
\end{enumerate}

\begin{solution}
\textbf{Limits of maxima (I)}

The program might look like this:
\begin{verbatim}
############################
### Limits of maxima (I) ###
############################
library(MASS)
n <- 100
N <- 1000 # how many times to draw individual X_i's, note that i = 1,2,..,n
X <- matrix(rnorm(N*n),N, n) #matrix of N rows and n columns filled with random numbers
dn <- sqrt(2*log(n))-(log(4*pi)+log(log(n)))/(2*sqrt(2*log(n)))
cn <- (2*log(n))^(-1/2)
Mn <- apply(X, 1, max) # get max of each row
Rn <- (Mn - dn)/cn #standardization
#display the sequence of random variables
truehist(Rn, main = paste("n =", toString(n),sep =" "))
coord <- par("usr")
x <- seq(coord[1], coord[2], length.out = 500)
lines(x, exp(-x-exp(-x)), col = "red")
\end{verbatim}
See also the Fisher-Tippett-Galambos Theorem which states that a sample of iid random variables after proper standardization can only converge in distribution to one of 3 possible distributions: the Gumbel distribution, the Fréchet distribution, or the Weibull distribution.
\end{solution}


\subsection{Limits of maxima (II)}

Let $X_{1},X_{2},\ldots $ be an i.i.d. sequence of $t$-distributed random
variables with 1.5 degrees of freedom. Define the random variables%
\begin{equation*}
M_{n}=\max_{i=1,\ldots ,n}X_{i}
\end{equation*}%
and its normalized version $R_{n}=M_{n}/c_{n}\ $with
\begin{equation*}
c_{n}=F_{t_{1.5}}^{-1}\left( 1-\frac{1}{n}\right)
\end{equation*}%
where $F_{t_{1.5}}^{-1}$ is the quantile function of the $t_{1.5}$%
-distribution (see the R command \texttt{qt}).

\begin{enumerate}
\item Write an R program to illustrate that $R_{n}$ converges in
distribution.

\item The limit distribution of $R_{n}$ is the Frechet distribution (with
tail index 1.5). Add the Frechet density $1.5x^{-2.5}\exp \left(
-x^{-1.5}\right) $ to a histogram of $R_{n}$.
\end{enumerate}

\begin{solution}
\textbf{Limits of maxima (II)}
The program might look like this:
\begin{verbatim}
###############################
#### Limits of maxima (II) ####
###############################
library(MASS)
n <- 100
N <- 1000 # how many times to draw individual X_i's, note that i = 1,2,..,n
X <- matrix(rt(n*N,df=1.5), N, n) #matrix of N rows and n columns filled with random numbers
Mn <- apply(X, 1, max) # get max of each row
Rn <- Mn/qt(1-1/n,1.5) #standardization
#display the sequence of random variables
truehist(Rn, main = paste("n =", toString(n),sep =" "))
coord <- par("usr")
x <- seq(coord[1], coord[2], length.out = 500)
lines(x, 1.5*x^(-2.5)*exp(-x^(-1.5)), col = "red")
\end{verbatim}
See also the Fisher-Tippett-Galambos Theorem which states that a sample of iid random variables after proper standardization can only converge in distribution to one of 3 possible distributions: the Gumbel distribution, the Fréchet distribution, or the Weibull distribution.
\end{solution}

\subsection{Limits of maxima (III)}

Let $X_{1},X_{2},\ldots $ be an i.i.d. sequence of random variables
uniformly distributed on the interval $[0,1]$. Define the random variables%
\begin{equation*}
M_{n}=\max_{i=1,\ldots ,n}X_{i}
\end{equation*}%
and its normalized version $R_{n}=(M_{n}-d_{n})/c_{n}\ $where
\begin{eqnarray*}
d_{n} &=&1 \\
c_{n} &=&\frac{1}{n}.
\end{eqnarray*}

\begin{enumerate}
\item Write an R program to illustrate that $R_{n}$ converges in
distribution.

\item The limit distribution of $R_{n}$ is the Weibull distribution. Add the
Weibull density $\exp \left( x\right) $ to a histogram of $R_{n}$.
\end{enumerate}

\begin{solution}
\textbf{Limits of maxima (III)}

The program might look like this:
\begin{verbatim}
################################
#### Limits of maxima (III) ####
################################
library(MASS)
n <- 100
N <- 1000 # how many times to draw individual X_i's, note that i = 1,2,..,n
X <- matrix(runif(n*N), N, n) #matrix of N rows and n columns filled with random numbers
Mn <- apply(X, 1, max) # get max of each row
dn <- 1
cn <- 1/n
Rn <- (Mn-dn)/cn #standardization
#display the sequence of random variables
truehist(Rn, main = paste("n =", toString(n),sep =" "))
coord <- par("usr")
x <- seq(coord[1], coord[2], length.out = 500)
lines(x, exp(x), col = "red")
\end{verbatim}
See also the Fisher-Tippett-Galambos Theorem which states that a sample of iid random variables after proper standardization can only converge in distribution to one of 3 possible distributions: the Gumbel distribution, the Fréchet distribution, or the Weibull distribution.
\end{solution}

\newpage
\section{Estimators and their properties\label{estimators}}

\subsection{Counter examples}

Let $X_{1},X_{2},\ldots $ be a sample from some random variable $X$ with $%
E(X)=\mu $ and $Var(X)=1$. While the variance is known, we would like to
estimate the expectation $\mu $.

\begin{enumerate}
\item Give an example of an estimator that is unbiased but inconsistent.

\item Give an example of an estimator that is biased but consistent.

\item Give an example of an estimator that is asymptotically biased but
consistent.
\end{enumerate}

\begin{solution}
\textbf{Counter examples}

We assume in addition that $X$ is normally distributed, $X\sim N\left( \mu
,1\right) $.

\begin{enumerate}
\item The sequence of estimators $\hat{\mu}_{n}=X_{1}$ is unbiased since $%
E\left( \hat{\mu}_{n}\right) =E\left( X_{1}\right) =\mu $. But it is
obviously not consistent.

\item The sequence of estimators $\hat{\mu}_{n}=\left( \frac{1}{n}%
\sum_{i=1}^{n}X_{i}\right) +1/n$ is biased with bias $1/n$. For given $n$
\begin{eqnarray*}
E\left( \hat{\mu}_{n}\right)  &=&\mu +\frac{1}{n} \\
Var\left( \hat{\mu}_{n}\right)  &=&\frac{1}{n},
\end{eqnarray*}%
and thus $\lim_{n\rightarrow \infty }E\left( \hat{\mu}_{n}\right) =\mu $
and $\lim_{n\rightarrow \infty }Var\left( \hat{\mu}_{n}\right) =0$.
\textbf{These are the sufficient conditions for consistency.}

\item This is the most tricky case. Consider the sequence of estimators%
\begin{equation*}
\hat{\mu}_{n}=\mathbf{1}\left( X_{1}-\frac{1}{n}\sum_{i=1}^{n}X_{i}<\Phi
^{-1}\left( 1/n\right) \right) \cdot n+\left[ 1-\mathbf{1}\left( X_{1}-\frac{%
1}{n}\sum_{i=1}^{n}X_{i}<\Phi ^{-1}\left( 1/n\right) \right) \right] \cdot
\frac{1}{n}\sum_{i=1}^{n}X_{i}
\end{equation*}%
where $\mathbf{1}\left( \cdot \right) $ is an indicator function which is 1
if the condition is true and 0 else, $\Phi ^{-1}\left( 1/n\right) $ is the $%
1/n$-quantile of the standard normal distribution. For large $n$, the term $%
X_{1}-\frac{1}{n}\sum_{i=1}^{n}X_{i}$ is approximately $N\left( 0,1\right) $
and the indicator variable will be 1 with probability $1/n$. Thus, the
expectation of the first summand of $\hat{\mu}_{n}$ is%
\begin{equation*}
E\left( \mathbf{1}\left( X_{1}-\frac{1}{n}\sum_{i=1}^{n}X_{i}<\Phi
^{-1}\left( 1/n\right) \right) \cdot n\right) =\frac{1}{n}\cdot n=1.
\end{equation*}%
The expectation of the second summand is%
\begin{equation*}
E\left( \left[ 1-\mathbf{1}\left( X_{1}-\frac{1}{n}\sum_{i=1}^{n}X_{i}<\Phi
^{-1}\left( 1/n\right) \right) \right] \cdot \frac{1}{n}\sum_{i=1}^{n}X_{i}%
\right) =\left( 1-\frac{1}{n}\right) \cdot \mu ,
\end{equation*}%
and hence the asymptotic bias of $\hat{\mu}_{n}$ is $1$. The estimator is
consistent because, as $n\rightarrow \infty $, the probability mass
concentrates on the second summand which converges to $\mu $.
\end{enumerate}
\end{solution}

\newpage
\section{Least Squares and Method of Moments\label{lsandmm}}

\subsection{Nonlinear least squares\label{nls1}}

\begin{enumerate}
\item Consider the exponential model%
\begin{equation*}
y_{i}=\exp \left( \alpha +\beta x_{i}\right) +u_{i}
\end{equation*}%
where $u_{i}\sim N(0,\sigma ^{2})$. Since the error term is additive one
cannot simply take logarithms to make the model linear. Load the dataset
\texttt{expgrowth.csv} from the course site and estimate the parameters $%
\alpha $ and $\beta $ by minimizing%
\begin{equation*}
\sum_{i=1}^{n}\left( y_{i}-\exp \left( a+bx_{i}\right) \right) ^{2}
\end{equation*}%
numerically with respect to $a$ and $b$.

\item Consider the following example from Davidson and MacKinnon (2004),%
\begin{equation*}
y_{t}=\beta _{1}+\beta _{2}x_{t1}+\frac{1}{\beta _{2}}x_{t2}+u_{t}.
\end{equation*}%
Assume that $u_{t}\sim N(0,1)$. Load the dataset \texttt{DMacK1.csv} and
estimate the parameters $\beta _{1}$ and $\beta _{2}$.
\end{enumerate}

\begin{solution}
\textbf{Nonlinear least squares}

\item The two programs could look like this:

\begin{verbatim}
############################################
#### Nonlinear least squares estimation ####
############################################

#1)
# Load the dataset
dat <- read.csv(file.choose())

# Definition of the objective function
# The first argument must be the parameter vector
squarediffs <- function(param,dat) {
  alpha <- param[1]
  beta <- param[2]
  u <- dat$y-exp(alpha+beta*dat$x)
  return(sum(u^2))
}

# Minimize the objective function
obj <- optim(c(1,0),squarediffs,dat=dat)
estimates <- obj$par
alphahat <- estimates[1]
betahat <- estimates[2]

# Plot the data and the estimated regression curve
plot(dat$x,dat$y)
g <- seq(0,40,length=500)
lines(g,exp(alphahat+betahat*g))


#2)

# Load the dataset
dat <- read.csv(file.choose())

# Definition of the objective function
# The first argument must be the parameter vector
squarediffs <- function(param,dat) {
  beta1 <- param[1]
  beta2 <- param[2]
  u <- dat$y-(beta1+beta2*dat$x1+1/beta2*dat$x2)
  return(sum(u^2))
}

# Minimize the objective function
obj <- optim(c(1,1),squarediffs,dat=dat)
estimates <- obj$par
beta1hat <- estimates[1]
beta2hat <- estimates[2]

# Plot the data and the estimated regression plane
library(rgl)
plot3d(dat$x1,dat$x2,dat$y)
gx1 <- seq(min(dat$x1),max(dat$x1),length=60)
gx2 <- seq(min(dat$x2),max(dat$x2),length=60)
yhat <- outer(gx1,gx2,function(x1,x2) beta1hat+beta2hat*x1+1/beta2hat*x2)
surface3d(gx1,gx2,yhat,col="light green")
\end{verbatim}
\end{solution}

\subsection{Method of moments for the binomial distribution}

Consider the binomial distribution $Binom(n,\theta )$ with parameters $n>0$
and $0<\theta <1$. The expectation and variance of $X\sim Binom(n,\theta )$
are%
\begin{eqnarray*}
E(X) &=&n\theta \\
Var(X) &=&n\theta \left( 1-\theta \right) .
\end{eqnarray*}%
Derive the moment estimators of $n$ and $\theta $. Ignore the restriction $%
n\in \mathbb{N}$.

\begin{solution}
\textbf{Method of moments for the binomial distribution}

The first step is already given in the text,%
\begin{eqnarray*}
\mu _{1} &=&n\theta  \\
\mu _{2}^{\prime } &=&n\theta \left( 1-\theta \right) .
\end{eqnarray*}%
The second step is the inversion of the two equations,%
\begin{eqnarray*}
\theta  &=&1-\frac{\mu _{2}^{\prime }}{\mu _{1}} \\
n &=&\frac{\mu _{1}}{\theta }.
\end{eqnarray*}%
Finally, the theoretical moments are replaced by their empirical
counterparts, and the moment estimators are%
\begin{eqnarray*}
\hat{\theta} &=&1-\frac{\hat{\mu}_{2}^{\prime }}{\hat{\mu}_{1}} \\
\hat{n} &=&\frac{\hat{\mu}_{1}}{\hat{\theta}}.
\end{eqnarray*}
\end{solution}

\subsection{Method of moments for the geometric distribution}

Consider the geometric distribution with parameter $\lambda $. The
expectation of $X\sim Geom(\lambda )$ is $E(X)=1/\lambda $.

\begin{enumerate}
\item Give a moment estimator of $\lambda .$

\item Explain why the moment estimator is biased.

\item Explain why the moment estimator is consistent.
\end{enumerate}

\begin{solution}
\textbf{Method of moments for the geometric distribution}

\begin{enumerate}
\item From $\mu _{1}=\lambda ^{-1}$ we derive $\lambda =\mu _{1}^{-1}$, and
hence the moment estimator of $\lambda $ is $\hat{\lambda}=\hat{\mu}_{1}^{-1}
$.

\item The empirical moment $\hat{\mu}_{1}$ is an unbiased estimator of $\mu
_{1}$. The estimator $\hat{\lambda}=1/\hat{\mu}_{1}$ is a nonlinear (convex)
transformation. According to Jensen's inequality $E\left( \hat{\lambda}%
\right) =E(1/\hat{\mu}_{1})>1/E(\hat{\mu}_{1})=\lambda $.

\item The rules of calculus for the probability limit are simple,%
\begin{equation*}
\textsl{plim}\hat{\lambda}=\textsl{plim}\frac{1}{\hat{\mu}_{1}}=\frac{1}{%
\textsl{plim}\hat{\mu}_{1}}=\frac{1}{\mu _{1}}=\lambda .
\end{equation*}
\end{enumerate}
\end{solution}

\subsection{Method of moments for the Gumbel distribution}

Consider the Gumbel distribution (also called extreme value distribution)
with parameters $\alpha \in \mathbb{R}$ and $\beta \in \mathbb{R}$. The
expectation and variance of $X\sim Gumbel(\alpha ,\beta )$ are%
\begin{eqnarray*}
E(X) &=&\alpha +0.5772\cdot \beta \\
Var(X) &=&\frac{1}{6}\beta ^{2}\pi ^{2}.
\end{eqnarray*}%
Derive the moment estimators.

\begin{solution}
\textbf{Method of moments for the Gumbel distribution}

Step 1:%
\begin{eqnarray*}
\mu _{1} &=&\alpha +0.5772\cdot \beta  \\
\mu _{2}^{\prime } &=&\frac{1}{6}\beta ^{2}\pi ^{2}.
\end{eqnarray*}%
Step 2:%
\begin{eqnarray*}
\beta  &=&\sqrt{\frac{6\mu _{2}^{\prime }}{\pi ^{2}}} \\
\alpha  &=&\mu _{1}-0.5772\cdot \beta .
\end{eqnarray*}%
Step 3:%
\begin{eqnarray*}
\hat{\beta} &=&\sqrt{\frac{6\hat{\mu}_{2}^{\prime }}{\pi ^{2}}} \\
\hat{\alpha} &=&\hat{\mu}_{1}-0.5772\cdot \hat{\beta}.
\end{eqnarray*}
\end{solution}

\subsection{Method of moments for the Pareto distribution}

The Pareto distribution has two parameters, $K\leq x<0$ and $\alpha >0$ and
density $f_{X}(x)=\alpha K^{\alpha }x^{-\alpha -1}$. The expectation and
variance of $X\sim Pareto(K,\alpha )$ are%
\begin{eqnarray*}
E(X) &=&\frac{\alpha K}{\alpha -1} \\
Var(X) &=&\frac{\alpha K^{2}}{\left( \alpha -2\right) \left( \alpha
-1\right) ^{2}}.
\end{eqnarray*}%
$\allowbreak $

\begin{enumerate}
\item Derive the moment estimators. What happens if $\alpha <2$ ?

\item Write an R program to simulate the distribution of the moment
estimator of $\alpha =5$ (with $K=1$ fixed). Generate $R=10000$ samples $%
X_{1},\ldots ,X_{n}$ of size $n=100$ each. What happens if you increase the
sample size to $n=1000$ ? What happens if you consider an $\alpha < 2$?
\end{enumerate}

\begin{solution}
\textbf{Method of moments for the Pareto distribution}

\begin{enumerate}
  \item For a given $K$ the moment estimator of $\alpha$ is given by
      \begin{align*}
        \mu_1 = \frac{\alpha K}{\alpha - 1}\\
        \alpha = \frac{\mu_1}{\mu_1-K}\\
        \hat{\alpha} = \frac{\hat{\mu_1}}{\hat{\mu_1}-K}
      \end{align*}
      For both parameters unknown one has to solve the system of
      equations
      \begin{align*}
        \mu_1 &= \frac{\alpha K}{\alpha - 1}\\
        \mu_{2}^{\prime} &= \frac{\alpha K^2}{(\alpha-2)(\alpha-1)^2}
      \end{align*}
      for $\alpha$ and $K$. Inserting the squared first expression into
      the second yields
      \begin{align*}
        &\mu_{2}^{\prime} = \frac{\mu_1^2}{\alpha(\alpha-2)}\\
        &\Leftrightarrow\alpha^2-2\alpha-\frac{\mu_1^2}{\mu_{2}^{\prime}}=0\\
        &\Leftrightarrow\alpha = \frac{2}{2} \pm \frac{\sqrt{4(1+\frac{\mu_1^2}{\mu_{2}^{\prime}})}}{2}=1\pm \sqrt{1+\frac{\mu_1^2}{\mu_{2}^{\prime}}}
      \end{align*}
      The variance is negative for $0<\alpha<2~~ (Var(X)<0)$. Thus,
      $\alpha$ must be greater than 2 and the moment estimators are
      \begin{align*}
        \hat{\alpha} &= 1 + \sqrt{1+\frac{\hat{\mu_1}^2}{\hat{\mu_{2}}^{\prime}}}\\
        \hat{K} &= \frac{\alpha-1}{\alpha}\hat{\mu}_1
      \end{align*}
  \item The program might look like this:

\begin{verbatim}
##################################################
#### Moment estimator for Pareto distribution ####
##################################################

install.packages("VGAM") # in order to get the Pareto distribution
library(VGAM)
library(AER)

a <- 5              #shape parameter
K <- 1              #location parameter
R <- 10000          # index for the loop
n <- 100            # number of random variables
ahat <- rep(NA,R)   # initialize estimator
ahat1 <- rep(NA,R)  # initialize estimator

for (i in 1:R) {
  x <- rpareto(n,location=K,shape=a)     #generate n random variables
  ahat[i] <- mean(x)/(mean(x)-1)         #moment estimator of alpha if K is known
  ahat1[i] <- 1+sqrt(1+mean(x)^2/var(x)) #moment estimator of alpha if K is unknown
}
#graphical output
par(mfrow=c(1,2))
truehist(ahat)
g <- seq(min(ahat),max(ahat),length=300)
lines(g,dnorm(g,mean(ahat),sd(ahat)))
truehist(ahat1)
h <- seq(min(ahat1),max(ahat1),length=300)
lines(h,dnorm(h,mean(ahat1),sd(ahat1)))
\end{verbatim}
For $\alpha <2$ the second estimator is not asymptotically normal
distributed, since the moment of order $2p$ does not exist (see the
lecture for the necessary conditions regarding moment estimators).
\end{enumerate}
\end{solution}

\subsection{Method of moments for the uniform distribution}

\begin{enumerate}
\item Consider the uniform distribution with parameters $a$ and $b$ (where $%
b>a$). The expectation and variance of $X\sim unif(a,b)$ are%
\begin{eqnarray*}
E(X) &=&\frac{a+b}{2} \\
Var(X) &=&\frac{\left( b-a\right) ^{2}}{12}.
\end{eqnarray*}%
Derive the moment estimators.

\item Write an R program to simulate the distribution of the moment
estimators of $a=0$ and $b=1$. Generate $R=10000$ samples $X_{1},\ldots
,X_{n}$ of size $n=40$ each. Are the estimators approximately normally
distributed? Check if the moment estimator of $a$ is always smaller than (or
equal to) the minimum in the sample.
\end{enumerate}

\begin{solution}
\textbf{Method of moments for the uniform distribution}

\begin{enumerate}
\item The theoretical moments $\mu _{1}$ and $\mu _{2}^{\prime }$ are
already given as functions of the unknown parameters. Solving the system%
\begin{eqnarray*}
2\mu _{1} &=&a+b \\
12\mu _{2}^{\prime } &=&a^{2}-2ab+b^{2}
\end{eqnarray*}%
for $a$ and $b$ yields two solutions. Since $b>a$ only one solution is valid,%
\begin{eqnarray*}
a &=&\mu _{1}-\sqrt{3\mu _{2}^{\prime }} \\
b &=&\mu _{1}+\sqrt{3\mu _{2}^{\prime }}
\end{eqnarray*}%
and the moment estimators are%
\begin{eqnarray*}
\hat{a} &=&\hat{\mu}_{1}-\sqrt{3\hat{\mu}_{2}^{\prime }} \\
\hat{b} &=&\hat{\mu}_{1}+\sqrt{3\hat{\mu}_{2}^{\prime }}.
\end{eqnarray*}

\item The program might look like this:

\begin{verbatim}
###################################################
#### Moment estimator for uniform distribution ####
###################################################
a <- 0
b <- 1
R <- 10000
ahat <- rep(NA,R)
bhat <- rep(NA,R)
minx <- rep(NA,R)
for(r in 1:R) {
  x <- runif(n=40,a,b)
  ahat[r] <- mean(x)-sqrt(3*var(x))
  bhat[r] <- mean(x)+sqrt(3*var(x))
  minx[r] <- min(x)
  }
par(mfrow=c(2,1))
truehist(ahat)
g <- seq(min(ahat),max(ahat),length=300)
lines(g,dnorm(g,mean(ahat),sd(ahat)))
truehist(bhat)
g <- seq(min(bhat),max(bhat),length=300)
lines(g,dnorm(g,mean(bhat),sd(bhat)))
print("Proportion of impossible estimates:")
print(sum(minx<ahat)/R)
\end{verbatim}
\end{enumerate}
\end{solution}

\subsection{Method of moments for the linear regression model\label{MethodOfMomentsLinReg}}

Consider the linear regression model under standard assumptions%
\begin{equation*}
y=X\beta +u.
\end{equation*}%
Suppose that X may be non-stochastic but independent of $u$, i.e. $E(u|X) = 0$. Left-multiply the model equation by $X^{\prime }$ and take expectations.
Show that the method of moment estimator of $\beta $ is identical to the OLS
estimator.

\begin{solution}
\textbf{Method of moments for the linear regression model}

First have a look at $X'y$:
\begin{align*}
  \underset{k\times T}{X}'\underset{T \times 1}{y} =
  \begin{pmatrix}
    \sum_{t=1}^T X_{t1}\cdot y_t\\
    \vdots\\
    \sum_{t=1}^T X_{tk}\cdot y_t
  \end{pmatrix}
\end{align*}
Taking the expectation yields for each row $j$: $\sum_{t=1}^T E[X_{tj}\cdot
y_t] = T\cdot E(X_{1j} \cdot y_1)$ or any other vector of $y$ and item
$X_{tj}$.

Now, left-multiplication of $y=X\beta +u$ with $X^{\prime }$ and taking expectations yields%
\begin{equation*}
E\left( X^{\prime }y\right) =E\left( X^{\prime }X\beta \right) +E\left(
X^{\prime }u\right) = T
\begin{pmatrix}
    E(X_{11}\cdot y_1)\\
    \vdots\\
    E(X_{1k}\cdot y_1)
\end{pmatrix}.
\end{equation*}%


Step 1: We relax the standard assumptions and assume that $X$ may be
non-stochastic but independent of $u$, i.e. $E(u|X)=0$. This implies that
$E\left( X^{\prime }X\beta \right) =E\left( X^{\prime }X\right) \beta $ and
$E\left( X^{\prime
}u\right) =E\left( X^{\prime }\right) E(u)$. Of course, $E(u)=0$, and thus%
\begin{eqnarray*}
E\left( X^{\prime }y\right)  &=&E\left( X^{\prime }X\beta \right) +E\left(
X^{\prime }u\right)  \\
E\left( X^{\prime }y\right)  &=&E\left( X^{\prime }X\right) \beta .
\end{eqnarray*}%
Step 2: The system is solved for the unknown parameters $\beta $,%
\begin{align*}
\beta &= \underbrace{\left[E\left( X^{\prime }X\right)\right]^{-1}}_{\text{Matrix of expectations}} \cdot \underbrace{E\left( X^{\prime}y\right)}_\text{Vector of expectations}\\
&= \frac{1}{T}\cdot \begin{bmatrix}
                    E(X_{11}^2) & \dots & E(X_{11}X_{1k})\\
                    \vdots & \ddots &\vdots\\
                    E(X_{11}X_{1k}) & \dots & E(X_{1k}^2)
                  \end{bmatrix}^{-1}
\cdot T
\begin{pmatrix}
    E(X_{11}\cdot y_1)\\
    \vdots\\
    E(X_{1k}\cdot y_1)
\end{pmatrix}.
\end{align*}%
Step 3: Replace the theoretical moments (expectations) by their empirical
counterparts,%
\begin{equation*}
\hat{\beta}=
\begin{bmatrix}
    \frac{1}{T}\sum_{t=1}^T X_{t1}^2 & \dots & \frac{1}{T}\sum_{t=1}^T X_{t1}X_{tk}\\
    \vdots & \ddots &\vdots\\
    \frac{1}{T}\sum_{t=1}^T X_{t1}X_{tk} & \dots & \frac{1}{T}\sum_{t=1}^T X_{tk}^2
\end{bmatrix}^{-1}
\cdot
\begin{pmatrix}
    \frac{1}{T}\sum_{t=1}^T X_{t1}\cdot y_t\\
    \vdots\\
    \frac{1}{T}\sum_{t=1}^T X_{tk}\cdot y_t
\end{pmatrix}=(X^{\prime }X)^{-1}X^{\prime }y.
\end{equation*}%
This is equal to the OLS estimator of $\beta $ (and also identical to the
maximum likelihood estimators, as we will see later).
\end{solution}

\newpage
\section{Maximum likelihood estimation\label{maximumlikelihood}}

\subsection{Extreme values\label{extremes}}

Let $X\sim Pareto(K,\alpha )$ where the parameter $K\leq x <0$ is known but
the tail parameter $\alpha $ is unknown. The density function of Pareto
distribution is%
\begin{equation*}
f_{X}(x)=\alpha K^{\alpha }x^{-\alpha -1}.
\end{equation*}

\begin{enumerate}
\item Derive the maximum likelihood estimator of $\alpha $.

\item The Pareto distribution is an excellent approximation of large daily
stock return losses (of, say, more than 2\%). Load the dataset \texttt{%
daxreturns.csv}. It contains the daily DAX returns (in \%) from 16/7/2001 to
13/7/2011 (without holidays). Multiply all DAX returns by $(-1)$ in order to
make losses positive, delete all losses that are smaller than 2\%, and
estimate the tail parameter $\alpha $ for the remaining observations.

\item Plot the likelihood of the observations as a function of $\alpha $.
\end{enumerate}

\begin{solution}
\textbf{Extreme values}

\begin{enumerate}
\item The maximum likelihood estimator of $\alpha$ is given by maximizing
    the loglikelihood function:
\begin{align*}
  L(\alpha;x,K) &= \prod_{i=1}^n \alpha K^\alpha x_i^{-\alpha-1}\\
  log(L(\alpha;x,K))&= \sum_{i=1}^n log(\alpha K^\alpha) + \sum_{i=1}^n log(x_i^{-\alpha-1})\\
    & = n(log(\alpha)+ \alpha log(K)) - (\alpha+1) \sum_{i=1}^n log(x_i)\\
  \frac{\partial log(L)}{\partial \alpha} &= \frac{n}{\alpha} + n ~ log(K) - \sum_{i=1}^n log(x_i) = 0\\
  \Rightarrow \hat{\alpha} &= \frac{n}{\sum_{i=1}^n (log(x_i)-log(K))}
\end{align*}
\item[2./3.] The code might look like this. The ML-estimator is also estimated by numerically optimizing the likelihood.
\begin{verbatim}
########################
#### Extreme values ####
########################
library(MASS)
library(VGAM)

daxreturns <- read.csv(file.choose(), sep=";", dec=",")
View(daxreturns)
daxret <- daxreturns$daxret
plot(daxret)
tmp <- daxret*(-1)
daxlosses <- tmp[tmp>=2]
alphahat <- length(daxlosses)/(sum(log(daxlosses) - log(2)))
plot(daxlosses)
truehist(daxlosses)
coord <- par("usr")
x <- seq(coord[1], coord[2], length.out = length(daxlosses))
lines(x,dpareto(x,scale=2,shape=alphahat),col="red",lwd=2)

#Likelihood of the observations as a function of alpha
loglikelihood <- function(alpha,K,x) {
  n <- length(x)
  z <- n*(log(alpha)+alpha*log(K)) - (alpha+1)*sum(log(x))
  return(z)
}
alpha <- seq(0,10,length.out=100)
plot(alpha,loglikelihood(alpha,2,daxlosses))
abline(v=alphahat)

#Just for comparison, let's do it numerically
likelihood <- function(alpha,K,x) {
  z <- prod(dpareto(x,K,alpha))
  return(z)
}

f <- function(x) -likelihood(x,2,daxlosses)
optimize(f,lower=0,upper=3)
\end{verbatim}
\end{enumerate}
\end{solution}

\subsection{Parameters of the uniform distribution\label{mluniform}}

Consider the uniform distribution on the interval $[a,b]$ with density%
\begin{equation*}
f_{X}(x)=\left\{
\begin{array}{ll}
\frac{1}{b-a} & \quad \text{for }a\leq x\leq b \\
0 & \quad \text{else.}%
\end{array}%
\right.
\end{equation*}

\begin{enumerate}
\item Derive the maximum likelihood estimators of $a$ and $b$.

\item Write an R program to generate $R=10000$ samples of size $n=100$ each.
For each sample compute and store the maximum likelihood estimates $\hat{a}$
and $\hat{b}$. Plot their histograms.
\end{enumerate}

\begin{solution}
\textbf{Parameters of the uniform distribution}
\begin{enumerate}
  \item Consider a sample $x_1,\dots,x_n$ and let the order statistics
      be: $x_{(1)} \leq x_{(2)} \leq \dots \leq x_{(n)}$. For $\alpha
      \leq x_{(1)}$ and $b \geq x_{(n)}$ the likelihood equals:
      \begin{align*}
        L(a,b;x) &= \prod_{i=1}^n f_X(x_i|a,b) = (b-a)^{-n}\\
        log(L(a,b;x)) &= -n\cdot log(b-a)\\
        \frac{\partial log(L)}{\partial a} &= \frac{n}{b-a} >0\\
        \frac{\partial log(L)}{\partial b} &= -\frac{n}{b-a} <0
      \end{align*}
      Thus the loglikelihood is a strictly increasing function in a and a
      strictly decreasing function in b. Hence the ML-estimator is given
      by:
      \begin{equation*}
        \hat{a} = x_{(1)} \text{ and }\hat{b} = x_{(n)}
      \end{equation*}
  \item The program might look like this:
\begin{verbatim}
################################################
#### Parameters of the uniform distribution ####
################################################
R <- 10000
n <- 100
a <- 3
b <- 6
ahat <- numeric(R)
bhat <- numeric(R)
for (r in 1:R) {
  x <- runif(n,min=a,max=b)
  ahat[r] <- min(x)
  bhat[r] <- max(x)
}
par(mfrow=c(1,2))
hist(ahat,prob=T)
hist(bhat,prob=T)
\end{verbatim}
Clearly, the ML estimators are not asymptotically normal distributed. See also execise 'Limits of maxima (III)'.
\end{enumerate}
\end{solution}

\subsection{Censored lognormal distribution\label{mllognormal}}

Let $X\sim LN(\mu ,\sigma ^{2})$ and let $X_{1},\ldots ,X_{n}$ be a sample
drawn from $X$. The $X_{i}$ are not observable. Instead one can only observe%
\begin{equation*}
Y_{i}=\left\{
\begin{array}{ll}
X_{i} & \quad \text{if }X_{i}<c \\
c & \quad \text{if }X_{i}\geq c%
\end{array}%
\right.
\end{equation*}%
where $c$ is a known constant. The likelihood of $Y_{1},\ldots ,Y_{n}$ is
the product of all densities $f_{X}(y_{i}),$ for observations with $Y_{i}<c$%
, times the product of all probabilities that $Y_{i}=c$ for observations
with $Y_{i}=c$.

\begin{enumerate}
\item Write an R function that computes the likelihood of $\mu $ and $\sigma
^{2}$ given the observations $Y_{1},\ldots ,Y_{n}$ (and given $c$).

\item Load the dataset \texttt{censoredln.csv}.

\item Numerically maximize the likelihood function. The censoring value is $%
c=12.$

\item Compute the asymptotic covariance matrix of $\hat{\mu}$ and $\hat{%
\sigma}^{2}$.
\end{enumerate}

\begin{solution}
\textbf{Censored lognormal distribution}

First, let's consider the probability of $Y_i=c$:
\begin{equation*}
  Pr(Y_i = c) = Pr(X_i \geq c) = 1 - Pr(X_i \leq c) = 1- F_X(c)
\end{equation*}
The likelihood function is now a mixture between the product of all densities $f_{X}(y_{i}),$ for observations with $Y_{i}<c$%
, times the product of all probabilities that $Y_{i}=c$ for observations with
$Y_{i}=c$:
\begin{equation*}
L(\mu,\sigma; y) = \prod_{i=1}^n\{ f_X(y_i;\mu,\sigma)\}^{\delta_i}\{1-F_X(c;\mu,\sigma)\}^{1-\delta_i}
\end{equation*}
with $\delta_i=1$ for exact observations and $\delta_i=0$ for a censored
observation. The loglikelihood is thus the sum of those two components (let
$n_1$ be the number of non-censored observations and $n_2$ the number of
censored observations, $n_1+n_2=n$):
\begin{equation*}
\log L(\mu,\sigma; y) = \sum_{i=1}^{n_1} \log f_X(y_i;\mu,\sigma) +
\sum_{i=1}^{n_2} \log(1-F_X(c;\mu,\sigma))
\end{equation*}
The code might look like this:
\begin{verbatim}
#########################################
#### Censored lognormal distribution ####
#########################################
# Definition of loglikelihood
neglogl <- function(param,dat,cens) {
  mu <- param[1]
  sigma <- param[2]
  y <- dat
  c <- cens
  z <- sum(log(dlnorm(y[y<c],meanlog=mu,sdlog=sigma)))
  + sum(log(1-plnorm(y[y==c],meanlog=mu,sdlog=sigma)))
  return(-z)
}
# Get data
censoredln <- read.table(file.choose(), header=T, quote="\"")
x <- censoredln$x
# Optimization
obj <- optim(c(0,1),neglogl,dat=x,cens=12,hessian=T)
print(obj$par)  # Point estimates
print(solve(obj$hessian)) # Numerical covariance matrix
\end{verbatim}
\end{solution}

\subsection{Exponential model}

Consider the exponential model%
\begin{equation*}
y_{i}=\exp \left( \alpha +\beta x_{i}\right) +u_{i}
\end{equation*}%
and load the dataset \texttt{expgrowth.csv} from the course site.

\begin{enumerate}
\item Assume that the error terms are i.i.d. and $u_{i}\sim N(0,\sigma ^{2})$%
. Write an R function that calculates the log-likelihood of $\alpha ,\beta $
and $\sigma ^{2}$.

\item Numerically find the maximum likelihood estimates of $\alpha ,$ $\beta
$ and $\sigma ^{2}$. Compare your results with exercise \ref{nls1}.

\item Compute the asymptotic covariance matrix of $\hat{\alpha}$, $\hat{\beta%
}$ and $\hat{\sigma}^{2}$.

\item Assume that the error terms are i.i.d. with known density%
\begin{equation*}
f_{u_{i}}(u)=\frac{1}{2}\exp \left( -|u|\right) .
\end{equation*}%
Numerically find the estimates of $\alpha $ and $\beta $.
\end{enumerate}

\begin{solution}
\textbf{Exponential model}

For the log-likelihood you have to consider the distribution of the error
term:
\begin{align*}
  u_i = y_i - exp\{\alpha + \beta x_i\} \sim N(0,\sigma^2)
\end{align*}
\begin{itemize}
\item[1)/2)/3)] The distributional assumption for the likelihood is thus
    the density of the normal distribution ($f_{u_i}$) and the
    log-likelihood is given by
\begin{align*}
  \log L = \sum_{i=1}^n f_u(y_i - exp\{\alpha + \beta x_i\})
\end{align*}
\item[4)] Now the log-likelihood is given by
\begin{align*}
  \log L = -n \log(2) - \sum_{i=1}^n |\alpha + \beta x_i|
\end{align*}
\end{itemize}
The code might look like this:
\begin{verbatim}
###########################
#### Exponential model ####
###########################
# Get data
expgrowth <- read.csv(file.choose())
View(expgrowth)

#1: Definition of negative loglikelihood
neglogl <- function(param,dat){
  a <- param[1]
  b <- param[2]
  sigma <- param[3]
  x <- dat[,2]
  y <- dat[,1]
  u <- y-exp(a+b*x)
  z <- sum(log(dnorm(u,mean=0,sd=sigma)))
  zz <- sum(log(dnorm(y,mean=exp(a+b*x),sd=sigma))) #works as well
  return(-z)
}

#2: Numerical Optimization
# Optimization, for start values one has to consider that the exponential function is very
# sensitive to the parameters a and b -> use small ones
obj <- optim(c(0,0.1,1),neglogl,dat=expgrowth,hessian=T)
print(obj$par)  # Point estimates

#3: Asymptotic numerical covariance matrix
print(solve(obj$hessian))

#4: An exponential density function for the error terms
#Definition of negative loglikelihood
neglogl2 <- function(param,dat){
  a <- param[1]
  b <- param[2]
  x <- dat[,2]
  y <- dat[,1]
  u <- y-exp(a+b*x)
  z <- -n*log(2) - sum(abs(u))     #analytically
  zz <- sum(log(0.5*exp(-abs(u)))) #alternatively
  return(-z) #or return(-zz)
}
obj2 <- optim(c(0,0.1,1),neglogl2,dat=expgrowth,hessian=T)
print(obj2$par)  # Point estimates
\end{verbatim}
\end{solution}

\subsection{Tobit model\label{mltobit}}

The Tobit model is a linear regression model where observations are censored
from below at zero. A latent (unobservable) variable $y_{t}^{\ast }$ is
assumed to depend linearly on a vector $x_{t}$ of exogenous variables,%
\begin{equation*}
y_{t}^{\ast }=x_{t}^{\prime }\beta +u_{t}
\end{equation*}%
where $u_{t}\sim N(0,\sigma ^{2})$. The observations are%
\begin{equation*}
y_{t}=\left\{
\begin{array}{ll}
y_{t}^{\ast } & \quad \text{if }y_{t}^{\ast }>0 \\
0 & \quad \text{else.}%
\end{array}%
\right.
\end{equation*}

\begin{enumerate}
\item Given the vector of exogenous variables $x_{t}$, derive the
probability that $y_{t}=0$.

\item The likelihood of $y_{1},\ldots ,y_{T}$ is the product of all
densities $f_{y_{t}}(y_{t}),$ for observations with $y_{t}>0,$ times the
product of all probabilities that $y_{t}=0$ for observations with $y_{t}=0$.
Derive the log-likelihood.

\item Load the dataset \texttt{tobitbsp.csv}. The dataset contains the
observed endogenous variable \texttt{y} and three exogenous variables
\texttt{x1}, \texttt{x2}, \texttt{x3} (where \texttt{x1} is just a vector of
ones). The data are simulated but have similar means, crossproducts etc. as
the data in \textquotedblleft Estimation of Relationships for Limited
Dependent Variables\textquotedblright\ by James Tobin, \emph{Econometrica},
26 (1958) 24-36.\footnote{%
This was the first article to use Tobit estimation (although the name was
coined later); it can be downloaded from the course site.} Numerically
compute the maximum likelihood estimates $\hat{\beta}$ and $\hat{\sigma}^{2}$%
.

\item Estimate an OLS regression without taking into account the censoring
at zero. Compare the OLS estimates with the Tobit estimates.

\item Compute the standard errors of $\hat{\beta}$ and $\hat{\sigma}^{2}.$
\end{enumerate}

\begin{solution}
\textbf{Tobit model}

First, let's consider censored observations, for $y_t^*\leq 0$:
\begin{equation*}
  Pr(y_t = 0) = Pr(y_t^* \leq 0) = Pr(u_t\leq -x_t'\beta) = Pr\left(\frac{u_t}{\sigma}\leq \frac{-x_t'\beta}{\sigma}\right) = \Phi\left(\frac{-x_t'\beta}{\sigma}\right)
\end{equation*}
with $\Phi$ being the cdf of the standard normal distribution. For uncensored observations we have a linear model, i.e. the likelihood for $y_t^*> 0$:
\begin{align*}
  f(u_t) = \frac{1}{\sigma\sqrt{2\pi}}e^{\frac{-(y_t^*-x_t'\beta)^2}{2\sigma^2}} = \frac{1}{\sigma} \frac{1}{\sqrt{2\pi}}e^{\frac{-1}{2}\left(\frac{y_t^*-x_t'\beta}{\sigma}\right)} = \frac{1}{\sigma}\phi\left(\frac{y_t^*-x_t'\beta}{\sigma}\right)
\end{align*}
with $\phi$ being the pdf of the standard normal distribution. The likelihood function is now a mixture:
\begin{equation*}
L(\mu,\sigma; y) = \prod_{t=1}^n\left\{ \frac{1}{\sigma}\phi\left(\frac{y_t^*-x_t'\beta}{\sigma}\right)\right\}^{\delta_t}
\left\{\Phi\left(\frac{-x_t'\beta}{\sigma}\right)\right\}^{1-\delta_t}
\end{equation*}
with $\delta_t=1$ for exact observations and $\delta_t=0$ for a censored observation. The loglikelihood is thus the sum of those two components:
\begin{equation*}
\log L(\mu,\sigma; y) = \sum_{y_t>0} \log\left(\frac{1}{\sigma}\phi\left(\frac{y_t^*-x_t'\beta}{\sigma}\right)\right) +
\sum_{y_t=0} \log\left(\Phi\left(\frac{-x_t'\beta}{\sigma}\right)\right)
\end{equation*}
The code might look like this:
\begin{verbatim}
#####################
#### Tobit model ####
#####################
# Definition of negative loglikelihood
neglogl <- function(param,dat) {
  y <- dat$y
  x1 <- dat$x1
  x2 <- dat$x2
  x3 <- dat$x3
  beta1 <- param[1]
  beta2 <- param[2]
  beta3 <- param[3]
  sigm   <- param[4]
  idx_cens <- which(y==0)
  idx_uncens <- which(y>0)
  uncens <- sum(log(1/sigm*dnorm((y[idx_uncens]-x1[idx_uncens]*beta1
                            -x2[idx_uncens]*beta2-x3[idx_uncens]*beta3)/sigm)))
  cens   <- sum(log(pnorm((-x1[idx_cens]*beta1-x2[idx_cens]*beta2-x3[idx_cens]*beta3)/sigm)))
  z <- uncens+cens
  return(-z)
}
# Get data
tobitbsp <- read.csv(file.choose())
# Note that if tobitbsp.csv has negative y values,
# set all negative values to zero, and proceed with the exercise
View(tobitbsp)
tobitbsp$y[tobitbsp$y<0] <- 0
# Optimization
obj <- optim(c(0.2,-0.02,0.005,0.125),neglogl,dat=tobitbsp,hessian=T)
print(obj$par)  # Point estimates
print(diag(solve(obj$hessian))) # Numerical covariance matrix
#OLS
X <- as.matrix(cbind(tobitbsp$x1,tobitbsp$x2,tobitbsp$x3))
y <- as.matrix(tobitbsp$y)
beta_ols <- solve(t(X)%*%X)%*%t(X)%*%y
\end{verbatim}

\begin{itemize}
  \item Compared to Logit or Probit, we do not throw away information in y.
  \item Note that OLS is not a consistent estimator, however Maximum Likelihood is given some regularity conditions always consistent.

\end{itemize}
\end{solution}

\subsection{Probit model\label{mlprobit}}

Suppose the endogenous variable $y_{t}$ can only take two values,%
\begin{equation*}
y_{t}=\left\{
\begin{array}{ll}
1 & \quad \text{with probability }p_{t} \\
0 & \quad \text{with probability }1-p_{t}.%
\end{array}%
\right.
\end{equation*}%
We would like to model the probability $p_{t}$ as a function of a vector of
exogenous variables $x_{t}$. In particular, assume that the probability
of $y_{t}=1$ equals the value of the cdf of $N(0,1)$ at $x_{t}^{\prime
}\beta $:%
\begin{eqnarray*}
p_{t} &=&\Phi \left( x_{t}^{\prime }\beta \right)
=\int_{-\infty }^{x_{t}^{\prime }\beta }\frac{1}{\sqrt{2\pi }}e^{-\frac{1}{%
2}z^{2}}dz.
\end{eqnarray*}

\begin{enumerate}\setlength{\itemsep}{0pt}
\item Derive the log-likelihood of $\beta $. The distribution function of $%
N(0,1)$ is \texttt{pnorm} in R.

\item Load the dataset \texttt{mroz.csv}. The file contains the data used in
the article \textquotedblleft The Sensitivity of an Empirical Model of
Married Women's Hours of Work to Economic and Statistical
Assumptions\textquotedblright\ by Thomas Mroz, \emph{Econometrica}, 55
(1987) 765-799.\footnote{%
The article can be downloaded from the course site. The data are available
on the internet site of Jeffrey Wooldridge, Econometric Analysis of Cross
Section and Panel Data, 2nd ed., 2010. A short description of the dataset
can be found on the course site.}

Use \texttt{inlf} (\textquotedblleft \textbf{in} \textbf{l}abour \textbf{f}%
orce\textquotedblright ) as endogenous variable and \texttt{nwifeinc},
\texttt{educ}, \texttt{exper}, \texttt{exper2}, \texttt{age}, \texttt{kidslt6%
}, and \texttt{kidsge6} as exogenous variables. Add a vector of constants
(ones). Numerically compute the maximum likelihood probit estimate $\hat{%
\beta}$.

\item Interpret the parameter estimates.

\item Predict the probability of $inlf=1$ for a woman with the following
covariates%
\begin{eqnarray*}
\text{nwifeinc} =30, \quad
\text{educ} =14, \quad
\text{exper} =10, \quad
\text{age} =44, \quad
\text{kidslt6} =0, \quad
\text{kidsge6} =3.
\end{eqnarray*}

\item Calculate the standard errors of $\hat{\beta}$. Is $\hat{\beta}_{educ}$
significantly different from zero?

\item Suppose that the true distribution of the disturbances is not $N(0,1)$
but a uniform distribution on the interval $[-1,1]$. Write a simulation
program to show that the maximum likelihood estimator is no longer
consistent under this kind of misspecification.
\end{enumerate}

\begin{solution}
\textbf{Probit model}

Since the probability that $y_t=1$ is $\Phi(x_t'\beta)$, the contribution to the loglikelihood function for observation $t$ when $y_t=1$ is $log(\Phi(x_t'\beta))$, that is the log-likelihood function is equal to
\begin{align*}
\sum_{i=1}^{n} \left(y_t log(\Phi(x_t'\beta)) + (1-y_t)log(1-\Phi(x_t'\beta))\right)
\end{align*}

The code might look like this:
\begin{verbatim}
######################
#### Probit model ####
######################
rm(list=ls())
dev.off()
cat("\014")

mroz <- read.csv(file.choose())
inlf <- mroz$inlf
nwifeinc <- mroz$nwifeinc
educ <- mroz$educ
exper <- mroz$exper
exper2 <- exper^2
age <- mroz$age
kidslt6 <- mroz$kidslt6
kidsge6 <- mroz$kidsge6


numTimeVals <- length(nwifeinc)
zuerkl <- inlf
numVar <- 8
erkl <- matrix(0,numTimeVals,numVar)
erkl[,1] <- nwifeinc
erkl[,2] <- educ
erkl[,3] <- exper
erkl[,4] <- exper2
erkl[,5] <- age
erkl[,6] <- kidslt6
erkl[,7] <- kidsge6
erkl[,8] <- rep(1,numTimeVals)

probitLL2 <- function(betas,x,y){
  vals <- x %*% betas
  TT <- length(y)
  summanden <- y*log(pnorm(vals, mean=0, sd=1)) + (1-y)*log(1-pnorm(vals, mean=0, sd=1))
  return (-sum(summanden))
}

beta_anf <- 0

obj2 <- optim(par=rep(beta_anf,numVar),probitLL2,x=erkl, y=zuerkl, hessian=T)
#print(obj)
obj2$par
solve(obj2$hessian)

plot(zuerkl)
points(seq(1,numTimeVals),pnorm(erkl%*%obj2$par),col = 2)

pred <- obj2$par%*%c(30,14,10,10^2,44,0,3,1)
pnorm(pred)

#t value
abs(obj2$par[2]/obj2$hessian[2,2])
# -> significantly different from 0

#simulate data
#datenpunkte
N <- 250

beta1_wahr <- 2
beta0_wahr <- -1.5

testen <- function(N, norm = T){
  #Daten produzieren
  if(norm == T){x <- rnorm(N)
  p <- pnorm(x*beta1_wahr + beta0_wahr  + rnorm(N,0,10^-1))
  }
  else{x <- rnorm(N)
  p <- pnorm(x*beta1_wahr + beta0_wahr  + runif(N,-1,1))
  }
  y <- runif(N)
  y2 <- runif(N)
  y[y2<=p] = 1
  y[y2>p] = 0
  #zu x konstante hinzufügen
  x <- cbind(x,rep(1,N))

  #Parameter schaetzen
  obj2 <- optim(par=c(0,0),probitLL2, x=x, y=y, hessian=T)
  return(obj2$par)
}

sampleSizes <- seq(100,10000,100)
#Inkonsistenz veranschaulichen bei Gleichverteilung
param = NULL
for(numbers in sampleSizes){
  param = rbind(param,testen(numbers,F))
}

par(mfrow=c(2,2))
plot(sampleSizes, param[,1], xlab="Stichprobengröße", ylab="beta1", main="Gleichverteilung",
                                                                                ylim=c(1,3))
 abline(h = beta1_wahr, col=2)

plot(sampleSizes, param[,2],, xlab="Stichprobengröße", ylab="beta0", main="Gleichverteilung",
                                                                                ylim=c(-2,-1))
abline(h = beta0_wahr, col=2)

#Konsistenz veranschaulichen
param = NULL
for(numbers in sampleSizes){
  param = rbind(param,testen(numbers,T))
}

plot(sampleSizes, param[,1], xlab="Stichprobengröße", ylab="beta1", main="Normalverteilung",
                                                                                ylim=c(1,3))
abline(h = beta1_wahr, col=2)

plot(sampleSizes, param[,2],, xlab="Stichprobengröße", ylab="beta0", main="Normalverteilung",
                                                                                ylim=c(-2,-1))
abline(h = beta0_wahr, col=2)
\end{verbatim}
\begin{itemize}
  \item Probit models are used to model binary outcomes, such as consumption decisions, labor decisions, agricultural decisions,\dots, and are among the most used models in applied econometrics.
  \item The probit model forecasts probabilities, which are in between 0 and 1: $Pr(Y=1|x)=F(x'\beta)$
  \item Interpretation of coefficients:
  \begin{itemize}
    \item Increase in $x$ makes outcome of 1 more (or less) likely, we therefore interpret the sign of the coefficients.
    \item Marginal effects at a specific $x$ (most commonly at mean $\bar{x}$): $\frac{\partial p}{\partial x_j} = \phi(x'\beta)\beta_j$. Note that coefficients and marginal effects have the same sign.
    \item Average marginal effects are computed over all $x$.
  \end{itemize}
  \item Probit and Logit have usually very similar marginal effects, which model depends on an evaluation of the data generating process. Most of the times it doesn't matter though.
\end{itemize}
\end{solution}

\subsection{Logit model\label{mllogit}}

Suppose the endogenous variable $y_{t}$ can only take two values,%
\begin{equation*}
y_{t}=\left\{
\begin{array}{ll}
1 & \quad \text{with probability }p_{t} \\
0 & \quad \text{with probability }1-p_{t}.%
\end{array}%
\right.
\end{equation*}%
We would like to model the probability $p_{t}$ as a function of a vector of
exogenous variables $x_{t}$. In particular, assume that the probability
of $y_{t}=1$ equals the value of the logistic function at $x_{t}^{\prime
}\beta $:%
\begin{eqnarray*}
p_{t} &=&\Lambda \left( x_{t}^{\prime }\beta \right)
=\frac{\exp \left( x_{t}^{\prime }\beta \right) }{1+\exp \left(
x_{t}^{\prime }\beta \right) }.
\end{eqnarray*}

\begin{enumerate}\setlength{\itemsep}{0pt}
\item Derive the log-likelihood of $\beta $. In R, the distribution function
$\Lambda $ of the logistic distribution is computed by \texttt{plogis}.

\item Redo the application of exercise \ref{mlprobit}.2. Numerically compute
the maximum likelihood logit estimate $\hat{\beta}$.

\item Predict the probability of $y=1$ for the values of the exogenous
variables given in exercise \ref{mlprobit}.4.

\item Calculate the standard errors of $\hat{\beta}$. Is $\hat{\beta}_{educ}$
significantly different from zero (at significance level 0.05)?
\end{enumerate}

\begin{solution}
\textbf{Logit model}

Since the probability that $y_t=1$ is $\Lambda \left( x_{t}^{\prime }\beta \right)$, the contribution to the loglikelihood function for observation $t$ when $y_t=1$ is $log(\Lambda \left( x_{t}^{\prime }\beta \right))$, that is the log-likelihood function is equal to
\begin{align*}
\sum_{i=1}^{n} \left(y_t log(\Lambda \left( x_{t}^{\prime }\beta \right)) + (1-y_t)log(1-\Lambda \left( x_{t}^{\prime }\beta \right)))\right)
\end{align*}

The code might look like this:
\begin{verbatim}
#####################
#### Logit model ####
#####################
rm(list=ls())
dev.off()
cat("\014")

mroz <- read.csv(file.choose())
inlf <- mroz$inlf
nwifeinc <- mroz$nwifeinc
educ <- mroz$educ
exper <- mroz$exper
exper2 <- exper^2
age <- mroz$age
kidslt6 <- mroz$kidslt6
kidsge6 <- mroz$kidsge6


numTimeVals <- length(nwifeinc)
zuerkl <- inlf
numVar <- 8
erkl <- matrix(0,numTimeVals,numVar)
erkl[,1] <- nwifeinc
erkl[,2] <- educ
erkl[,3] <- exper
erkl[,4] <- exper2
erkl[,5] <- age
erkl[,6] <- kidslt6
erkl[,7] <- kidsge6
erkl[,8] <- rep(1,numTimeVals)

neglogitloglik <- function(betas,x,y){
  vals <- x %*% betas
  TT <- length(y)
  summanden <- y*log(plogis(vals)) + (1-y)*log(1-plogis(vals))
  return (-sum(summanden))
}

beta_anf <- 0

obj2 <- optim(par=rep(beta_anf,numVar),neglogitloglik,x=erkl, y=zuerkl, hessian=T)
print(obj2$par)
solve(obj2$hessian)

pred <- obj2$par%*%c(30,14,10,10^2,44,0,3,1)
plogis(pred)

#t value
abs(obj2$par[2]/obj2$hessian[2,2])
# -> significantly different from 0
\end{verbatim}
\begin{itemize}
  \item Logit models are used to model binary outcomes, such as consumption decisions, labor decisions, agricultural decisions,\dots, and are among the most used models in applied econometrics.
  \item The logit model forecasts probabilities, which are in between 0 and 1: $Pr(Y=1|x)=F(x'\beta)$
  \item Interpretation of coefficients:
  \begin{itemize}
    \item Increase in $x$ makes outcome of 1 more (or less) likely, we therefore interpret the sign of the coefficients.
    \item Marginal effects at a specific $x$ (most commonly at mean $\bar{x}$): $\frac{\partial p}{\partial x_j} = \frac{e^{x'\beta}}{(1+e^{x'\beta})^2}\beta_j$. Note that coefficients and marginal effects have the same sign.
    \item Average marginal effects are computed over all $x$.
  \end{itemize}
  \item Probit and Logit have usually very similar marginal effects, which model depends on an evaluation of the data generating process. Most of the times it doesn't matter though.
\end{itemize}
\end{solution}

\subsection{Sample selectivity: Heckman regression\label{mlheckman}}

If the sample is not selected randomly standard OLS methods cease to be
consistent. Consistent estimators are, however, still possible. We consider
the following simple sample selection model (see Davidson and MacKinnon,
2004, p. 486),%
\begin{equation*}
\left[
\begin{array}{c}
y_{t}^{\ast } \\
z_{t}^{\ast }%
\end{array}%
\right] =\left[
\begin{array}{c}
X_{t}\beta \\
W_{t}\gamma%
\end{array}%
\right] +\left[
\begin{array}{c}
u_{t} \\
v_{t}%
\end{array}%
\right] ,\qquad \left[
\begin{array}{c}
u_{t} \\
v_{t}%
\end{array}%
\right] \sim N\left( \left[
\begin{array}{c}
0 \\
0%
\end{array}%
\right] ,\left[
\begin{array}{cc}
\sigma ^{2} & \rho \sigma \\
\rho \sigma & 1%
\end{array}%
\right] \right)
\end{equation*}%
where $X_{t}$ and $W_{t}$ are vectors of exogenous variables, $\beta $ and $%
\gamma $ are unknown parameter vectors, $\sigma $ is the standard deviation
of $u_{t}$ and $\rho $ is the correlation between $u_{t}$ and $v_{t}$. Both $%
y_{t}^{\ast }$ and $z_{t}^{\ast }$ are latent (unobservable). Actually
observed are%
\begin{equation*}
\begin{array}{l}
y_{t}=y_{t}^{\ast } \\
z_{t}=1%
\end{array}%
\quad \text{if }z_{t}^{\ast }>0
\end{equation*}%
and%
\begin{equation*}
\begin{array}{l}
y_{t}\text{ unobserved} \\
z_{t}=0%
\end{array}%
\quad \text{if }z_{t}^{\ast }\leq 0.
\end{equation*}

\begin{enumerate}
\item Derive the log-likelihood of $\left( \beta ,\gamma ,\rho ,\sigma
\right) ^{\prime }$. Hint: If $y_{t}$ is not observed, its contribution to
the log-likelihood is $\ln P\left( z_{t}=0\right) $, else it is $\ln P\left(
z_{t}=1\right) f\left( y_{t}^{\ast }|z_{t}=1\right) $.

\item Load the dataset \texttt{womanwk.dta}.\footnote{This is a dataset used in the Stata Manual, Section Heckman selection model.} The dataset consists of 2,000 women, 1,343 of whom work. Create a new variable \texttt{inlf} \emph{(In-Labor-Force)}, which is equal to 1, if the wage is observed, and equal to 0, if the wage is unobserved.
\item Assume that the hourly wage is a function of education and age, whereas the likelihood of working (the likelihood of the wage being observed) is a function of marital status, the number of children at home, and (implicitly) the wage (via the inclusion of age and education). Explain why this is a sample selection problem and the wage would be upward biased if one performed ordinary least squares.
\item Use \texttt{wage} as endogenous variable $y_{t}$ and \texttt{inlf} as selection variable $z_{t}$. Define the vectors
\begin{equation*}
W_{t}=\left[
\begin{array}{c}
\text{constant}\\
\text{married} \\
\text{children} \\
\text{age} \\
\text{educ}%
\end{array}%
\right] ,\quad X_{t}=\left[
\begin{array}{c}
\text{constant}\\
\text{educ}\\
\text{age}
\end{array}%
\right]
\end{equation*}%
and numerically compute the maximum likelihood estimates $\hat{\beta},\hat{%
\gamma},\hat{\rho}$ and $\hat{\sigma}$.
\item Calculate the standard errors for all parameters.
\end{enumerate}

\begin{solution}
\textbf{Heckman regression}

\begin{enumerate}
\item Each observation contributes a factor to the likelihood function:
\begin{align*}
I(z_t=0)Pr(z_t=0) + I(z_t=1)Pr(z_t=1)f(y_t^*|z_t=1)
\end{align*}
where $I(z_t=0)$ is the indicator function and $f(y_t^*|z_t=1)$ denotes the density of $y_t^*$ conditional on $z_t=1$. The loglikelihood function is
\begin{align*}
\sum_{z_t=0}log Pr(z_t=0) + \sum_{z_t=1}log Pr(z_t=1)f(y_t^*|z_t=1) = \sum_{z_t=0}log Pr(z_t=0) + \sum_{z_t=1}log\left(Pr(z_t=1|y_t^*)f(y_t^*)\right)
\end{align*}
where $f(y_t^*)$ is the normal density with mean $X_t\beta$ and variance $\sigma^2$. The first term, where $z_t=0$, is the same as in a probit model.

Lets calculate $Pr(z_t=1|y_t^*)$. Since $u_t$ and $v_t$ are bivariate normal, we can write $v_t=\rho u_t/\sigma + \varepsilon_t$, where $\varepsilon_t$ is a normally distributed random variable with mean 0 and variance $(1-\rho^2)$. Thus
\begin{align*}
z_t^* = W_t\gamma+\rho(y_t^*-X_t\beta)/\sigma + \varepsilon_t
\end{align*}
Because $y_t=y_t^*$ for $z_t=1$, it follows that
\begin{align*}
Pr(z_t=1|y_t^*) = \Phi\left(\frac{W_t\gamma + \rho(y_t-X_t\beta)/\sigma}{(1-\rho^2)^{1/2}}\right)
\end{align*}
Combining everything the loglikelihood is given by
\begin{align*}
\sum_{z_t=0}log\Phi(-W_t\gamma) + \sum_{z_t=1}log\left(\frac{1}{\sigma}\phi\left((y_t-X_t\beta)/\sigma\right)\right) + \sum_{z_t=1} log \Phi\left(\frac{W_t\gamma + \rho(y_t-X_t\beta)/\sigma}{(1-\rho^2)^{1/2}}\right)
\end{align*}

\item Make variable in-labor-force if wage is observed:
\begin{verbatim}
library(foreign); womanwk <- read.dta(file.choose());
womanwk$inlf <- rep(1,dim(womanwk)[1]); womanwk$inlf[is.na(womanwk$wage)] <- 0
\end{verbatim}

\item In the underlying model women choose whether to work and if they do, we observe their wages. If women made this decision randomly, we could ignore that not all wages are observed and use ordinary regression to fit a wage model. Such an assumption of random participation, however, is unlikely to be true; women who would have low wages may be unlikely to choose to work, and thus the sample of observed wages is biased upward. In the jargon of economics, women choose not to work when their personal reservation wage is greater than the wage offered by employers. Thus women who choose not to work might have even higher offer wages than those who do work—they may have high offer wages, but they have even higher reservation wages. We could tell a story that competency is related to wages, but competency is rewarded more at home than in the labor force In any case, in this problem—which is the paradigm for most such problems—a solution can be found if there are some variables that strongly affect the chances for observation (the reservation wage) but not the outcome under study (the offer wage). Such a variable might be the number of children in the home. (Theoretically, we do not need such identifying variables, but without them, we depend on functional form to identify the model. It would be difficult for anyone to take such results seriously because the functional form assumptions have no firm basis in theory.)

\item The code might look like this
\begin{verbatim}
################################################
#### Sample selectivity: Heckman regression ####
################################################
rm(list=ls())
dev.off()
cat("\014")
library(foreign)
womanwk <- read.dta(file.choose())
# make variable in-labor-force if wage is observed
womanwk$inlf <- rep(1,dim(womanwk)[1])
womanwk$inlf[is.na(womanwk$wage)] <- 0

negloglik <- function(param,dat){
  y <- as.matrix(dat$wage)
  X <- as.matrix(cbind(1,dat$educ, dat$age))
  z <- as.matrix(dat$inlf)
  W <- as.matrix(cbind(1, dat$married, dat$children, dat$age, dat$educ))

  bet <- param[1:dim(X)[2]]
  gam <- param[(dim(X)[2]+1):(dim(X)[2]+dim(W)[2])]
  rho_aux <- param[dim(X)[2]+dim(W)[2]+1] #rho_aux is unconstrained
  #rho <- -1 + 0.5*(1+1)*(1+tanh(rho_aux)) #this constraints rho to be between -1 and 1
  rho <- (exp(2*rho_aux)-1)/(1+exp(2*rho_aux)) #this constraints rho to be between -1 and 1
  sigm_aux <- param[dim(X)[2]+dim(W)[2]+2]#sigm_aux is unconstrained
  sigm <- exp(sigm_aux) #sigm is positive
  X_bet <- X%*%as.matrix(bet)
  W_gam <- W%*%as.matrix(gam)
  sum1 <- sum(log(pnorm(-W_gam[z==0])))
  sum2 <- sum(log(1/sigm*dnorm((y[z==1]-X_bet[z==1])/sigm)))
  sum3 <- sum(log(pnorm( (W_gam[z==1]+rho*(y[z==1]-X_bet[z==1])/sigm)/(sqrt(1-rho^2)) )))
  return (-(sum1+sum2+sum3))
}

#Finding good starting values is hard, an alternative method is Heckman's two-step method
#Step1: Probit regression of selection equation
y <- as.matrix(womanwk$wage)
X <- as.matrix(cbind(1,womanwk$educ, womanwk$age))
z <- as.matrix(womanwk$inlf)
W <- as.matrix(cbind(1, womanwk$married, womanwk$children, womanwk$age, womanwk$educ))
step1 <- glm(z ~ -1 + W, family=binomial(link="probit"))
gam_anf <- coef(step1)
#Step 2: OLS regression with inverse Mills ratio as additional regressor
aux <- dnorm(W%*%as.matrix(gam_anf))/pnorm(W%*%as.matrix(gam_anf))
step2 <- lm(y ~ -1 + X + aux)
bet_anf <- coef(step2)[1:dim(X)[2]]
rho_anf <- c(0.7)
sigm_anf <- coef(step2)[dim(X)[2]+1]/rho_anf
par_anf <- c(bet_anf,gam_anf,rho_anf,sigm_anf); names(par_anf) <- NULL;

#Maximum Likelihood with constraints: rho in (-1;1) and sigm>0
#par_anf[9] <- tanh( 2*(par_anf[9]+1)/(1+1)-1) #rescale rho
par_anf[9] <- 1/2*log((1+par_anf[9])/(1-par_anf[9])) #rescale rho
par_anf[10] <- log(par_anf[10]) #rescale sigm
obj <- optim(par=par_anf,negloglik,dat=womanwk, hessian=T)
obj$value

#Show parameters, rescale rho and sigm
estimates <- obj$par
#estimates[9] <- -1 + 0.5*(1+1)*(1+tanh(estimates[9]))
estimates[9] <- (exp(2*estimates[9])-1)/(1+exp(2*estimates[9]))
estimates[10] <- exp(estimates[10])
print(estimates)
diag(solve(obj$hessian))
\end{verbatim}

\end{enumerate}
\end{solution}
\newpage
\subsection{Count data}\label{mlcountdata}

The standard multiple linear regression model is not working properly if the
endogenous variable $y_{i}$ takes on only small integer values. In this case
one should use \textquotedblleft count data\textquotedblright\ regression
methods. We consider a fictional application of the simplest count data
regression model -- the Poisson regression model. Let $y_{i}$ denote the
number of goals scored by soccer player $i$ (e.g. during a championship).
Assume that $y_{i}$ has a Poisson distribution with probability function%
\begin{equation*}
P\left( y_{i}=k\right) =\frac{e^{-\mu _{i}}\mu _{i}^{k}}{k!}.
\end{equation*}%
The parameter $\mu _{i}$ of the Poisson distribution depends on exogenous
variables such that,%
\begin{equation*}
\mu _{i}=\exp \left( X_{i}^{\prime }\beta \right) .
\end{equation*}%
The vector of exogenous variables $X_{i}$ includes a constant of unity,
position (1=striker, 0=else), age, age$^{2}$, training time (in hours per
week), fixed salary, and goal bonus (both in 1000 Euro).

\begin{enumerate}
\item Load the artificial dataset \texttt{players.csv}. It contains
information about 300 players.

\item Write an R program to estimate the vector of coefficients $\beta $ by
maximum likelihood.

\item Compute $\hat{\beta}$ and its standard errors.

\item What is the probability that a striker aged 25 scores more than 3
goals if he is training 15 hours per week, has a fixed salary of 700,000
Euro and receives no bonuses.
\end{enumerate}

\begin{solution}
\textbf{Count data}

The loglikelihood function is
\begin{align*}
\sum_{i=1}^{n}\left(-exp(x_i\beta) + y_i X_i\beta - log(y_i!)\right)
\end{align*}
The code might look like this:
\begin{verbatim}
####################
#### Count data ####
####################
rm(list=ls()); dev.off(); cat("\014");

players <- read.csv(choose.files())

## Negative Log-likelihood
neg_ll <- function(bet,dat){
  position <- dat$position
  age <- dat$age
  training <- dat$training
  salary <- dat$salary
  bonus <- dat$bonus
  y <- dat$goals
  X <- cbind(1,position,age,age^2,training,salary,bonus)
  ll <- sum(-exp(X%*%bet)+y*X%*%bet-log(factorial(y)))
  return(-ll)
}
#Log-likelihood maximieren
out <- optim(par=c(-1,0.01,0.01,-0.01,0.01,0.01,0.01), fn=neg_ll,dat=players,hessian=T)
par <- out$par

## Standardabweichung der einzelnen Parameter
sdbeta <- round(sqrt(diag(solve(out$hessian))),8)
sdbeta

#Wahrscheinlichkeit, über drei Tore
mu <- (1*par[1]+1*par[2]+25*par[3]+25^2 * par[4] + 15*par[5]+ 700 *
         par[6] + 0)

1-ppois(3,mu)
#0.001064403
# Sehr unwahrscheinlich unter diesen Gegebenheiten über drei Tore zu schießen.
\end{verbatim}
\end{solution}

\subsection{Stochastic frontier analysis\label{mlsfa}}

See Greene, 2008, section 16.9.5a. Consider the Cobb-Douglas production
function%
\begin{equation*}
y=Ax_{1}^{\alpha }x_{2}^{\beta }.
\end{equation*}%
By definition, the production function returns the maximal output for given
inputs, and actual production cannot be larger than $y$. Due to
inefficiencies, actual production could be modeled (in logs) as%
\begin{equation*}
\ln y=\ln A+\alpha \ln x_{1}+\beta \ln x_{2}-u
\end{equation*}%
where $u$ is a non-negative random variable. Since other disturbances (e.g.
measurement errors) can enter the production function, it is more common to
add another, symmetrically distributed, disturbance term,%
\begin{equation*}
\ln y=\ln A+\alpha \ln x_{1}+\beta \ln x_{2}-u+v.
\end{equation*}%
Assume that $u\sim Exp\left( \lambda \right) $ and $v\sim N(0,\sigma ^{2})$
are independent.

\begin{enumerate}
\item Show that the density function of $\varepsilon =v-u$ is%
\begin{equation*}
f_{\varepsilon }\left( x\right) =\lambda\exp \left( \lambda x+%
\frac{1}{2}\lambda ^{2}\sigma ^{2}\right) \Phi \left( \frac{-x}{\sigma }%
-\lambda \sigma \right)
\end{equation*}%
where $\Phi $ is the cdf of $N(0,1)$. Hint: $f_{v-u}(x)=\int_{0}^{\infty
}f_{v}\left( u+x\right) f_{u}\left( u\right) du$.

\item Write an R program to estimate the parameters $A,\alpha ,\beta
,\lambda $ and $\sigma $ by maximum likelihood.

\item Load the dataset \texttt{sfa.csv}. This dataset is an abbreviated
version of table F7.2 of Greene, 2008. The original data appeared in Zellner
and Revankar, \textquotedblleft Generalized Production
Functions\textquotedblright , \emph{Review of Economic Studies}, 36 (1969)
241-250. Reported is the value added in the transportation equipment
manufacturing industries of 25 US states and capital and Labour inputs.
Compute the ML estimates and their standard errors.

\item Tabulate the estimated inefficiencies for the 25 states.
\end{enumerate}

\begin{solution}
\textbf{Stochastic frontier analysis}

\begin{enumerate}
\item The density of the merged error term $\varepsilon $ is%
\begin{eqnarray}
f_{\varepsilon }\left( x\right)  &=&f_{v-u}\left( x\right)   \notag \\
&=&\int_{0}^{\infty }f_{v}\left( u+x\right) \cdot f_{u}\left( u\right) du
\notag \\
&=&\int_{0}^{\infty }\phi \left( \frac{u+x}{\sigma }\right) \cdot \lambda
e^{-\lambda u}du  \notag \\
&=&\int_{0}^{\infty }\frac{1}{\sqrt{2\pi }}e^{-\frac{1}{2}\left( \frac{u+x}{%
\sigma }\right) ^{2}}\lambda e^{-\lambda u}du  \notag \\
&=&\frac{\lambda }{\sqrt{2\pi }}\int_{0}^{\infty }\exp \left( -\frac{1}{2}%
\left( \frac{u+x}{\sigma }\right) ^{2}-\lambda u\right) du.  \label{eq1}
\end{eqnarray}%
The integral in (\ref{eq1}) can be manipulated in a way similar to exercise %
\ref{moments}.2,%
\begin{eqnarray*}
&&\int_{0}^{\infty }\exp \left( -\frac{1}{2}\left( \frac{u+x}{\sigma }%
\right) ^{2}-\lambda u\right) du \\
&=&\int_{0}^{\infty }\exp \left( -\frac{u^{2}+2u\left( x+\lambda \sigma
^{2}\right) +x^{2}}{2\sigma ^{2}}\right) du \\
&=&\int_{0}^{\infty }\exp \left( -\frac{u^{2}+2u\left( x+\lambda \sigma
^{2}\right) +\left( x+\lambda \sigma ^{2}\right) ^{2}+x^{2}-\left( x+\lambda
\sigma ^{2}\right) ^{2}}{2\sigma ^{2}}\right) du \\
&=&\int_{0}^{\infty }\exp \left( -\frac{\left( u+\left( x+\lambda \sigma
^{2}\right) \right) ^{2}+x^{2}-\left( x+\lambda \sigma ^{2}\right) ^{2}}{%
2\sigma ^{2}}\right) du \\
&=&\exp \left( -\frac{x^{2}-\left( x+\lambda \sigma ^{2}\right) ^{2}}{%
2\sigma ^{2}}\right) \int_{0}^{\infty }\exp \left( -\frac{1}{2}\left( \frac{%
u-\left( -x-\lambda \sigma ^{2}\right) }{\sigma }\right) ^{2}\right) du.
\end{eqnarray*}%
Substituting the integral in (\ref{eq1}) we arrive at%
\begin{eqnarray*}
f_{\varepsilon }\left( x\right)  &=&\lambda \exp \left( -\frac{x^{2}-\left(
x+\lambda \sigma ^{2}\right) ^{2}}{2\sigma ^{2}}\right)  \\
&&\times \int_{0}^{\infty }\frac{1}{\sqrt{2\pi }}\exp \left( -\frac{1}{2}%
\left( \frac{u-\left( -x-\lambda \sigma ^{2}\right) }{\sigma }\right)
^{2}\right) du.
\end{eqnarray*}%
The first factor can be simplified to%
\begin{equation*}
\lambda \exp \left( -\frac{x^{2}-\left( x+\lambda \sigma ^{2}\right) ^{2}}{%
2\sigma ^{2}}\right) =\lambda \exp \left( \lambda x+\frac{\sigma ^{2}\lambda
^{2}}{2}\right) ,
\end{equation*}%
and the integrand in the second factor is simply the density of a normal
distribution with mean $\left( -x-\lambda \sigma ^{2}\right) $ and variance $%
\sigma ^{2}$. Thus the value of the integral can be derived from the cdf of $%
N(0,1)$ as follows,%
\begin{equation*}
\int_{0}^{\infty }\frac{1}{\sqrt{2\pi }}\exp \left( -\frac{1}{2}\left( \frac{%
u-\left( -x-\lambda \sigma ^{2}\right) }{\sigma }\right) ^{2}\right) du=\Phi
\left( \frac{-x-\lambda \sigma ^{2}}{\sigma }\right) .
\end{equation*}%
In sum, the density of $\varepsilon $ is%
\begin{equation*}
f_{\varepsilon }\left( x\right) =\lambda \exp \left( \lambda x+\frac{\sigma
^{2}\lambda ^{2}}{2}\right) \Phi \left( \frac{-x-\lambda \sigma ^{2}}{\sigma
}\right)
\end{equation*}%
and its logarithm is%
\begin{equation*}
\ln f_{\varepsilon }\left( x\right) =\ln \lambda +\lambda x+\frac{\sigma
^{2}\lambda ^{2}}{2}+\ln \Phi \left( \frac{-x-\lambda \sigma ^{2}}{\sigma }%
\right) .
\end{equation*}

\item The code might look like this:
\begin{verbatim}
######################################
#### Stochastic Frontier Analysis ####
######################################

##Log-Likelihood Funktion, die maximiert werden soll
neg_log_likeli <- function(theta, daten){
  A <- theta[1]
  alpha <- theta[2]
  beta <- theta[3]
  lambda <- theta[4]
  sigma <- theta[5]

  x1 <- daten[,3]
  x2 <- daten[,4]
  y <- daten[,2]

  logA <- log(A)
  logy <- log(y)
  logx1 <- log(x1)
  logx2 <- log(x2)

  eps <- logy - logA - alpha*logx1 - beta*logx2

  log_likeli <- sum(log(lambda*exp(lambda*eps+lambda^2/2*sigma^2)*pnorm(
                                  -eps/sigma-lambda*sigma,mean=0,sd=1)))
  return(-log_likeli)
}

daten <- read.csv(file.choose())
head(daten)

##Schätzung der Parameter
opt <- optim(c(7, 0.3, 0.7, 10, 3), neg_log_likeli, daten=daten, hessian=T)
opt

A <- opt$par[1]
alpha <- opt$par[2]
beta <- opt$par[3]
lambda <- opt$par[4]
sigma <- opt$par[5]

##Standardfehler: Problem mit negativen Werten, hier kleine Stichprobe das Problem
diag(solve(opt$hessian))

##Geschätzte Ineffizienzen
epsilon <- log(daten$ValueAdd)-log(A)-alpha*log(daten$Capital)-beta*log(daten$Labor)
plot(epsilon)
hist(epsilon, freq=F)
curve(lambda*exp(lambda*x+lambda^2/2*sigma^2)*pnorm(-x/sigma-lambda*sigma, mean=0, sd=1),
                                                        from=-1, to=1, add=T, col="red")
\end{verbatim}
\end{enumerate}

\begin{itemize}
  \item SFA are also called \emph{composed error models}. In a production context efficiency loss is modelled as $-u$, in a cost context as $+u$.
  \item SFA models are also used to measure efficiency of the banking system, level of competitiveness of a market, quality of inputs and outputs, regulation, management evaluation,\dots
  \item Basic idea is that the ratio of observed output to maximum possible output is less or equal to 1.
  \item How to estimate inefficiency?
  \begin{itemize}
    \item We can compute residuals $\hat{\varepsilon_t} = \widehat{u_t+v_t}$. Since $E(v_t)=0$ we can conclude that if the residual is high, then so is the inefficiency.
    \item Jondrow, Lovell, Materov, Schmidt (JLMS) approach: Look at the mean (or mode) of the conditional distribution.
  \end{itemize}
\end{itemize}
\end{solution}


\subsection{ARCH models\label{arch}}

Models with autoregressive conditional heteroscedasticity have many
applications in empirical finance. We only consider the simple case of an $%
ARCH(1)$-process. Let $X_{t}$ denote the stock return in period $t$. Suppose%
\begin{equation*}
X_{t}=\sigma _{t}\varepsilon _{t}
\end{equation*}%
with $\varepsilon _{t}\sim N(0,1)$ and%
\begin{equation*}
\sigma _{t}^{2}=\omega +\alpha X_{t-1}^{2}.
\end{equation*}

\begin{enumerate}
\item Factorize the joint density function of $X_{1},\ldots ,X_{T}$.

\item Ignore the marginal density of $X_{1}$ and write an R function to
compute the log-likelihood of $X_{2},\ldots ,X_{T}$.

\item Load the (artificial) dataset \texttt{arch1bsp.csv} and estimate $%
\omega $ and $\alpha $ by maximizing the log-likelihood numerically.

\item Compute the covariance matrix of $\hat{\omega},\hat{\alpha}$.
\end{enumerate}

\begin{solution}
\textbf{ARCH models}

For ARCH models we have:
\begin{align*}
E(X_t|X_{t-1}) = E(\varepsilon_t \sigma_t|X_{t-1}) = E(\varepsilon_t\sqrt{\omega+\alpha X_{t-1}^2}|X_{t-1}) =\sqrt{\omega+\alpha X_{t-1}^2} E(\varepsilon_t|X_{t-1}) = 0
\end{align*}
and
\begin{align*}
Var(X_t|X_{t-1}) = Var(\varepsilon_t \sigma_t|X_{t-1}) = Var(\varepsilon_t\sqrt{\omega+\alpha X_{t-1}^2}|X_{t-1}) = (\omega+\alpha X_{t-1}^2)\underbrace{E(\varepsilon_t^2|X_{t-1})}_{=1} = \omega+\alpha X_{t-1}^2
\end{align*}
since $\varepsilon_t$ is independent of $X_{t-1}$. Given that $\varepsilon_t \sim N(0,1)$, the conditional density of $X_t$ given $X_{t-1}$ is
\begin{align*}
X_t|X_{t-1} \sim N(0,\omega+\alpha X_{t-1}^2)
\end{align*}
In general we can factorize a joint likelihood function:
\begin{align*}
  f_{X_1,\dots,X_T}(x_1,\dots,x_T) = \prod_{t=1}^{T}f_{X_t|X_{t-1},\dots,X_1}(x_t|X_{t-1,\dots,x_1})
\end{align*}
In our case:
\begin{align*}
  f_{X_1,\dots,X_T}(x_1,\dots,x_T) = f_{X_1}(x_1)\prod_{t=2}^{T}\frac{1}{\sqrt{2\pi(\omega+\alpha x_{t-1}^2)}}\exp\left(-\frac{1}{2}\left(\frac{x_t}{\omega+\alpha x_{t-1}^2}\right)^2\right)
\end{align*}
We can ignore $f_{X_1}(x_1)$ for large sample sizes, since it contribution is relatively small the larger the sample size. The log-likelihood is then given by
\begin{align*}
  -\frac{T-2}{2}log(2\pi)-\frac{1}{2}\sum_{t=2}^{T}log(\omega+\alpha x_{t-1}^2) - \frac{1}{2} \sum_{t=2}^{T} \left(\frac{x_t}{\omega+\alpha x_{t-1}^2}\right)^2
\end{align*}
The code might look like this
\begin{verbatim}
#####################
#### ARCH Models ####
#####################

arch1_ll <- function(theta, tseries) {
  omega <- theta[1]
  alpha <- theta[2]
  Te <- length(tseries)
  return((Te - 1) / 2 * log(2 * pi)
         + 0.5 * sum(log(omega + alpha * tseries[1:(Te - 1)]^2))
         + 0.5 * sum((tseries[2:Te]^2 /
                        (omega + alpha * tseries[1:(Te - 1)]^2))))
}

arch1bsp <- read.csv(file.choose(), header = TRUE)

estimate <- optim(par = c(0.2, 0.9), fn = arch1_ll, tseries = arch1bsp$x,
                  hessian = TRUE)
estimate$par
solve(estimate$hessian)

gitter <- seq(0.05, 0.95, by = 0.01)
value_omega <- rep(NA, length(gitter))
value_alpha <- rep(NA, length(gitter))
for (i in 1:length(gitter)) {
  value_omega[i] <- -arch1_ll(c(gitter[i], estimate$par[2]),
                              tseries = arch1bsp$x)
  value_alpha[i] <- -arch1_ll(c(estimate$par[1], gitter[i]),
                              tseries = arch1bsp$x)
}
plot(gitter, value_omega, type = "l", xlab = expression(omega),
     ylab = "",
     main = expression(paste("log-Likelihood in Abhängigkeit von ", omega,
                             ", ", alpha, " = 0.9376")), lwd = 2,
     cex.axis = 1.5, cex.lab = 2, cex.main = 2)
mtext(text = expression(paste("logL(", omega, ")")), side = 2, cex = 2,
      line = 2.2)
abline(h = -estimate$value, col = "red")
abline(v = estimate$par[1], col = "red")

plot(gitter, value_alpha, type = "l", xlab = expression(alpha),
     ylab = "",
     main = expression(paste("log-Likelihood in Abhängigkeit von ", alpha,
                             ", ", omega, " = 0.2056")), lwd = 2,
     cex.axis = 1.5, cex.lab = 2, cex.main = 2)
mtext(text = expression(paste("logL(", alpha, ")")), side = 2, cex = 2,
      line = 2.2)
abline(h = -estimate$value, col = "red")
abline(v = estimate$par[2], col = "red")

# Unbedingte Varianz
estimate$par[1] / (1 - estimate$par[2])
\end{verbatim}
\begin{itemize}
\item Volatility is not observable. To measure volatility by the empirical standard deviation is only valid if volatility is relatively stable, but usually we have time-variation in volatility.
\item Idea of ARCH is that volatility is dependent on past observations (conditional distribution is time-varying), however, the unconditional distribution is still stationary.
\end{itemize}
\end{solution}


\subsection{Duration models\label{duration}}

There is a huge number of duration models, but we only consider a
particularly easy case, see Davidson and MacKinnon, 2004, pp. 490ff. Suppose
that how long a state endures is measured by a non-negative random variable $%
T$ with density function $f(t)$ and cdf $F(t)$. Define the survival function
$S(t)=1-F(t)$ and the hazard function%
\begin{equation*}
h\left( t\right) =\frac{f\left( t\right) }{S\left( t\right) }.
\end{equation*}%
The survivor function measures the probability that a state which stated at time $t=0$ is sill going on at time $t$. The hazard function can be interpreted as the probability that the state
ends in the next instant, given it has not ended yet.

\begin{enumerate}\setlength{\itemsep}{0pt}
\item Let $T$ have the cdf $F(t;\theta ,\alpha )=1-\exp \left( -\left( \theta t\right) ^{\alpha}\right)$
with parameters $\theta $ and $\alpha $ (note that this is the Weibull distribution). Derive the density $f\left(
t\right) $, the survival function $S(t)$ and the hazard function $h(t)$. Interpret the parameter $\alpha$.

\item Assume that $n$ completed (independent) durations $t_{1}$,..,$t_{n}$
have been observed. Derive the log-likelihood function under the assumption that parameter $\theta $ depends on some exogenous vector $X_{i} $ in the following way,
\begin{equation*}
\theta _{i}=\exp \left( X_{i}^{\prime }\beta \right) .
\end{equation*} Use $f(t)=h(t)S(t)$ to split the log-likelihood function into two sums and rewrite the log-likelihood accordingly. Note that, if some spells are incomplete (i.e. they have not ended yet) the log-likelihood can be adapted easily by simply dropping their contributions to the hazard part of the log-likelihood.

\item Load the artificial dataset \texttt{spells.csv}. The first variable is
the duration, the other three variables are exogenous (one is the
intercept). Spells with duration 0.5 are incomplete. Estimate the parameters
$\beta _{1},\beta _{2},\beta _{3}$, and $\alpha $ and their standard errors
by maximum likelihood.
\end{enumerate}

\begin{solution}
\textbf{Duration models}

\begin{enumerate}
\item Obtaining the survivor function is very easy:
\begin{align*}
  S(t) \equiv 1-F(t) = exp(-(\theta t)^\alpha)
\end{align*}
For the pdf
\begin{align*}
  f(t) = \frac{\partial F(t)}{\partial t} = \alpha \theta^\alpha t^{\alpha-1} exp(-(\theta t)^\alpha)
\end{align*}
The hazard function is then given by
\begin{align*}
h(t) \equiv \frac{f(t)}{S(t)} = \frac{ \alpha \theta^\alpha t^{\alpha-1} exp(-(\theta t)^\alpha)}{exp(-(\theta t)^\alpha)} = \alpha \theta^\alpha t^{\alpha-1}
\end{align*}
When $\alpha=1$, the Weibull distribution collapses to the exponential and the hazard is just a constant (duration independent). For $\alpha<1$, the hazard is decreasing over time (negative duration dependence) and for $\alpha>1$ it is increasing (positive duration dependence).
\item Taking the log of $f(t_i)$ yields
\begin{align*}
  log(f(t_i)) = log\left(h(t_i)\right) + log\left(S(t_i)\right) = log(\alpha_i) +\alpha_i log(\theta_i) + (\alpha_i-1)log(t_i) -(\theta_i t_i)^\alpha_i
\end{align*}
Since $\theta_i = exp(X_i\beta)$ and $\alpha_i=\alpha$ for all i, we have
\begin{align*}
  log(f(t_i,X_i,\alpha,\beta)) = log(\alpha) +\alpha X_i \beta + (\alpha-1)log(t_i) - t_i^\alpha \cdot exp(\alpha X_i \beta)
\end{align*}
Summing over all n independent observations gives the log-likelihood
\begin{align*}
log(f(t,X,\alpha,\beta)) = n log(\alpha) + \alpha \sum_{i=1}^{n} X_i\beta + (\alpha-1) \sum_{i=1}^{n}log(t_i) - \sum_{i=1}^{n} t_i^\alpha \cdot exp(\alpha X_i \beta)
\end{align*}
Since data sets contain observations for which $t_i$ is not actually observed (e.g. sample of people who entered unemployment at various points in time, then it is extremely likely that some peaple in the sample were still unemployed when data collection ended). We can deal with this censoring, i.e. the observed $t_i$ is the duration of an incomplete spell, it is the logarithm of the probability of censoring, which is the probability that the duration exceed $t_i$, that is, the log of the survivor function. Denote U as the set of $n_u$ uncensored observations, the loglikelihood function for the entire sample is then
\begin{align*}
log(f(t,X,\alpha,\beta)) = n_u log(\alpha) + \alpha \sum_{i\in U} X_i\beta + (\alpha-1) \sum_{i\in U}log(t_i)  - \sum_{i=1}^{n} t_i^\alpha \cdot exp(\alpha X_i \beta)
\end{align*}
Uncensored observations contribute to both terms, while censored observations contribute only to the Survivor function.
\item The code might look like this
\begin{verbatim}
#########################
#### Duration models ####
#########################

## Load data
spells <- read.csv(file.choose())
View(spells)

##Negative log-likelihood
neg_log_likeli <- function(param, dat) {
  beta <- as.matrix(param[1:3])
  alpha <- param[4]

  t <- dat$duration
  const <- dat$const
  x1 <- dat$X1
  x2 <- dat$X2
  X <- as.matrix(cbind(const,x1,x2))
  idx_uncens <- which(t<0.5)
  nu <- length(idx_uncens)
  log_likeli <- nu*log(alpha) + alpha*sum(X[idx_uncens,]%*%beta)
            + (alpha-1)*sum(log(t[idx_uncens])) - sum((t^alpha)*exp(alpha*X%*%beta))
  return(-log_likeli)
}

##Schätzung der Parameter
par_anf <- c(1,0.7,0.3, 1.3)
neg_log_likeli(par_anf,spells)
opt <- optim(par=par_anf, neg_log_likeli, dat=spells, hessian=T)
print(opt$par)

##Standardfehler
diag(solve(opt$hessian))
\end{verbatim}
\end{enumerate}
\begin{itemize}
\item The terminology like survival or hazard is due to evolutionary concepts, however, duration models in economics are quite often found in the labor market literature. Also: strike duration, state of being single (until marriage), etc.
\end{itemize}
\end{solution}



\subsection{Ultra-high-frequency data\label{uhfdata}}

A model of the duration between individual transactions on stock exchanges
has been suggested by Engle and Russell, \textquotedblleft Autoregressive
Conditional Duration: A New Model for Irregularly Spaced Transaction
Data\textquotedblright , \emph{Econometrica}, 66 (1998) 1127-1162. The
article can be downloaded (password protected pdf) from the internet site of
this course. Let $X_{i}$ denote the duration between transaction $i-1$ and
transaction $i$. The model assumes that%
\begin{equation*}
X_{i}\sim \psi _{i}\varepsilon _{i}
\end{equation*}%
where $\varepsilon _{i}$ is i.i.d. standard exponentially distributed with
density function $e^{-x}$. The scale parameter depends on previous
observations in a way similar to ARCH models%
\begin{equation*}
\psi _{i}=\omega +\sum_{j=1}^{p}\alpha _{j}X_{i-j}.
\end{equation*}%
For simplicity, we set $p=1$.

\begin{enumerate}\setlength{\itemsep}{0pt}
\item Factorize the joint density function of $X_{1},\ldots ,X_{T}$.

\item Ignore the marginal density of $X_{1}$ and write an R function to
compute the log-likelihood of $X_{2},\ldots ,X_{T}$.

\item Load the (artificial) dataset \texttt{acd1bsp.csv} and estimate $%
\omega $ and $\alpha _{1}$ by maximizing the log-likelihood numerically.

\item Compute the covariance matrix of $\hat{\omega},\hat{\alpha}_{1}$.
\end{enumerate}

\begin{solution}
\textbf{Ultra-high-frequency data}
\end{solution}


\subsection{Spatial dependence}

Observations may not only be dependent over time, but also over space. For
instance, real estate prices can be influenced by prices in neighboring
regions. A simple case of spatial dependence is the spatial autoregressive
model,%
\begin{equation}
y=\rho Wy+\alpha +\delta z+u  \label{sarmodel}
\end{equation}%
where $y$ is an $\left( n\times 1\right) $-vector of endogenous variables, $%
W $ is a symmetric $\left( n\times n\right) $-weight matrix, $Z$ is an $%
\left( n\times 1\right) $-vector of a (single) exogenous variable, $u\sim
N\left( 0,\sigma ^{2}I\right) $ is an $\left( n\times 1\right) $-vector of
disturbances. The unknown parameters of the model are $\alpha ,\delta ,\rho $%
, and $\sigma $, the spatial autocorrelation is driven by the parameter $%
\rho $. The weight matrix $W$ can be specified in a number of ways. Often,
element $W_{ij}$ simply indicates if regions $i$ and $j$ are direct
neighbors, $W_{ij}>0$, or not, $W_{ij}=0$. If $m_{i}$ is the number of
direct neighbors of $i$, then $W_{ij}=1/m_{i}$, such that $\sum_{j}W_{ij}=1$%
. Since the model (\ref{sarmodel}) cannot be estimated consistently by OLS,
we perform a maximum likelihood estimation of the parameters.

\begin{enumerate}
\item Solve (\ref{sarmodel}) for $y$ and derive its multivariate normal
distribution (ie. its expectation vector and its covariance matrix).

\item Use the multivariate distribution of $y$ to show that the
log-likelihood function is%
\begin{equation*}
-\frac{n}{2}\ln \left( 2\pi \sigma ^{2}\right) +\ln \left( \det \left(
I_{n}-\rho W\right) \right) -\frac{\left( y-\rho Wy-\alpha -\delta z\right)
^{\prime }\left( y-\rho Wy-\alpha -\delta z\right) }{2\sigma ^{2}}
\end{equation*}%
Hints: If $X\sim N\left( \mu ,\Sigma \right) $ is multivariate normal with $%
K $ dimensions, then its density at $x=(x_{1},\ldots ,x_{K})^{\prime }$ is $%
f_{X}\left( x\right) =(2\pi )^{-K/2}\left[ \det (\mathbf{\Sigma })\right]
^{-1/2}\cdot \exp \left( -\frac{1}{2}\left( \mathbf{x}-\mathbf{\mu }\right)
^{\prime }\mathbf{\Sigma }^{-1}\left( \mathbf{x}-\mathbf{\mu }\right)
\right) $. The following results for determinants (of suitable matrices) may
also help: $\det \left( AB\right) =\det A\det B$, $\det (aA)=a^{n}\det A$,
where $A$ is $n\times n$ and $a$ is a real scalar, and $\det (A^{-1})=\det
(A)^{-1}$.

\item Load the datasets \texttt{spatialdata.csv} and \texttt{%
neighbourhood.csv}. The first one contains data on house prices (column 1)
and disposable household income (column 2) in the 413 German
\textquotedblleft Kreise\textquotedblright ; the second one is the $%
413\times 413$ neighborhood matrix.

\item Calculate the normalized neighborhood matrix $W$ such that each row of
$W$ sums to unity.

\item Write an R program to compute the log-likelihood function and estimate
the parameters $\alpha ,\delta ,\rho $ and $\sigma $ of the model.

\item Compute the standard errors for $\hat{\alpha}$, $\hat{\delta}$, $\hat{%
\rho}$, and $\hat{\sigma}$. Test if there is significant spatial
autocorrelation.
\end{enumerate}

\begin{solution}
\textbf{Spatial dependence }

\begin{enumerate}
\item The model can be written as%
\begin{eqnarray*}
\left( I-\rho W\right) y &=&\alpha +\delta z+u \\
y &=&\left( I-\rho W\right) ^{-1}\left( \alpha +\delta z\right) +\left(
I-\rho W\right) ^{-1}u.
\end{eqnarray*}%
To keep the notation short, define $D=\left( I-\rho W\right) ^{-1}$. Since $u
$ is multivariate normal, $u\sim N(0,\sigma ^{2}I_{n})$, we find that%
\begin{equation*}
y\sim N\left( \mu ,\Sigma \right)
\end{equation*}%
with%
\begin{eqnarray*}
\mu  &=&D\left( \alpha +\delta z\right)  \\
\Sigma  &=&\sigma ^{2}DD^{\prime }
\end{eqnarray*}

\item The joint density of $y$, i.e. the likelihood function, is%
\begin{eqnarray*}
L\left( \alpha ,\delta ,\rho ,\sigma \right)  &=&(2\pi )^{-n/2}\left[ \det (%
\mathbf{\Sigma })\right] ^{-1/2}\cdot \exp \left( -\frac{1}{2}\left( \mathbf{%
x}-\mathbf{\mu }\right) ^{\prime }\mathbf{\Sigma }^{-1}\left( \mathbf{x}-%
\mathbf{\mu }\right) \right)  \\
&=&(2\pi )^{-n/2}\left[ \det (\sigma ^{2}DD^{\prime })\right] ^{-1/2} \\
&&\times \exp \left( -\frac{1}{2}\left( y-D\left( \alpha +\delta z\right)
\right) ^{\prime }\left( \sigma ^{2}DD^{\prime }\right) ^{-1}\left(
y-D\left( \alpha +\delta z\right) \right) \right) .
\end{eqnarray*}%
Consider the terms in turn,%
\begin{eqnarray*}
\left[ \det (\sigma ^{2}DD^{\prime })\right] ^{-1/2} &=&\left[ \sigma
^{2n}\det \left( DD^{\prime }\right) \right] ^{-1/2} \\
&=&\left( \sigma ^{2}\right) ^{-n/2}\det \left( D\right) ^{-1} \\
&=&\left( \sigma ^{2}\right) ^{-n/2}\det (I-\rho W)
\end{eqnarray*}%
since $\det (D)=\det (D^{\prime })$ and $\det (D)^{-1}=\det (I-\rho W)$.
Next, the term inside the exponential function is%
\begin{eqnarray*}
&&-\frac{1}{2}\left( y-D\left( \alpha +\delta z\right) \right) ^{\prime
}\left( \sigma ^{2}DD^{\prime }\right) ^{-1}\left( y-D\left( \alpha +\delta
z\right) \right)  \\
&=&-\frac{1}{2\sigma ^{2}}\left( y-D\left( \alpha +\delta z\right) \right)
^{\prime }D^{-1\prime }D^{-1}\left( y-D\left( \alpha +\delta z\right)
\right)  \\
&=&-\frac{1}{2\sigma ^{2}}\left[ D^{-1}\left( y-D\left( \alpha +\delta
z\right) \right) \right] ^{\prime }\left[ D^{-1}\left( y-D\left( \alpha
+\delta z\right) \right) \right]  \\
&=&-\frac{1}{2\sigma ^{2}}\left[ D^{-1}y-\alpha -\delta z\right] ^{\prime }%
\left[ D^{-1}y-\alpha -\delta z\right]  \\
&=&-\frac{\left( y-\rho Wy-\alpha -\delta z\right) ^{\prime }\left( y-\rho
Wy-\alpha -\delta z\right) }{2\sigma ^{2}}.
\end{eqnarray*}%
Hence, the log-likelihood function is%
\begin{eqnarray*}
\ln L\left( \alpha ,\delta ,\rho ,\sigma \right)  &=&-\frac{n}{2}\ln \left(
2\pi \sigma ^{2}\right) +\ln \left( \det \left( I-\rho W\right) \right)  \\
&&-\frac{\left( y-\rho Wy-\alpha -\delta z\right) ^{\prime }\left( y-\rho
Wy-\alpha -\delta z\right) }{2\sigma ^{2}}.
\end{eqnarray*}

\item (...)
\end{enumerate}

The code might look like this:
\begin{verbatim}
############################
#### Spatial dependence ####
############################
# 3)
# Einlesen der Datensätze
spatial = read.csv(file.choose(), header = T, sep = "")
neighbour = read.csv(file.choose(), header = T, sep = "")

# 4)
# Normalisieren der Nachbarschaftsmatrix
W.norm = neighbour / rowSums(neighbour)

# 5)
# Log-Likelihood-Funktion bestimmen und optimieren
ll_Spatial = function(y, z, W, theta){
  W = as.matrix(W)
  n = length(y)
  rho = theta[1]
  alpha = theta[2]
  delta = theta[3]
  sigma = theta[4] # hier sigma^2
  In = diag(n)
  A = t(y - rho*W%*%y - alpha - delta*z)%*%(y - rho*W%*%y - alpha - delta*z)
  loglik = -n/2*log(2*pi*sigma) + log(det(In - rho*W)) - A/(2*sigma)
  return(loglik)
}

optModel = optim(c(0.5 ,1 ,2 ,3) , ll_Spatial , y = spatial$HPrice07 ,
                 z =spatial$HHIncome07 , W = W.norm, hessian = T,
                 method = "L-BFGS-B",lower = c(-1,-Inf ,-Inf ,0),
                 upper = c(1, Inf , Inf , Inf), control = list(fnscale = -1))

# optimale Parameter des Modells
(param <- optModel$par)

# 6)
# Test auf signifikante spatiale Autokorrelation
# Hessematrix
(H = optModel$hessian)

# Geschätzte Kovarianzmatrix
(covmat <- -solve(H))

# Berechnen der Standardfehler
(errors = sqrt(diag(covmat)))

# t-Test auf signifikante Autokorrelation
param[1]/errors[1]
# Eindeutig größer als das 1-alpha/2 Quantil der N(0,1)-Verteilung (1.96)
# --> zum Niveau 5% ein von 0 verschiedene räumliche Autokorrelation
\end{verbatim}
\end{solution}

\newpage
\section{Instrumental variables\label{ivestimation}}



\subsection{The miracle of the instruments}

Since instruments are elements of information sets, one can construct an
arbitrary number of additional instruments by (nonlinear) transformations of
instruments. The following example shows that creating instruments
\textquotedblleft out of nothing\textquotedblright\ is possible but does not
work very well in practice. Consider the following simple linear model,%
\begin{equation*}
y_{t}=\alpha +\beta _{1}x_{1t}+\beta _{2}x_{2t}+u_{t}
\end{equation*}%
for $t=1,\ldots ,T$. The error term $u_{t}$ is correlated with both
exogenous variables $x_{1t},x_{2t}$ but uncorrelated with an instrument
variable $w_{t} $,%
\begin{equation*}
\left(
\begin{array}{c}
x_{1t} \\
x_{2t} \\
u_{t} \\
w_{t}%
\end{array}%
\right) \sim N\left( \left[
\begin{array}{c}
5 \\
5 \\
0 \\
5%
\end{array}%
\right] ,\left[
\begin{array}{cccc}
2 & 0.3 & 0.5 & 0.7 \\
0.3 & 1 & 0.5 & 0.7 \\
0.5 & 0.5 & 1 & 0 \\
0.7 & 0.7 & 0 & 1%
\end{array}%
\right] \right) .
\end{equation*}

\begin{enumerate}
\item Activate the packages \texttt{MASS} and \texttt{AER}. Generate a
sample of size $n=1000$ from the multivariate normal distribution using the
\texttt{mvrnorm} command of the \texttt{MASS} package. Show that the command
\texttt{ivreg} (of the \texttt{AER} package) does not work as there is only
one instrument but two endogenous regressors.

\item Write a program that performs the following steps.
\end{enumerate}

\begin{itemize}
\item Create an empty matrix $Z$ with $R=1000$ rows and 3 columns.

\item Start a \texttt{for}-loop over $r=1,\ldots ,R$.

\item Inside the loop, generate a sample of size $n=1000$ from the
multivariate normal distribution using the \texttt{mvrnorm} command of the
\texttt{MASS} package.

\item Use the columns for $x_{1},x_{2}$ and $u$ to compute the values of the
endogenous variable%
\begin{equation*}
y_{t}=1+2x_{1t}+3x_{2t}+u_{t}.
\end{equation*}

\item Use the column for $w$ to create \emph{two} instruments $w_{1}$ and $%
w_{2}$,%
\begin{eqnarray*}
w_{1t} &=&w_{t}^{2} \\
w_{2t} &=&w_{t}^{3}.
\end{eqnarray*}

\item Use the command \texttt{ivreg} of the \texttt{AER} package to compute
the IV estimation. Save the coefficient estimates $\hat{\alpha},\hat{\beta}%
_{1},\hat{\beta}_{2}$ in row $r$ of the matrix $Z$.

\item End the loop.

\item Compute the median of the three estimates $\hat{\alpha},\hat{\beta}%
_{1},\hat{\beta}_{2}$.

\item Compute the standard errors of the estimates.

\item Split the screen using the command \texttt{par(mfrow=c(3,1))} and plot
the three histograms.
\end{itemize}

\begin{solution}
\textbf{The miracle of the instruments}
The code might look like this:
\begin{verbatim}
########################################
#### The miracle of the instruments ####
########################################
#1)
library(MASS)
library(AER)

# Generate data: both exogenous variables are correlated with u
Omega <- matrix(c(
  2,0.3,0.5,0.7,
  0.3,1,0.5,0.7,
  0.5,0.5,1,0,
  0.7,0.7,0,1),nrow=4,ncol=4)

n <- 100
dat <- mvrnorm(n,c(5,5,0,5),Omega)
x1 <- dat[,1]
x2 <- dat[,2]
u <- dat[,3]
w <- dat[,4] # instrument
y <- 1+2*x1+3*x2+u
obj <- ivreg(y~x1+x2|w) #does not work

#2)
R <- 1000
Z <- matrix(NA,nrow=R,ncol=3)
ZZ <- matrix(NA,nrow=R,ncol=3)
for(r in 1:R) {
  dat <- mvrnorm(n,c(5,5,0,5),Omega)
  x1 <- dat[,1]
  x2 <- dat[,2]
  u <- dat[,3]
  w <- dat[,4] # instrument
  y <- 1+2*x1+3*x2+u

  #OLS is inconsistent
  ols <- lm(y~x1+x2)

  # IV from AER-package
  w2 <- w^2
  w3 <- w^3
  obj <- ivreg(y~x1+x2|w2+w3)

  #store estimates
  Z[r,] <- coefficients(obj)
  ZZ[r,] <- coefficients(ols)
}

print(apply(Z,2,median))
print(apply(Z,2,mean))
print(apply(Z,2,sd))

print(apply(ZZ,2,median))
print(apply(ZZ,2,mean))
print(apply(ZZ,2,sd))

par(mfrow=c(3,1))
truehist(Z[,1])
truehist(Z[,2])
truehist(Z[,3])
\end{verbatim}
\end{solution}

\subsection{Linear combinations of instruments}

This exercise is close to exercise 8.2 of Davidson and MacKinnon (2004).
Consider the simple IV estimator $\hat{\beta}_{IV}$, computed first with an $%
T\times K$ matrix $W$ of instruments, and then with another $T\times K$
matrix $WJ$, where $J$ is a $K\times K$ nonsingular matrix. Show that the
two estimators coincide. Hence, if the model is just identified, linear
combinations of the $K$ instruments have no effect.

\begin{solution}
\textbf{Linear combinations of instruments}

The two estimators are
\begin{align*}
(W'X)^{-1}W'y \text{ and } (J'W'X)^{-1}J'W'y
\end{align*}
Since $J$ and $(W'X)$ are both $k\times k$ nonsingular matrices, we get
\begin{align*}
(J'W'X)^{-1}J'W'y = (W'X)^{-1}(J')^{-1}J'W'y = (W'X)^{-1}W'y
\end{align*}
\end{solution}

\subsection{Compulsory School Attendance}

This exercise is a replication of some parts of the article
\textquotedblleft Does Compulsory School Attendance Affect Schooling and
Earnings?\textquotedblright\ by Angrist and Krueger, \emph{Quarterly Journal
of Economics }106 (1991) 979-1014.

\begin{enumerate}
\item Load the Stata dataset \texttt{AngristKrueger1991Data.dta}. For
persons born between 1930Q1 and 1939Q4, plot the years of education against
the year of birth (see Figure I in the article). Do the same for persons
born between 1940Q1 and 1949Q4 (see Figure II).

\item For 1930Q1 until 1949Q4, plot the mean log weekly earnings against the
year of birth (see Figure V).

\item From now on, we only consider persons born between 1920Q1 and 1929Q4.
Drop all other observations.\footnote{%
If you have \texttt{attach}ed the dataframe, please first delete all
variables from your workspace by \texttt{rm(list=ls())}. Then re-load the
dataset.} Regress the log weekly earnings on the years of education and a
set of nine dummies for the year of birth\footnote{%
The easiest way to deal with the dummy variable is as follows: Create a new
variable in the following way: \texttt{Dyear <- factor(yob)}.
If this variable is included as a regressor in the \texttt{lm} command, R
will automatically generate the necessary dummy variables.} using the OLS
command \texttt{lm} (see column (1) in Table IV of the article).

\item Compute age as the difference 1970 minus date-of-birth, e.g. a person
born in 1925Q3 has age $1970-1925.75=44.25$. Add age and age-squared to the
OLS regression (see column (3) in Table IV).

\item Activate the \texttt{AER} package. The \texttt{ivreg} command can be
used for instrumental variables estimation; its syntax is close to the
syntax of the \texttt{lm} command, see \texttt{?ivreg}.

The instrumental variables used by Angrist and Krueger are the year of
birth, and the year of birth interacting with the quarter of birth. To avoid
multicollinearity, one quarter per year has to be dropped from the list of
instruments. To economize on time and computer resources, define the
instrument variable as a \texttt{factor}.\footnote{%
Suppose the date of birth (\texttt{dob}) is given as 1920, 1920.25, 1920.5,
1920.75, \ldots . Then execute the following commands to create a \texttt{%
factor} of instruments:
\par
\texttt{Dq <- dob}
\par
\texttt{Dq[Dq-floor(Dq)==0.75] <- 0}
\par
\texttt{Dq <- factor(Dq)}
\par
The \texttt{factor} \texttt{Dq} can now be used as an instrument,
representing all required dummy instruments.}

Estimate an IV regression of log weekly wage on education and year dummies
using the instruments of Angrist and Krueger (see column (2) in Table IV).

\item Add age and age-squared to the IV regression (see column (4) in Table
IV).
\end{enumerate}

\begin{solution}
\textbf{Compulsory School Attendance}

The code might look like this:
\begin{verbatim}
######################################
#### Compulsory School Attendance ####
######################################
library(foreign)
library(AER)
graphics.off()

#1) Replicate Figure I and II
x <- read.dta(file.choose())
View(x)
dob <- x$yob+(x$qob-1)*0.25

# Figure I
Z <- matrix(NA,40,2)
Z[,1] <- seq(1930,1939.75,by=0.25)
for(i in 1:dim(Z)[1]) {
  Z[i,2] <- mean(x$educ[dob==Z[i,1]])
}
plot(Z,t="o",main="Figure I",xlab="Year of Birth",
     ylab="Years of Completed Education",ylim=c(12.2,13.2))

# Figure II
Z <- matrix(NA,40,2)
Z[,1] <- seq(1940,1949.75,by=0.25)
for(i in 1:dim(Z)[1]) {
  Z[i,2] <- mean(x$educ[dob==Z[i,1]])
}
plot(Z,t="o",main="Figure II",xlab="Year of Birth"
     ,ylab="Years of Completed Education",ylim=c(13,13.9))

#2) Replicate Figure V
Z <- matrix(NA,80,2)
Z[,1] <- seq(1930,1949.75,by=0.25)
for(i in 1:dim(Z)[1]) {
  Z[i,2] <- mean(x$lwklywge[dob==Z[i,1]])
}
plot(Z,t="o",main="Figure V",xlab="Year of Birth",ylab="Log Weekly Earnings")

#3) Replicate column 1 of table IV
# Backup data
backup <- x
# Drop all persons born after 1929Q4
x <- x[x$yob<1930,]
dob <- x$yob+(x$qob-1)*0.25

# Create yob dummies
Dyear <- factor(x$yob)

# Column (1)
regr <- lm(x$lwklywge ~ x$educ + Dyear)
summary(regr)

#4) Replicate column 3 of table IV
age <- 1970-dob
# Column (3)
regr <- lm(x$lwklywge~x$educ+Dyear+age+I(age^2))
summary(regr)

#5) Replicate column 2 of table IV
Dq <- dob
Dq[Dq-floor(Dq)==0.75] <- 0
Dq <- factor(Dq)
# Column (2)
regr <- ivreg(x$lwklywge~x$educ+Dyear|Dq+Dyear)
summary(regr)

#6) Replicate column 4 of table IV
# Column (4)
regr <- ivreg(x$lwklywge~x$educ+age+I(age^2)+Dyear|Dq+Dyear)
summary(regr)
\end{verbatim}
\end{solution}

\subsection{A simple example}

This \textquotedblleft simple example\textquotedblright\ is close to
exercise 8.10 of Davidson and MacKinnon (2004). Consider the model%
\begin{eqnarray*}
y_{t} &=&\beta _{0}x_{t}+\sigma _{u}u_{t} \\
x_{t} &=&\pi _{0}w_{t}+\sigma _{v}v_{t}
\end{eqnarray*}%
with%
\begin{equation*}
\left(
\begin{array}{c}
u_{t} \\
v_{t}%
\end{array}%
\right) \sim N\left( \left[
\begin{array}{c}
0 \\
0%
\end{array}%
\right] ,\left[
\begin{array}{cc}
1 & \rho \\
\rho & 1%
\end{array}%
\right] \right) .
\end{equation*}%
and $t=1,\ldots ,T$. Write an R program to generate at least $R=1000$
samples for $x$ and $y$ with sample size $T=10$ using the parameters $\sigma
_{u}=\sigma _{v}=1$, $\pi _{0}=1$, $\beta _{0}=0$, and $\rho =0.5$. For the
exogenous instrument $w=(w_{1},\ldots ,w_{T})^{\prime }$, use independent
drawings from the standard normal distribution, and then rescale $w$ so that
$w^{\prime }w$ is equal to $T$.

For each simulated sample, compute the simple IV estimator (if you use the
\texttt{ivreg} command of the \texttt{AER} package, make sure to drop the
intercept by including \textquotedblleft \texttt{-1}\textquotedblright as a
regressor). Then draw the empirical distribution function\footnote{%
The easiest way to do so is to use the R function \texttt{ecdf}, e.g.
\texttt{plot(ecdf(...))}.} of the realizations of the estimator on the same
plot as the cdf of the normal distribution with mean zero and variance $%
\sigma _{u}^{2}/(T\pi _{0}^{2})$.

In addition, for each simulated sample, compute the OLS estimator, and plot
the empirical distribution function of the realizations of this estimator on
the same axes as the empirical distribution function of the realizations of
the IV estimator.

Redo the exercise for sample size $T=100$, and -- if your computer is fast
enough -- also for $T=1000$.
\begin{solution}
\textbf{A simple example}

The code might look like this
\begin{verbatim}
##########################
#### A simple example ####
##########################
library(AER);library(MASS);
R <- 1000
TT <- 10
sigu <- 1
sigv <- 1
pi0 <-1
bet0 <- 0
rho <- 0.5
w <- rnorm(TT)
w <- sqrt(TT/(t(w)%*%w))*w
(t(w)%*%w==TT)
#Rescaling w would not affect the results if x remained unchanged.
#If x changes, any change in the scaling of w would have to be offset
#by a compensating change in pi0. With the speicfied rescaling there
#is no need to change pi0 when we change the way in which the instrument
#is generated, because the variance of w is equal to 1 in every set
#of simulated data

Z <- matrix(NA,R,2)
fb <- txtProgressBar(min=0, max=R, style=3)
for (r in 1:R){
  uv <- mvrnorm(TT,c(0,0),matrix(c(1,rho,rho,1),2,2))
  w <- rnorm(TT)
  w <- sqrt(TT/(t(w)%*%w))*w
  x <- pi0*w+sigv*uv[,1]
  y <- bet0*x + sigu*uv[,2]
  Z[r,1] <- coefficients(ivreg(y~x-1|w))
  Z[r,2] <- coefficients(lm(y~x-1))
  setTxtProgressBar(fb,r)
}
close(fb)

plot(ecdf(Z[,1]),col="blue",main="IV and OLS Estimators",xlim=c(-2,2))
lines(ecdf(Z[,2]),col="red")
curve(pnorm(x,mean=0,sd=(sigu^2/(TT*pi0^2))),add=T)
legend(locator(1),c("TRUE","IV","OLS"),fill=c("black","blue","red"))
#Distributions differ, especially in the left-hand tail, which is very much
#longer for the edf of the iv estimator than asymptotic theory would suggest
\end{verbatim}
\end{solution}


\subsection{Money demand\label{mdemand}}

Load the dataset \texttt{money.csv}. The data is taken from the web site of
Davidson and MacKinnon (2004). The file contains seasonally adjusted
quarterly data for the logarithm of real money supply ($m_{t}$), real GDP ($%
y_{t}$), and the 3-month treasury bill rate ($r_{t}$) for Canada.

\begin{enumerate}
\item This is exercise 8.25 of Davidson and MacKinnon (2004). Estimate the
model
\begin{equation*}
m_{t}=\beta _{1}+\beta _{2}r_{t}+\beta _{3}y_{t}+\beta _{4}m_{t-1}+\beta
_{5}m_{t-2}+u_{t}
\end{equation*}%
by OLS for the period 1968:1 to 1998:4. Then perform a Durbin-Wu-Hausman
test for the hypothesis that the interest rate, $r_{t}$, can be treated as
exogenous, using $r_{t-1}$ and $r_{t-2}$ as additional instruments.

\item This is exercise 8.26 of Davidson and MacKinnon (2004). Estimate the
model by generalized instrumental variables, treating $r_{t}$ as endogenous
and using $r_{t-1}$ and $r_{t-2}$ as additional instruments. Are the
estimates much different from the OLS ones?

\item For the IV estimation, perform a test of over-identifying restrictions.
\end{enumerate}

\begin{solution}
\textbf{Money demand}
\begin{enumerate}
  \item Durbin-Wu-Hausman test: \\
  $H_0:$ $r_t$ can be treated as exogenous (OLS is better $E(X'u)=0$) vs.
      $H_1:$ $r_t$ cannot be treated as exogenous (OLS is not consistent,
      IV model is better $E(W'u)=0$), with $r_{t-1}$ and $r_{t-2}$ as
      additional instruments.\\
      Idea: compare $\widehat{\beta_{IV}} -
      \widehat{\beta_{OLS}}$. To test if this difference is significantly
      different from zero, perform a Wald test of $\delta=0$ in the
      Wu-regression: $y=X\beta + P_W \widetilde{X}\delta$ with
      $\widetilde{X}$ including all possible endogenous regressors (here
      $r_t$) and $P_W = W(W'W)^{-1}W'$
  \item $\widehat{\beta_{GIV}} = (X'P_WX)^{-1}X'P_Wy$
  \item Test of overidentifying restrictions: \\
  Idea: Test if IV residuals can be explained by the full set of
      instruments $W$.\\ $H_0:$ Instruments are valid and uncorrelated
      with the residuals. \\Testregression: $u_i=W_i'\gamma +
      \varepsilon_i$ with $i=1,\dots,n$. \\Teststatistics:$n~R^2 \sim
      \chi^2(m)$ with $m:$ degrees of overidentification. \\If
      instruments pass the test (that is $H_0$ is not rejected), they are
      valid by this criterion.
\end{enumerate}
The code might look like this:
\begin{verbatim}
######################
#### Money demand ####
######################
library(AER)
# read data, define vectors and matrices
money <- read.csv(file.choose())
View(money)
m <- money$m
r <- money$r
y <- money$y
TT <- length(m)
# Matrix of regressors
X <- cbind(1,r[5:TT],y[5:TT],m[4:(TT-1)],m[3:(TT-2)])
# Matrix of Instruments
W <- cbind(1,r[4:(TT-1)], r[3:(TT-2)],y[5:TT], m[4:(TT-1)], m[3:(TT-2)])
# Projection-matrix
Pw <- W %*% solve( t(W)%*%W ) %*% t(W)

#1)
OLS <- lm(m[5:TT] ~ r[5:TT] + y[5:TT] + m[4:(TT-1)] + m[3:(TT-2)])
summary(OLS)

# Durbin-Wu-Hausman test
# If r_t-1 and r_t-2 are appropriate instruments for r_t,
# then the OLS estimator is not consistent, but the IV-estimator is!
WuRegr <- lm(m[5:TT] ~ Pw%*%r[5:TT] + r[5:TT] + y[5:TT] + m[4:(TT-1)] + m[3:(TT-2)])
summary(WuRegr)
# H0 cannot be rejected, meaning r_t can be treated as exogenous

#2)
IV <- ivreg(m[5:TT] ~ r[5:TT] + y[5:TT] + m[4:(TT-1)] + m[3:(TT-2)]
            |r[4:(TT-1)]+ r[3:(TT-2)] + y[5:TT] + m[4:(TT-1)] + m[3:(TT-2)])
summary(IV)
#You can get the same coefficients, if you simply use the Formula
betaGIV <- solve(t(X)%*%Pw%*%X)%*%t(X)%*%Pw%*%m[5:TT]
betaGIV

# compare to OLS coefficients:
OLS$coefficients
IV$coefficients

# Test if the coefficient is the same
covmat <- IV$sigma^2*IV$cov
linearHypothesis(IV,paste("r[5:TT]=",OLS$coefficients[2]),V=covmat) # H0 can not be rejected
# Not surprising as we have seen in the DWU test

#3)
u <- residuals(IV)
n <- length(u)
#perform OLS and store the results
obj <- summary(lm(u ~ r[4:(TT-1)] + r[3:(TT-2)] + y[5:TT] + m[4:(TT-1)] + m[3:(TT-2)]))
teststat <- n*obj$r.squared
1-pchisq(teststat,df=1) # p-value
#Nullhypothesis can be rejected
#This strongly suggests that either at least one of rt-1 and rt-2 is not a valid instrument,
#or that the model is misspecified. Here the latter is more sensible, in particular,
#we could simply add rt-1 and rt-2 to the regressors of the model
\end{verbatim}
\end{solution}

\subsection{Tests for the IV model}

Load the dataset \texttt{fertility.csv} and its description (\texttt{%
fertility.pdf}) from the internet site of the course. The dataset is
provided on the internet site of the textbook by Stock and Watson. It is a
subset of the data used by Angrist and Evans, \textquotedblleft Children and
their parents' labor supply: Evidence from exogenous variation in family
size\textquotedblright , \emph{American Economic Review},\emph{\ }88 (1998)
450-77. Since some variables included in Angrist and Evans are missing, we
cannot reproduce their results exactly.

\begin{enumerate}
\item The variable \texttt{morekids} indicates if there are more than two
children. The variable \texttt{samesex} indicates if the first two children
are both boys or both girls. Compute the fraction of families that had
another child if the first two children were of the same sex, and the
fraction if the first two children were of different sex (see Table 3,
married women, 1980 data, lower half of the table, in Angrist and Evans).

\item We would like to estimate the causal effect of \texttt{morekids} on
the number of weeks worked by the mother. Perform an OLS regression of
\texttt{weeksm1} on \texttt{morekids} plus all other variables except
\texttt{samesex}. Explain why OLS is inappropriate for estimating the causal
effect.

\item Explain why the variable \texttt{samesex} is a valid instrument for
the regression of \texttt{weeksm1} on \texttt{morekids}.

\item Perform an IV regression of \texttt{weeksm1} on \texttt{morekids}
using \texttt{samesex} as instrument.

\item Perform an asymptotic $t$-test of the null hypothesis that the
coefficients of \texttt{hispan} and \texttt{othrace} are equal. Hint: The
estimated covariance matrix of $\hat{\beta}_{IV}$ can be computed by \texttt{%
a\$sigma\symbol{94}2*a\$cov} where \texttt{a} is the object returned by the
command \texttt{ivreg}.

\item Perform a Wald test of the null hypothesis that the three coefficients
of \texttt{boy1st}, \texttt{boy2nd}, and \texttt{hispan} are all equal to
zero.
\end{enumerate}

\begin{solution}
\textbf{Tests for the IV model}

The code might look like this:
\begin{verbatim}
################################
#### Tests for the IV model ####
################################
library(AER)
fertility <- read.csv2(file.choose())
View(fertility)
morekids <- fertility$morekids
samesex <- fertility$samesex
weeksm1 <- fertility$weeksm1
boy1st <- fertility$boy1st
boy2nd <- fertility$boy2nd
agem1 <- fertility$agem1
black <- fertility$black
hispan <- fertility$hispan
othrace <- fertility$othrace
#1)
n <- dim(fertility)[1] #number of families
x <- fertility[morekids==1,] # only families that had another child
m <- dim(x)[1] #number of families that had another child
dim(x[x$samesex==1,])[1]/m
dim(x[x$samesex==0,])[1]/m

#2)
regr <- lm(weeksm1 ~ morekids + boy1st + boy2nd + agem1 + black + hispan + othrace)
summary(regr)
#The coefficient is -6.23209. This indicates that women with
#more than 2 children work 6.23209 fewer weeks per year than women
#with 2 or fewer children. However, both fertility (morekids) and
#laborsupply (weeks worked) are choice variables. A woman who works
#more than average (positive regression error) may also be a woman
#who is less likely to have an additional child. This would imply
#that morekids is positively correlated with the regression error.
#OLS estimator is thus positively biased.

#3)
#Relevance of instrument
#Samesex is random and unrelated to any of the other variables
#including the error term in the labor supply equation. Thus, the
#instrument is exogenous. The instrument is also relevant (see part 1).
#You can also test for relevance: compute the F-Statistic in the
#regression: $morekids_i = \beta_0+\beta_1 samesex_i +\varepsilon_i$.
relevInst <- lm(morekids ~ samesex)
summary(relevInst) # F-Statistic is high!

#4)
ivmod <- ivreg(weeksm1 ~ morekids+boy1st + boy2nd + agem1 + black
               + hispan + othrace|samesex + boy1st + boy2nd
               + agem1 + black + hispan + othrace)
summary(ivmod)

#5) t-test that two coefficients are equal
covmat <- ivmod$sigma^2*ivmod$cov
betaHisp <- ivmod$coefficients[7]
betaOthr <- ivmod$coefficients[6]
varHisp <- covmat[7,7]
varOthr <- covmat[6,6]
ttest <- (betaHisp -betaOthr)/sqrt(varHisp + varOthr)
abs(ttest) > 2.56 # reject H0 at least on a 1% level!
1-pt(abs(ttest),n-7) #p-value is zero!


#6)
betahat <- ivmod$coefficients[c(3,4,7)]
beta0 <- c(0,0,0)
ivmod0 <- ivreg(weeksm1 ~ morekids + agem1 + black + hispan
                + othrace|samesex + boy1st + boy2nd
                + agem1 + black + hispan + othrace)
# Wald test formula and p-value
Wald <- (betahat-beta0)%*%solve(covmat[c(3,4,7),c(3,4,7)])%*%(betahat - beta0)
1-pchisq(Wald,df=3)
#or simply use the linearHypothesis command
linearHypothesis(ivmod,c("boy1st=0","boy2nd=0","hispan=0"), V=covmat)
\end{verbatim}
\end{solution}

\newpage
\section{GMM\label{GMM}}

\subsection{The R package gmm}

Install and activate the R package \texttt{gmm}.

\begin{enumerate}
\item Read (at least) section 2 of the R vignette \textquotedblleft
Computing Generalized Empirical Likelihood and Generalized Method of Moments
with R\textquotedblright\ (\texttt{gmm\_with\_R.pdf}) which can be found on
the internet site of the course or in the documentation of the package.

\item Explain the relationship between the elementary zero functions $f$ and
the functions $g$ used in the \texttt{gmm} package.

\item This is the example given in section 3.1 of the R vignette. Suppose
you want to estimate the parameters $\mu $ and $\sigma $ of a normal
distribution $X$ by GMM using the three moment conditions%
\begin{eqnarray*}
E(X) &=&\mu \\
E(\left( X-\mu \right) ^{2}) &=&\sigma ^{2} \\
E(X^{3}) &=&\mu \left( \mu ^{2}+3\sigma ^{2}\right) .
\end{eqnarray*}%
Write an R function with arguments $\theta =(\mu ,\sigma )$ and data $X$
that computes and returns the moment conditions $g$.

\item Set the random number seed, \texttt{set.seed(123)}. Generate $n=100$
random numbers from the normal distribution $N(4,2^{2})$. Using the starting
values $\left( \mu _{0},\sigma _{0}\right) =(0,0)$ run the \texttt{gmm}
command and save the estimation results in the object \texttt{res}. Print
\texttt{summary(res)} and interpret the output.
\end{enumerate}

\begin{solution}
\textbf{The R packagae gmm}

Basic scheme to use gmm:
\begin{enumerate}
  \item Set up matices with data $Y$ and/or instruments $W$.
  \item Specify the moment conditions
  \begin{verbatim} g <- function(param,dat) {moment conditions} \end{verbatim}.
  For the gmm package this means that you have to program the formula
      inbetween the expectation $E[g(\theta,Y)]=0$. Here: \begin{align*}
      X-\mu\\
      (X-\mu)^2 - \sigma^2\\
      X^3-\mu(\mu^2-3\sigma^2)
      \end{align*}
  \item Call
      $gmm(\underbrace{g=g}_\text{function},\underbrace{x=Y}_\text{data},\underbrace{t0=c(0,\dots,0)}_{\text{starting
      values}},\dots)$. Use appropriate arguments for numerical
      optimization, weights, gradient (to improve precision), etc. Most
      of the times just use the standard setting.
\end{enumerate}

The code might look like this:
\begin{verbatim}
############################
#### The R packagae gmm ####
############################
install.packages("gmm")
library(gmm)
##generate data
set.seed(123)
n <- 200
dat <- rnorm(n,mean=4,sd=2)

##moment conditions
g <- function(param,x) {
  m1 <- param[1]-x
  m2 <- param[2]^2-(x-param[1])^2
  m3 <- x^3 - param[1]*(param[1]^2+3*param[2]^2)
  f <- cbind(m1,m2,m3)
  return(f)
}
g(c(3,1),dat) #gives you a n-by-L matrix with n observations and L moments

##gmm estimation with standard settings
t0 <- c(mu=3, sigm=1)
res1 <- gmm(g,dat,t0)
print(res1)
summary(res1)
coef(res1)
vcov(res1)
confint(res1)
specTest(res1)

##gmm estimation with gradient
#gradient of gbar=1/n*sum(g)
Dgbar <- function(param,x){
  xbar <- mean(x)
  Dgbar <- matrix(c(1, 2*(xbar-param[1]),
    -3*(param[1]^2+param[2]^2), 0, 2*param[2], -6*param[1]*param[2]),nrow=3)
  return(Dgbar)
}
Dgbar(c(3,1),dat)
t0 <- c(mu=3, sigm=1)
res2 <- gmm(g,dat,t0,grad=Dgbar)
print(res2);summary(res2);coef(res2);vcov(res2);confint(res2);specTest(res2)

##gmm estimation with restrictions on parameter space (with nlminb)
t0 <- c(mu=3, sigm=1)
res3 <- gmm(g,dat,t0,optfct="nlminb",lower=c(-5,0),upper=c(5,5))
print(res3);summary(res3);coef(res3);vcov(res3);confint(res3);specTest(res3)

##gmm estimation: Iterative GMM (ITGMM) and continuous updated GMM (CUE)
t0 <- c(mu=3, sigm=1)
res4 <- gmm(g,dat,t0,type="iterative",crit=1e-5,itermax=200)
print(res4);summary(res4);coef(res4);vcov(res4);confint(res4);specTest(res4)
res5 <- gmm(g,dat,res4$coef,type="cue")#use ITGMM as starting values
print(res5);summary(res5);coef(res5);vcov(res5);confint(res5);specTest(res5)
\end{verbatim}
\end{solution}


\subsection{Nonlinear least squares estimation and GMM}

Nonlinear least squares estimation is a special case of GMM. Consider the
nonlinear regression model%
\begin{equation*}
y_{t}=x_{t}(\beta )+u_{t}
\end{equation*}%
where $x_{t}(\beta )$ is nonlinear function of the parameters and the data.
Assume that the $u_{t}$ are i.i.d. with $E(u_{t})=0$ and $Var(u_{t})=\sigma
^{2}$ and independent of the $x_{t}$.

\begin{enumerate}
\item Formulate the general model and its least squares estimation in the
GMM framework.

\item As a special case, consider the exponential model (see exercise \ref%
{nls1}),%
\begin{equation*}
y_{t}=\exp \left( \alpha +\beta x_{t}\right) +u_{t}
\end{equation*}%
where $u_{t}\sim N(0,\sigma ^{2})$. Load the dataset \texttt{expgrowth.csv}
from the course site and estimate the parameters $\alpha $ and $\beta $ and
their standard errors by GMM using the command \texttt{gmm}. Compare your
results with the maximum likelihood estimates computed in exercise \ref{nls1}%
.
\end{enumerate}

\begin{solution}
\textbf{Nonlinear least squares estimation and GMM}

\begin{enumerate}
  \item As elementary zero functions we can use $E(u_t)=0$ and $E(u_t^2-\sigma^2)=0$. As Instruments we can use the elementary zero function itself and $x_t(\beta)$, since $E(x_t(\beta)\cdot u_t)=E(x_t(\beta)\cdot (y_t-x_t(\beta)))=0$. That is we have three moment conditions:
      \begin{align*}
      E(u_t) = E(y_t-x_t(\beta)) &= 0\\
      E(x_t(\beta) u_t) = E(x_t(\beta)\cdot (y_t-x_t(\beta)))&= 0\\
      E(u_t^2-\sigma^2) = E((y_t-x_t(\beta)^2-\sigma^2) &=0
      \end{align*}
  \item Here $x_t(\beta)=exp(\alpha+\beta x_t)$. This yields the same results as in exercise \ref{nls1}.
\end{enumerate}
The code might look like this:
\begin{verbatim}
####################################
#### Nonlinear regression model ####
####################################
expgrowth <- read.csv(file.choose())
View(expgrowth)
x <- expgrowth$x
y <- expgrowth$y

g <- function(param,dat){
  y <- dat[,1]
  x <- dat[,2]
  u <- y - exp(param[1]+param[2]*x)
  f1 <- u
  f2 <- u^2
  m1 <- 1*f1
  m2 <- x*f1
  m3 <- f2-param[3]^2
  return(cbind(m1,m2,m3))
}
g(c(1,0.1,1),cbind(y,x)) # g gives you a nxq matrix with n observations and q moments

gmmmod <- gmm(g,x=cbind(y,x),c(alpha=0,beta=0.01,sigma=1))
print(gmmmod)
gmmmod <- gmm(g,x=cbind(y,x),c(alpha=2,beta=0.01,sigma=2))
print(gmmmod$coefficients)
#depends on the start-values, in order to be more precise one could specify the gradient
summary(gmmmod)
\end{verbatim}
\end{solution}


\subsection{Ordinary least squares estimation and GMM}
Ordinary least squares estimation is a special case of GMM. This exercise compares the two uses of the \texttt{gmm} package, i.e. explicitly taking into account linearity or not. Consider the
linear regression model%
\begin{equation*}
y_{t}= \alpha + \beta_1 x_{1,t} + \beta_2 x_{2,t} + u_t
\end{equation*}%
for $t=1,\dots,n$. Assume that the standard assumptions are satisfied.

\begin{enumerate}  \setlength{\itemsep}{5pt}

\item Load the dataset \texttt{olsgmm.csv}. It contains $n=100$ (artificial) observations $(y_1,x_{11},x_{21})$,\dots, $(y_n,x_{1n},x_{2n})$.
\item Estimate the linear regression model using the ordinary least squares command \texttt{lm}.
\item Estimate the linear regression model with GMM explicitly taking into account linearity. Save the results and compare it to the OLS estimator.
\item Program a function \texttt{g} with two arguments. The first argument is the vector of deep parameters $\theta = (\alpha,\beta_1,\beta_2)$. The second argument is the data matrix. The function \texttt{g1} should return the moment conditions as a matrix,
    \begin{equation*}
      \begin{bmatrix}
        f_{11}    & f_{21}      & f_{31}\\
        f_{12}    & f_{22}      & f_{32}\\
        \vdots & \vdots & \vdots\\
        f_{1n}    & f_{2n}      & f_{3n}
      \end{bmatrix}
    \end{equation*}
    where
    \begin{align*}
      f_{1t} &= (y_t - \alpha - \beta_1 x_{1t} - \beta_2 x_{2t})\\
      f_{2t} &= (y_t - \alpha - \beta_1 x_{1t} - \beta_2 x_{2t})x_{1t}\\
      f_{3t} &= (y_t - \alpha - \beta_1 x_{1t} - \beta_2 x_{2t})x_{2t}
    \end{align*}
    for $t=1,\dots,n$.
\item Now re-estimate the linear regression model using the \texttt{gmm} syntax for general nonlinear models. Set the starting values to (1,0.5,1.2) (these are the true values used for simulating the data). Compare the result with the result obtained when taking into account linearity.
\end{enumerate}

\begin{solution}
\textbf{Ordinary least squares estimation and GMM}

The code might look like this
\begin{verbatim}
###################################################
#### Ordinary least squares estimation and GMM ####
###################################################
#Get data
olsgmm <- read.csv(file.choose())
View(olsgmm)
y <- olsgmm$y
x1 <- olsgmm$x1
x2 <- olsgmm$x2

#OLS
ols <- lm(y~x1+x2)
summary(ols)

#GMM estimation explicitly taking into account linearity
gmm_lin <- gmm(y~x1+x2,cbind(x1,x2))
gmm_lin$coef - ols$coef #numerically the same

#GMM estimation without linearity
g <- function(param,dat){
  y <- dat$y
  x1 <- dat$x1
  x2 <- dat$x2
  u <- y -param[1] -param[2]*x1 -param[3]*x2
  f1 <- u
  f2 <- u*x1
  f3 <- u*x2
  return(cbind(f1,f2,f3))
}
g(c(1,0.5,1.2),olsgmm) # g gives you a nxq matrix with n observations and q moments

t0<-c(alpha=1,beta1=0.5,beta2=1.2)
gmm_nonlin <- gmm(g,olsgmm,t0)
print(gmm_nonlin$coefficients)
print(gmm_lin$coefficients)
\end{verbatim}
\end{solution}

\subsection{Maximum likelihood estimation and GMM}

Maximum likelihood estimation is a special case of GMM. Let $X_{1},\ldots
,X_{n}$ be a random sample from the random variable $X$. We know the
distributional family of $X$ (e.g. normal distribution) but we do not know
the parameters. Denote the density function by $f$ and the parameters by $%
\theta $.

\begin{enumerate}
\item Show that the maximum likelihood estimation can be formulated in the
GMM framework.

\item As a special case, consider the censored lognormal distribution (see
exercise \ref{mllognormal}). Let $X\sim LN(\mu ,\sigma ^{2})$ and let $%
X_{1},\ldots ,X_{n}$ be an unobserved sample from $X$. The observations are%
\begin{equation*}
Y_{i}=\left\{
\begin{array}{ll}
X_{i} & \quad \text{if }X_{i}<c \\
c & \quad \text{if }X_{i}\geq c%
\end{array}%
\right.
\end{equation*}%
where $c=12$ is a known constant. Load the dataset \texttt{censoredln.csv}
and estimate the parameters $\mu $ and $\sigma $ and their standard errors
by GMM using the command \texttt{gmm}. Compare your results with the maximum
likelihood estimates computed in exercise \ref{mllognormal}.
\end{enumerate}

\begin{solution}
\textbf{Maximum likelihood estimation and GMM}

\begin{enumerate}
\item The likelihood function is given by $L(\theta;x_1,\dots,x_n)=\prod_{i=1}^{n}f_X(x_i;\theta)$ and the log Likelihood
\begin{align*}
  logL(\theta;x_1,\dots,x_n)=\sum_{i=1}^{n}log f_X(x_i;\theta)
\end{align*}
We gradient vector $g(\theta)=\partial log L(\theta)/\partial \theta$ is called the score vector. Define the gradient contributions
\begin{align*}
G_{ij}(\theta,x_i) = \frac{\partial log f_X(x_i;\theta)}{\partial \theta_j}
\end{align*}
We have shown in the lecture that
\begin{align*}
E(G_{ij}(\theta,x_i) = \frac{\partial log f_X(x_i;\theta)}{\partial \theta_j}) = 0
\end{align*}
So the gradient contributions are our moment conditions.
\item Let's first derive the gradient contributions of a normally distributed variable $X$ with density function $f_X(x)=\frac{1}{\sigma\sqrt{2\pi}}exp\{-(x-\mu)/(2\sigma^2)\}$:
\begin{align*}
\frac{\partial log f_X(x)}{\partial \mu} &= \frac{x-\mu}{\sigma^2}\\
\frac{\partial log f_X(x)}{\partial \sigma} &= \frac{-1}{\sigma} + \frac{(x-\mu)^2}{\sigma^3}
\end{align*}
Furthermore we can use the implicit function theorem to derive the derivatives of the cdf of a normally distributed variable $x$ with mean $\mu$ and standard deviation $\sigma$:
\begin{align*}
  \frac{\partial \Phi\left(\frac{x-\mu}{\sigma}\right)}{\partial \mu} &=-\phi\left(\frac{x-\mu}{\sigma}\right)\\
  \frac{\partial \Phi\left(\frac{x-\mu}{\sigma}\right)}{\partial \sigma} &=-\phi\left(\frac{x-\mu}{\sigma}\right)\frac{x-\mu}{\sigma^2}
\end{align*}
with $\Phi$ the cdf and $\phi$ as the pdf of a standard normally distributed variable. We have shown in exercise \ref{mllognormal} that the log-likelihood function for censored data is equal to
\begin{equation*}
\log L(\mu,\sigma; x) = \sum_{i=1}^{n_1} \log f_X(log(x_i);\mu,\sigma) + \sum_{i=1}^{n_2} \log\left(1-\Phi\left(\frac{log(c)-\mu}{\sigma}\right)\right)
\end{equation*}
where we used the fact that if a random vector $X$ is log-normally distributed, $log(X)$ is normally distributed. Furthermore, $n_1$ is the number of non-censored observations and $n_2$ the number of censored observations, $n_1+n_2=n$). The function g for the gmm package should therefore distinguish between censored and uncensored observations. A typical row for an uncensored observation of $g$ should have the following  structure
\begin{align*}
\begin{bmatrix}
  \frac{log(x_i)-\mu}{\sigma^2}, & \frac{(log(x_i)-\mu)^2-\sigma^2}{\sigma^3}
\end{bmatrix}
\end{align*}
whereas for a censored observation we require
\begin{align*}
\frac{\phi\left(\frac{log(c)-\mu}{\sigma}\right) }{1-\Phi\left(\frac{log(c)-\mu}{\sigma}\right)} \cdot
\begin{bmatrix}
  1, & \frac{log(c)-\mu}{\sigma^2}
\end{bmatrix}
\end{align*}

The code might look like this
\begin{verbatim}
###############################################
#### Maximum likelihood estimation and GMM ####
###############################################
library(gmm)
# Get data
censoredln <- read.table(file.choose(), header=T, quote="\"")

g <- function(param,dat){
  cens <- 12
  x <- dat
  mue <- param[1]
  sigm <- param[2]
  muncens_i1 <- (log(x[x<12])-mue)/(sigm^2)
  muncens_i2 <- ((log(x[x<12])-mue)^2-sigm^2)/(sigm^3)
  muncens <- cbind(muncens_i1,muncens_i2)
  fact_cens <- 1/pnorm((mue-log(cens))/sigm)*dnorm((mue-log(cens))/sigm)
  num_cens <- length(x[x==12])
  mcens_i1 <- rep(fact_cens,num_cens)
  mcens_i2 <- rep(fact_cens*(log(cens)-mue)/sigm,num_cens)
  mcens <- cbind(mcens_i1,mcens_i2)
  return(rbind(muncens,mcens))
}

# Optimization
t0 <- c(mue = 2, sigm=0.5)
gmm(g,censoredln,t0=t0)
\end{verbatim}
\end{enumerate}
\end{solution}

\subsection{Instrumental variables estimation and GMM}

Instrumental variable estimation is a special case of GMM. Consider the
linear regression model%
\begin{equation*}
y=X\beta +u
\end{equation*}%
with $u\sim N(0,\sigma ^{2}I)$. The error term and the regressor matrix $X$
may be correlated but there is a set of instrumental variables $W$ such that
$E(u_{t}|W_{t})=0$.

\begin{enumerate}
\item Formulate the general model and the IV estimation in the GMM framework.

\item As a special case, consider the money demand model of exercise \ref%
{mdemand},
\begin{equation*}
m_{t}=\beta _{1}+\beta _{2}r_{t}+\beta _{3}y_{t}+\beta _{4}m_{t-1}+\beta
_{5}m_{t-2}+u_{t}
\end{equation*}%
with the logarithm of real money supply ($m_{t}$), real GDP ($y_{t}$), and
the 3-month treasury bill rate ($r_{t}$) for Canada. Load the dataset
\texttt{money.csv} and estimate the parameters $\beta _{1},\ldots ,\beta
_{5} $ by GMM using $r_{t-1}$ and $r_{t-2}$ as instruments for the
endogenous regressor $r_{t}$.
\end{enumerate}

\begin{solution}
\textbf{Instrumental variables estimation and GMM }

The gmm framework is given by
\begin{align*}
  E[m_t -X_t\beta] = 0\\
  E[y_t(m_t -X_t\beta)] = 0\\
  E[m_{t-1}(m_t -X_t\beta)] = 0\\
  E[m_{t-2}(m_t -X_t\beta)] = 0\\
  E[r_{t-1}(m_t -X_t\beta)] = 0\\
  E[r_{t-2}(m_t -X_t\beta)] = 0
\end{align*}

 The code might look like this:
\begin{verbatim}
###################################################
#### Instrumental variables estimation and GMM ####
###################################################
library(gmm)
library(AER)
# read data, define vectors and matrices
money <- read.csv(file.choose())
View(money)
m <- money$m
r <- money$r
y <- money$y
TT <- length(m)

yt <- y[5:TT]
mt <- m[5:TT]; mt1 <- m[4:(TT-1)]; mt2 <- m[3:(TT-2)]
rt <- r[5:TT]; rt1 <- r[4:(TT-1)]; rt2 <- r[3:(TT-2)]

# Gmm estimation in linear model notation
obj <- gmm(mt~rt+yt+mt1+mt2, ~rt1+rt2+yt+mt1+mt2)
obj$coefficients

# Generalized IV estimation
IV <- ivreg(mt~rt+yt+mt1+mt2|rt1+rt2+yt+mt1+mt2)
IV$coefficients
\end{verbatim}
\end{solution}

\subsection{Moment conditions and moment existence}

Consider the simple linear model without an intercept%
\begin{equation*}
y_{t}=\beta x_{t}+u_{t}.
\end{equation*}%
Assume that $x_{t}$ has a $t$-distribution with 3 degrees of freedom and
variance 1. The unit variance can be attained by dividing the $t$%
-distribution by $\sqrt{3}$, i.e. \texttt{rt(n,df=3)/sqrt(3)}. The error
terms $u_{t}$ are independent of $x_{t}$; they have a $t_{3}$-distribution
with variance $\sigma ^{2}$. Set $\beta =0.9$ and $\sigma ^{2}=1$.

\begin{enumerate}
\item Generate a sample $\left( x_{1},y_{1}\right) ,\ldots ,\left(
x_{n},y_{n}\right) $ of size $n=100$.

\item Compute the GMM estimates $\hat{\beta}$ and $\hat{\sigma}$ using the
moment conditions%
\begin{eqnarray*}
g_{t1} &=&y_{t}-\beta x_{t} \\
g_{t2} &=&\left( y_{t}-\beta x_{t}\right) x_{t} \\
g_{t3} &=&\left( y_{t}-\beta x_{t}\right) ^{2}-\sigma ^{2}.
\end{eqnarray*}

\item Within a loop $r=1,\ldots ,R$, repeat steps 1. and 2. a large number
of times and plot the histogram of $\hat{\sigma}$. Is the distribution of $%
\hat{\sigma}$ well approximated by a normal distribution?

\item Check if the weighting scheme (\texttt{wmatrix="{}ident"} or \texttt{%
wmatrix="{}optimal"}) influences the distribution of $\hat{\sigma}$.

\item Change the distribution of $x_{t}$ and $u_{t}$ from the $t_{3}$%
-distribution with variance 1 to the standard normal distribution.

\item If your computer is fast enough (or you are willing to wait longer),
increase the sample size $n$ and redo this exercise.
\end{enumerate}

\begin{solution}
\textbf{Moment conditions and moment existence }

The code might look like this:
\begin{verbatim}
################################################
#### Moment conditions and moment existence ####
################################################
library(gmm)
library(MASS)

n <- 100
x <- rt(n,df=3)/sqrt(3)
u <- rt(n,df=3)/sqrt(3)
y <- 0.9*x+u

g <- function(theta,dat) {
  y <- dat[,1]
  x <- dat[,2]
  bet <- theta[1]
  sigm <- theta[2]
  u <- y-bet*x
  m1 <- u
  m2 <- u*x
  m3 <- u^2-sigm^2
  return(cbind(m1,m2,m3))
}
# GMM estimates for beta and sigma
obj <- gmm(g,cbind(y,x),t0=c(0.9,1),wmatrix="optimal")
obj1 <- gmm(g,cbind(y,x),t0=c(0.9,1),wmatrix="ident")
obj
obj1

# Whole thing within a loop
R <- 1000
n <- 100 # or try n=1000
nu <- 3
Z1 <- matrix(NA,R,2)
Z2 <- matrix(NA,R,2)
for(r in 1:R) {
  x <- rt(n,df=nu)/sqrt(nu/(nu-2))
  u <- rt(n,df=nu)/sqrt(nu/(nu-2))
  y <- 0.9*x+u
  obj1 <- gmm(g,x=cbind(y,x),t0=c(0.9,1),wmatrix="optimal")
  Z1[r,] <- (coefficients(obj1)-c(0.9,1))/sqrt(diag(vcov(obj1)))
  obj2 <- gmm(g,x=cbind(y,x),t0=c(0.9,1),wmatrix="ident")
  Z2[r,] <- (coefficients(obj2)-c(0.9,1))/sqrt(diag(vcov(obj2)))
}
# mean of the estimates for beta and sigma
apply(Z1,2,mean)
apply(Z2,2,mean)
#histogram of sigma compared to a normal distribution
truehist(Z1[,2])
xx <- seq(min(Z1[,2]),max(Z1[,2]),length=500)
lines(xx,dnorm(xx,mean(Z1[,2]),sd(Z1[,2])))

truehist(Z2[,2])
xx <- seq(min(Z2[,2]),max(Z2[,2]),length=500)
lines(xx,dnorm(xx,mean(Z2[,2]),sd(Z2[,2])))

###Now with the normal distribution
R <- 1000 # or try R=1000
n <- 100 # or try n=1000
Z1 <- matrix(NA,R,2)
Z2 <- matrix(NA,R,2)
for(r in 1:R) {
  x <- rnorm(n)
  u <- rnorm(n)
  y <- 0.9*x+u
  obj1 <- gmm(g,cbind(y,x),t0=c(0.9,1),wmatrix="optimal")
  obj2 <- gmm(g,cbind(y,x),t0=c(0.9,1),wmatrix="ident")
  Z1[r,] <- (coefficients(obj1)-c(0.9,1))/sqrt(diag(vcov(obj1)))
  Z2[r,] <- (coefficients(obj2)-c(0.9,1))/sqrt(diag(vcov(obj2)))
}
# mean of the estimates for beta and sigma
apply(Z1,2,mean)
apply(Z2,2,mean)

#histogram of sigma compared to a normal distribution
truehist(Z1[,2])
xx <- seq(min(Z1[,2]),max(Z1[,2]),length=500)
lines(xx,dnorm(xx,mean(Z1[,2]),sd(Z1[,2])))

truehist(Z2[,2])
xx <- seq(min(Z2[,2]),max(Z2[,2]),length=500)
lines(xx,dnorm(xx,mean(Z2[,2]),sd(Z2[,2])))
\end{verbatim}
\begin{itemize}
  \item The GMM estimator is asymptotically normally distributed.
      However, the estimator for the t-distribution with 3 degrees of
      freedom is not asymptotically normally distributed. This is because
      for convergence higher moments have to exist, which they don't for
      the t-distribution. If you use the normal distribution or another
      distribution, then the distribution is well approximated by a
      normal distribution.
  \item The weighting scheme does not influence the approximate
      distribution.
\end{itemize}
\end{solution}

\subsection{Standard CAPM}

In their overview article about GMM applications in finance,\footnote{%
Jagannathan, R., Skoulakis, G. and Wang, Z. (2002), Generalized Method of
Moments: Applications in Finance, \emph{Journal of Business and Economic
Statistics}, 20: 470-481. The password protected article is downloadable
from the course site.} Jagannathan et al. (2002) consider the stochastic
discount factor representation of the standard capital asset pricing model,%
\begin{eqnarray*}
E\left( m_{t}R_{it}\right) &=&1 \\
m_{t} &=&\theta _{1}+\theta _{2}R_{mt}
\end{eqnarray*}%
where $R_{it}=1+r_{it}$ is the gross return (and $r_{it}$ the return) of
asset $i$ and $R_{mt}$ the market portfolio gross return.

\begin{enumerate}
\item Rewrite the standard CAPM\ such that it fits into the GMM framework,
i.e. formulate the moment conditions.

\item Type \texttt{data(Finance)} to load the dataset that is included in
the \texttt{gmm} package. Read \texttt{?Finance} to learn about the data
structure.

\item Estimate the standard CAPM for the first five companies (i.e. \texttt{%
WMK}, \texttt{UIS}, \texttt{ORB}, \texttt{MAT}, \texttt{ABAX}) using the
variable \texttt{rm} as the (net) market return.

\item Use the function \texttt{specTest} to test the overidentifying
restrictions.
\end{enumerate}

\begin{solution}
\textbf{Standard CAPM }

The moment conditions are given by
\begin{equation*}
  E\left[(\theta_1+\theta_2 R_{mt})R_{it}-1\right]=0
\end{equation*}
The code might look like this:
\begin{verbatim}
#######################
#### Standard CAPM ####
#######################
library(gmm)
#Get data
data(Finance)
?Finance
View(Finance)
#let's only take the first 500 observations
r <- Finance[,c("WMK","UIS","ORB","MAT","ABAX")]
rm <- Finance[,"rm"]

#Define moment conditions
g <- function(param,dat){
  R <- 1+dat[,1:5]
  Rm <- 1+ dat[,6]
  m <- (param[1]+param[2]*Rm)*R-1
  return(m)
}

#estimate GMM
obj <- gmm(g,x=cbind(r,rm),c(0,0)) #this gives you an error
mode(r) # problem is in the data: r is a list and not numeric. gmm needs numeric data
mode(rm) # rm is numeric
mode(cbind(r,rm)) #if we combine the data, we still get a list
X <- as.matrix(cbind(r,rm)) #so let's convert it into a numeric structure
mode(X)
obj <- gmm(g,x=X,c(0,0))
obj
summary(obj)

# Test of overidentifying restrictions
specTest(obj) #confirms the non-rejection of the theory
\end{verbatim}
We can think of a return as a payoff with price one. If you pay one dollar
today, the return is how many dollars or units of consumption you get
tomorrow.

The asset pricing model says that, although expected returns can vary
across time and assets, expected discounted returns should always be the
same, 1.
\end{solution}

\subsection{Consumption-based CAPM}

In their overview article about GMM applications in finance,\footnote{%
Jagannathan, R., Skoulakis, G. and Wang, Z. (2002), Generalized Method of
Moments: Applications in Finance, \emph{Journal of Business and Economic
Statistics}, 20: 470-481. The password protected article is downloadable
from the course site.} Jagannathan et al. (2002) consider the moment
equations%
\begin{equation*}
E\left( \left( \beta \left( \frac{c_{t+1}}{c_{t}}\right) ^{-\gamma
}R_{i,t+1}-1\right) z_{t}\right) =0
\end{equation*}%
for $i=1,\ldots ,N$. Here, $c_{t}$ is consumption in period $t$, $R_{i,t}$
is the gross return of asset $i$ from $t-1$ to $t$, $z_{t}$ is a vector of
variables known at time $t$, the parameter $\beta $ is the time-preference
parameter, and the parameter $\gamma $ is the coefficient of relative risk
aversion in the utility function $u(c)=c^{1-\gamma }/(1-\gamma )$.

\begin{enumerate}
\item Load the datasets \texttt{consumptiondata.csv} and \texttt{dax30ann.csv%
}. The consumption dataset contains information about aggregate consumption
levels in current prices from 1970 to 1991 (West Germany) and 1991 to 2010
(Germany). We will only consider variable \texttt{V7} (see \texttt{%
LangeReihenKonsum2011Q3.pdf}, page 8). The second dataset contains the
start-of-year levels of the DAX30 performance index from 1969 to 2011.

\item Compute the consumption growth rates for West Germany from 1971 to
1991 and for Germany from 1992 to 2010 and concatenate them.

\item Let $z_{t}=\left( 1,c_{t}/c_{t-1},R_{DAX,t}\right) $. Set up the model
in the GMM framework and estimate $\beta $ and $\gamma $ using the \texttt{%
gmm} package.
\end{enumerate}

\begin{solution}
\textbf{Consumption-based CAPM}

The code might look like this:
\begin{verbatim}
####################################
###### Consumption-based CAPM ######
####################################
#Load Data
consumptiondata <- read.csv(file.choose(), sep=";", dec=",")
dax30ann <- read.csv(file.choose())
View(consumptiondata)
View(dax30ann)
V7 <- consumptiondata$V7
dax30 <- dax30ann$dax30

#growth rates for consumption
gr1 <- V7[2:22]/V7[1:21]
gr2 <- V7[24:42]/V7[23:41]
gr <- c(gr1,gr2)
#gross return of DAX
R <- dax30[3:42]/dax30[2:41]
#Matrix of data
X <- cbind(gr,R)

g <- function(param,dat){
  TT <- dim(dat)[1]
  gr <- dat[,1]
  R <- dat[,2]
  f <- param[1]*(gr[-1]^(-param[2]))*R[-1]-1
  z <- cbind(1,gr[-TT],R[-TT])
  m <- f*z
  return(m)
}
g(c(1,1),X)
obj <- gmm(g,x=X,c(1,1))
obj
\end{verbatim}
\end{solution}

\subsection{Minimum distance estimation}

Consider the following, highly simplified, model of earnings dynamics (F.
Guvenen, \textquotedblleft An empirical investigation of labor income
processes\textquotedblright , \emph{Review of Economic Dynamics}, 12 (2009)
58-79),%
\begin{eqnarray*}
y_{t}^{i} &=&\beta _{i}t+u_{t}^{i} \\
u_{t}^{i} &=&\rho u_{t-1}^{i}+\eta _{t}^{i}
\end{eqnarray*}%
where $y_{t}^{i}$ is log-earnings of person $i$ with $t$ periods of Labour
market experience, $\beta _{i}$ is an individual specific random effect with
variance $\sigma _{\beta }^{2}$, $\rho $ is the persistence parameter, and $%
\eta _{t}^{i}$ are i.i.d. innovations with variance $\sigma _{\eta }^{2}$.
It can be shown that for $h\geq 2$ the covariance between $\Delta y_{t}^{i}$
and $\Delta y_{t+h}^{i}$ is%
\begin{equation}
Cov\left( \Delta y_{t}^{i},\Delta y_{t+h}^{i}\right) =\sigma _{\beta }^{2}-
\left[ \rho ^{h-1}\left( \frac{1-\rho }{1+\rho }\right) \sigma _{\eta }^{2}%
\right] .  \label{covdy}
\end{equation}

\begin{enumerate}
\item The theoretical $\left( H+1\right) \times \left( H+1\right) $%
-covariance matrix of $\left[ \Delta y_{t}^{i},\Delta y_{t+1}^{i},\ldots
,\Delta y_{t+H}^{i}\right] $ depends on the three unknown parameters $\sigma
_{\beta }^{2}$, $\rho $ and $\sigma _{\eta }^{2}$. The GMM approach to
estimation requires that the differences between the elements of the
theoretical covariance matrix\footnote{%
Due to the symmetry of the covariance matrix, the elements below (or above)
the diagonal are omitted.} and its empirical counterparts should be
minimized with respect to the parameter vector $\theta =(\sigma _{\beta
}^{2},\rho ,\sigma _{\eta }^{2})$. Set $H=10$ and write an R program that
can estimate the parameters by GMM using the command \texttt{gmm}. Note that
the first order covariances must not enter the estimation since (\ref{covdy}%
) is only valid for $h\geq 2$.

\item Load the artificial dataset \texttt{logearnings.csv}. The rows are
individuals $i=1,\ldots ,N$, the column are the periods $t=1,\ldots ,T$ with
$N=2000$ and $T=15$. Compute the parameter estimates and their standard
errors.

\item Perform a test of the overidentifying restrictions.
\end{enumerate}

\begin{solution}
\textbf{Minimum distance estimation}
\end{solution}

\newpage
\section{Indirect inference}

\subsection{AR(1) processes}

The seemingly simple autoregressive process%
\begin{equation*}
x_{t}=\rho x_{t-1}+\varepsilon _{t}
\end{equation*}%
with $\varepsilon _{t}\sim N(0,\sigma ^{2})$ is sometimes surprisingly hard
to estimate. In the following, always use%
\begin{equation*}
\text{\texttt{x\ <-\
filter(rnorm(n),rho,method="r",init=rnorm(1))}}
\end{equation*}%
to generate a path of length $n$.

\begin{enumerate}
\item Simulate the distribution of the estimator $\hat{\rho}$ for $\rho =0.8$
and $n=100$. Use the command \texttt{ar} with options \texttt{order=1} and
\texttt{aic=F} to estimate $\rho $ (if you like, try different estimation
methods, e.g. \texttt{ols} or \texttt{mle}).

\item Simulate the distribution of $\hat{\rho}$ for the unit root process
with $\rho =1$.

\item Simulate the distribution of $\hat{\rho}$ for the explosive process
with $\rho =1.01$.

\item Write an R program to estimate the $AR(1)$ parameter $\rho $ by
indirect inference. The auxiliary model is, of course, itself an $AR(1)$
process. The number of auxiliary paths should be $H=10$.

\item Determine the distribution of the indirect inference estimator $\hat{%
\rho}$ by simulation for values of $\rho =0.8,1,1.01$.
\end{enumerate}

\begin{solution}
\textbf{AR(1) processes}

The code might look like this:
\begin{verbatim}
############################################
#### Indirect Inference - AR(1) process ####
############################################
library(MASS)
#Define function that estimates the ar process for different values of rho,n and R
simestim <- function(rho,n,R) {
  Z <- matrix(NA,R,3)
  for (r in 1:R) {
    x <- filter(rnorm(n),rho,method="r",init=rnorm(1))
    Z[r,1]<- ar(x,aic=F,order = 1,method = "yw")$ar # method: yule-walker
    Z[r,2]<- ar(x,aic=F,order = 1, method = "ols")$ar #method: ols
    Z[r,3]<- ar(x,aic=F,order = 1, method = "mle")$ar #method: mle
  }
  return(Z)
}

#1) Stationary AR(1)
stationary <- simestim(rho=0.9,n=100,R=1000)
truehist(stationary[,1],main="Yule-Walker")
truehist(stationary[,2],main="OLS")
truehist(stationary[,3],main="ML")

#2) Random Walk, methods: yule-walker, ols, mle
randomwalk <- simestim(rho=1,n=100,R=1000)
truehist(randomwalk[,1],main="Yule-Walker")
truehist(randomwalk[,2],main="OLS")
truehist(randomwalk[,3],main="ML")

#3) Explosive AR(1)
explosive <- simestim(rho=1.01,n=100,R=1000)
truehist(explosive[,1],main="Yule-Walker")
truehist(explosive[,2],main="OLS")
truehist(explosive[,3],main="ML")


#4) Estimation by indirect inference, auxiliary model is an AR(1)-process
H <- 100; n <- 100; W <- diag(1)

rhohat <- function(truedata,H,n,W){
  thetahat <- ar(truedata,aic=F,order = 1)$ar #auxiliary model with true data
  thetahat <- as.matrix(thetahat)             #as matrix so you can compute Q

  f <- function(rho){
    thetahatsim <- rep(NA,H)
    set.seed(123)
    for (h in 1:H){
      simdata <- filter(rnorm(n),rho,method="r",init=rnorm(1)) #simulated data depending on beta
      thetahatsim[h] <- ar(simdata,aic=F,order = 1)$ar          #store estimator
    }
    thetatilde <- mean(thetahatsim)
    Q <- t(thetahat - thetatilde) %*% W %*% (thetahat - thetatilde)
    return(Q)
  }
  # indirect inference estimator for rho
  rhohat <- optimize(f,lower=0.7,upper=1.5)
  return(rhohat$minimum)
}

truedata <- filter(rnorm(n),0.9,method="r",init=rnorm(1)) #true model with rho=0.8

rhohat(truedata,H=10,n=100,W=diag(1))
ar(truedata,aic=F,order = 1,method="ols")$ar #method: ols
ar(truedata,aic=F,order = 1,method="yw")$ar #method: yw
ar(truedata,aic=F,order = 1,method="mle")$ar #method: mle
\end{verbatim}
\end{solution}

\subsection{Filter models using the Kalman filter}
Consider the univariate dynamic linear model%
\begin{eqnarray*}
y_{t} &=&\theta _{t}+v_{t},\qquad v_{t}\sim N\left( 0,V\right) \\
\theta _{t} &=&\theta _{t-1}+w_{t},\qquad w_{t}\sim N(0,W) \\
\theta _{0} &\sim &N\left( m_{0},C_{0}\right)
\end{eqnarray*}%
where only $y_{t}$ is observed, but we are interested in the unobservable
state variable $\theta _{t}$. This model is sometimes called random walk
plus noise. In R, the package \texttt{dlm} provides commands to deal with
dynamic linear models. The notation in this exercise is adapted to the
\texttt{dlm} package.

\begin{enumerate}
\item Install and activate the package \texttt{dlm}.

\item Define a \texttt{dlm}-object \texttt{mod <-
dlm(FF=1,GG=1,V=9,W=1,m0=0,C0=100)}. This object represents the random walk
plus noise model with known parameters $V$ and $W$ (and $m_{0}$ and $C_{0}$).

\item Load the dataset \texttt{rwnoise.csv} and plot the variable \texttt{y}.

\item Add the Kalman filtered estimated state variable $\hat{\theta}_{t}$ to
the plot. The Kalman filtered series can very easily be computed by the
command \texttt{dlmFilter(y,mod)\$m[-1]}.

\item In general, the model parameters $V$ and $W$ are unknown. Estimate $V$
and $W$ by indirect inference. The auxiliary parameters are the $MA(1)$
parameter and the error term variance of an $ARIMA(0,1,1)$ model\footnote{%
To estimate an $ARIMA(p,d,q)$ model in R, use the command \texttt{a
<- arima(x,order=c(p,d,q))}. The coefficients can be extracted
by \texttt{a\$coef} and error term variance is \texttt{a\$sigma2}.} (hence,
the model is exactly identified).
\end{enumerate}

\begin{solution}
\textbf{Filter models }
\end{solution}

\subsection{Estimation of the Cox-Ingersoll-Ross model}

Cox, Ingersoll and Ross, \textquotedblleft A Theory of the Term Structure of
Interest Rates\textquotedblright , \emph{Econometrica} 53 (1985) 385-407,
suggest a continuous-time model for the short-term interest rate. The
stochastic process is described by the stochastic differential equation%
\begin{equation}
dX_{t}=\left( \theta _{1}-\theta _{2}X_{t}\right) dt+\theta _{3}\sqrt{X_{t}}%
dW_{t}  \label{sdecir}
\end{equation}%
where $W_{t}$ is a standard Wiener process and $X_{0}>0$.

\begin{enumerate}
\item Activate the R package \texttt{sde}. Generate and plot a single path
on the time interval $[0,200]$ of an Cox-Ingersoll-Ross process with
parameters $\theta _{1}=0.03$, $\theta _{2}=0.5$, and $\theta _{3}=0.08$ and
starting value $X_{0}=0.06$ using the command \texttt{sde.sim}. Set the
number of steps to $N=200$.

\item Continuous-time models are sometimes estimated by discretizing them in
a crude way. The discretized version of (\ref{sdecir}) is, of course,%
\begin{equation}
X_{t}=X_{t-1}+\left( \theta _{1}-\theta _{2}X_{t-1}\right) +\theta _{3}\sqrt{%
X_{t-1}}\varepsilon _{t}  \label{dissde}
\end{equation}%
with $\varepsilon _{t}\sim N(0,1)$ and starting value $X_{0}=\theta
_{1}/\theta _{2}$. Find estimators for the parameters $\theta _{1},\theta
_{2},\theta _{3}$ in (\ref{dissde}) that can be computed fast (e.g. least
squares estimators).

\item Load the dataset \texttt{cirpath.csv}. The process is not observed
continuously. The dataset only contains observations of $X_{t}$ at discrete
time points $t=1,\ldots ,200$. Estimate the parameters $\theta _{1},\theta
_{2}$, and $\theta _{3}$ by indirect inference with the auxiliary model (\ref%
{dissde}). Assume that the (unobserved) starting value is $X_{0}=\theta
_{1}/\theta _{2}$.
\end{enumerate}

\begin{solution}
\textbf{Estimation of the Cox-Ingersoll-Ross model}

Estimators:
\begin{itemize}
  \item \textbf{OLS}: We first need to transform the model variables in \eqref{dissde} to:
      \begin{align*}
        Y_t := \frac{X_t}{\sqrt{X_{t-1}}}, \qquad
        Z_t := \sqrt{X_{t-1}}, \qquad
        V_t := \frac{1}{\sqrt{X_{t-1}}}
      \end{align*}
  Then
  \begin{align*}
    Y_t = \theta_1 V_t + (1-\theta_2)Z_t + \theta_3 \varepsilon_t
  \end{align*}
  This model is linear, the parameters are constant and $\theta_3 \varepsilon_t \overset{iid}{\sim}N(0,\theta_3^2)$, the exogenous variables and the error terms are uncorrelated and there is no multicollinearity. Hence the OLS estimator is an unbiased and consistent estimator.
  \item \textbf{GMM}: As there are three parameters to estimate, at least three moment conditions are needed. For instance:
      \begin{align*}
        E(X_t - X_{t-1}-\theta_1 + \theta_2 X_{t-1}) = 0\\
        E(X_t (X_t - X_{t-1}-\theta_1 + \theta_2 X_{t-1})) = 0\\
        E\left( \left[\frac{X_t - X_{t-1}-\theta_1 + \theta_2 X_{t-1}}{\theta_3 \sqrt{X_{t-1}}}\right]^2-1\right) = 0
      \end{align*}
      since $\theta_3\sqrt{X_{t-1}}\varepsilon_t$ has expectation zero, $X_t$ and $\varepsilon_t$ are independent, and $\varepsilon_t$ has variance of one.
\end{itemize}
\begin{verbatim}
####################################################
#### Estimation of the Cox-Ingersoll-Ross modell####
####################################################
library(gmm)
#since the package sde has its own gmm command,
#save the gmm-command from the gmm package
gmm1 <- gmm
library(sde)

#1) single path of an Cox-Ingersoll-Ross process

tet1 <- 0.03;  tet2 <- 0.5; tet3 <- 0.08
simdat <- sde.sim(t0=0,T=200,X0=tet1/tet2,N=200,theta=c(tet1,tet2,tet3),model="CIR")
plot(simdat,main="Cox-Ingersoll-Ross")  # process seems to be stationary
#add the longrun mean
abline(a=(tet1/tet2), b=0, col="red")
if (2*tet1 > tet3^2)  print("stationary") else print("not stationary")
#the condition 2*theta1 > theta3^2 is fulfilled, process is stationary


#2) Estimation of auxiliary AR(1)-model: Define Functions for OLS-model and GMM-model

# Auxiliary model with OLS
AuxOLS <- function(dat,startval){
  #transform the data to get a linear model
  z <- sqrt(dat[1:(length(dat)-1)])
  v <- 1/z
  y <- dat[2:length(dat)]/sqrt(dat[1:(length(dat)-1)])
  #calculate the estimate:
  ols <- lm(y~v+z -1)
  theta1hat<-coefficients(ols)[1]
  theta2hat<-(1-coefficients(ols)[2])
  theta3hat<-sqrt(var(residuals(ols)))
  thetahat <- rbind(theta1hat,theta2hat,theta3hat)
  colnames(thetahat) <- "Discrete OLS Estimate"
  rownames(thetahat) <- c("theta1","theta2","theta3")
  return(thetahat) #returns a 3x1 matrix
}

# Auxiliary model with GMM
AuxGMM <- function(dat,startval){
  # define moment functions
  g <- function(theta, X){
    TT <- length(X)
    u <- X[-TT] -theta[1] - (1-theta[2])*X[-1]
    m1 <- u
    m2 <- u*X[-1]
    m3 <- u^2 -theta[3]^2*X[-1]
    m4 <- (u^2 -theta[3]^2*X[-1])*X[-1]
    f <- cbind(m1,m2,m3,m4)
    return(f)
  }
  # estimate using the gmm package
  estim <- gmm1(g,dat,startval)
  thetahat <- cbind(estim$coefficients)
  colnames(thetahat) <- "Discrete GMM Estimate"
  rownames(thetahat) <- c("theta1","theta2","theta3")
  return(thetahat) #returns a 3x1 matrix
}

# Compare estimates
AuxOLS(dat=simdat,startval=c(0.03,0.5,0.08)) #ols
AuxGMM(dat=simdat,startval=c(0.03,0.5,0.08)) #gmm
AuxGMM(dat=simdat,startval=c(0.01,0.1,0.01)) #start values don't matter much


#3) Indirect inference estimation

# Function that computes the Indirect Inference estimator given
#the data, steps, matrix of weights, starting values, auxiliary model and optim or constrOptim
CIRIndirect <- function(truedata,H,W,startval,GMM=F,fast=F){
  if (GMM==F) AuxMod = AuxOLS else AuxMod = AuxGMM
  thetahat <- AuxMod(dat=truedata,startval=startval) #auxiliary model with true data

  f <- function(theta){
    thetahatsim <- matrix(NA,H,3)
    set.seed(123)
    for (h in 1:H){
      theta1 <- theta[1]; theta2 <- theta[2]; theta3 <- theta[3]
      simdata <- sde.sim(t0=0,T=200,X0=theta1/theta2,N=200,theta=c(theta1,theta2,theta3),
                              model="CIR") #simulate data depending on theta
      thetahatsim[h,] <- t(AuxMod(simdata,startval)) 
      #AuxMod returns a 3x1 matrix, thus transpose it to store in the matrix
    }
    thetatilde <- colMeans(thetahatsim)
    Q <- t(thetahat - thetatilde) %*% W %*% (thetahat - thetatilde)
    return(Q)
  }

  # Find the optimal theta
  if (fast==F) {
    res <- constrOptim(startval,f,ui=rbind(c(1,0,0),c(0,-1,0),c(0,0,1)),ci=c(0,-2,0),
                                                                 method="Nelder-Mead")
  }else {
    # optim command using bounds, (fast)
    res <- optim(startval,f,lower=c(0.01,0.01,0.01),upper=c(Inf,2,Inf),method="L-BFGS-B") 
  }

  #print the output depending on which model was used
  if (GMM==F) {
    result <- rbind(res$par); rownames(result) <- "Indirect Inference (OLS)"; 
    colnames(result) <- c("theta1","theta2","theta3")
  }else {
    result <- rbind(res$par); rownames(result) <- "Indirect Inference (GMM)"; 
    colnames(result) <- c("theta1","theta2","theta3")
  }

  return(result)
}

# Estimation for dataset cirpath.csv
cirpath <- read.csv(file.choose(),header=T)
plot(cirpath$x,type="l") #seems to be stationary

#fast - using bounded optim
resultOLS <- CIRIndirect(truedata=cirpath$x,H=10,W=diag(3),startval=c(0.03,0.5,0.08),
                                                                      GMM=F,fast=T)
print(resultOLS)
resultOLS1 <- CIRIndirect(truedata=cirpath$x,H=10,W=diag(3),startval=c(0.02,0.4,0.07),
                                                                      GMM=F,fast=T)
print(resultOLS1)
resultGMM <- CIRIndirect(truedata=cirpath$x,H=10,W=diag(3),startval=c(0.03,0.5,0.08),
                                                                      GMM=T,fast=T)
print(resultGMM)
resultGMM1 <- CIRIndirect(truedata=cirpath$x,H=10,W=diag(3),startval=c(0.02,0.4,0.07),
                                                                      GMM=T,fast=T)
print(resultGMM1)

#slow - using constrOptim
resultOLSslow <- CIRIndirect(truedata=cirpath$x,H=10,W=diag(3),startval=c(0.03,0.5,0.08),
                                                                      GMM=F,fast=F) #slow
print(resultOLSslow)
resultGMMslow <- CIRIndirect(truedata=cirpath$x,H=10,W=diag(3),startval=c(0.03,0.5,0.08),
                                                                      GMM=T,fast=F) #very slow
print(resultGMMslow)


# Test the estimation procedure with the simulated dataset simdat
tet1 <- 0.03;  tet2 <- 0.5; tet3 <- 0.08 #stationary process
simdat <- sde.sim(t0=0,T=200,X0=tet1/tet2,N=200,theta=c(tet1,tet2,tet3),model="CIR")

r1 <- CIRIndirect(truedata=simdat,H=10,W=diag(3),startval=c(0.03,0.5,0.08),GMM=F,fast=T)
r2 <- CIRIndirect(truedata=simdat,H=10,W=diag(3),startval=c(0.03,0.5,0.08),GMM=F,fast=F)
r3 <- CIRIndirect(truedata=simdat,H=10,W=diag(3),startval=c(0.03,0.5,0.08),GMM=T,fast=T)
r4 <- CIRIndirect(truedata=simdat,H=10,W=diag(3),startval=c(0.03,0.5,0.08),GMM=T,fast=F)
r5 <- CIRIndirect(truedata=simdat,H=10,W=diag(3),startval=c(0.02,0.4,0.07),GMM=T,fast=T)

\end{verbatim}
\end{solution}


\subsection{Ornstein-Uhlenbeck process\label{ouprocess}}

Consider the continuous-time stochastic process described by the stochastic
differential equation%
\begin{equation}
dX_{t}=\lambda \left( \mu -X_{t}\right) dt+\sigma dW_{t}  \label{sdeou}
\end{equation}%
where $W_{t}$ is a standard Wiener process, $\lambda >0$ is a parameter of
the strength of mean-reversion, $\mu $ is the long-run mean, and $\sigma >0$
is a volatility parameter.

\begin{enumerate}\setlength{\itemsep}{-1pt}
\item Install and activate the R package \texttt{sde}. It provides commands
to simulate paths of stochastic processes described by stochastic
differential equations. Generate and plot a single path on the time interval
$[0,100]$ of an Ornstein-Uhlenbeck process with parameters $\lambda =0.9$, $%
\mu =0$, and $\sigma =1$ and starting value $X_{0}=2$ using the command
\texttt{sde.sim}. Note that the parametrization of the \texttt{sde} command
differs from (\ref{sdeou}) with $\theta _{1}=\lambda \mu $, $\theta
_{2}=\lambda $, and $\theta _{3}=\sigma $. Set the number of steps to $N=100$%
.

\item Continuous-time models are sometimes estimated by discretizing them in
a rough way. The discretized version of (\ref{sdeou}) is, of course, $X_{t}-X_{t-1}=\lambda \left( \mu -X_{t-1}\right) +\sigma \varepsilon _{t}$ with $\varepsilon _{t}\sim N(0,1)$ and starting value $X_{0}=\mu $. Rewriting gives%
\begin{equation*}
X_{t}=\lambda \mu +\left( 1-\lambda \right) X_{t-1}+u_{t}
\end{equation*}%
with $u_{t}\sim N(0,\sigma ^{2})$. This exercise shows that simply
estimating the discrete model can be severely misleading!

For this create an empty vector \texttt{Z} of length $R=1000$. Write a loop over $%
r=1,\ldots ,R$ performing the following steps for each replication.

\begin{itemize}
\item Generate a path of the Ornstein-Uhlenbeck process \texttt{x} given in
exercise 1.

\item Fit an $AR(1)$ process to the path using the command \texttt{%
ar(x,order=1,aic=F)} or, alternatively, the command \texttt{%
arima(x,order=c(1,0,0))}. Both commands estimate (\ref{dissde}), but only
\texttt{arima} reports the estimated intercept.

\item Save the $AR$ coefficient in \texttt{Z[r]}. The $AR$ coefficient is
the estimate of $1-\lambda $.

\item After the loop, plot the histogram of \texttt{Z}. Comment on the
distribution of $1-\hat{\lambda}$.
\end{itemize}

\item Load the dataset \texttt{oupath.csv}. The process is not observed
continuously. The dataset only contains observations of $X_{t}$ at discrete
time points $t=1,\ldots ,100$. Estimate the parameters $\lambda ,\mu $, and $%
\sigma $ by indirect inference with the auxiliary model (\ref{dissde}).
Assume that the (unobserved) starting value is $X_{0}=\mu $.
\end{enumerate}

\begin{solution}
\textbf{Ornstein-Uhlenbeck process}

The code might look like this
\begin{verbatim}
  #########################################################
  #### Indirect Inference - Ornstein-Uhlenbeck process ####
  #########################################################

  #1) Single path of an Ornstein-Uhlenbeck process
  install.packages("sde")
  library(sde)
  ?sde.sim #look at the example
  # Ornstein-Uhlenbeck
  drift <- expression(0.9*0 - 0.9 * x)
  sigma <- expression(1)
  X <- sde.sim(X0=2,drift=drift, sigma=sigma,N=100,T=100)
  plot(X,main="Ornstein-Uhlenbeck")
  # or specify model to generate data
  mu <- 0;  lambda <- 0.9; sigma <- 1
  X <- sde.sim(t0=0,T=100,X0=2, N=100,theta=c(lambda*mu,lambda,sigma),model="OU")
  plot(X,main="Ornstein-Uhlenbeck")

  #2) Estimation of discrete model
  library(AER)
  R <- 1000
  Z <- rep(NA,R)
  mu <- 0;  lambda <- 0.9; sigma <- 1

  for (r in 1:R){
    X <- sde.sim(t0=0,T=100,X0=2, N=100,theta=c(lambda*mu,lambda,sigma),model="OU")
    estim <- arima(X,order=c(1,0,0))
    Z[r] <- estim$coef[1]
  }

  truehist(Z) #histogram of 1-lambda, totally wrong
  truehist(1-Z) #histogram of lambda, totally wrong

  #3) Indirect inference estimation
  oupath <- read.table(file.choose(), header=T)
  truedata <- oupath$x
  plot(truedata,type="l")
  #Hint: The data generating process uses lambda=1.3; mu=9; sigma=3; T=N<-100

  OUIndirect <- function(truedata,H,W,startval){
    estim <- arima(truedata,order=c(1,0,0)) #auxiliary model with true data
    lambdahat <- 1-estim$coef[1]
    muhat <- estim$coef[2]/lambdahat
    sigmahat <- sqrt(estim$sigma2)
    thetahat <- rbind(lambdahat,muhat,sigmahat)

    f <- function(theta){
      thetahatsim <- matrix(NA,H,3)
      set.seed(123)
      for (h in 1:H){
        lambda <- theta[1]; mu <- theta[2]; sigma <- theta[3]
        #simulated data depending on theta
        simdata <- sde.sim(t0=0,T=100,X0=mu, N=100,theta=c(lambda*mu,lambda,sigma),model="OU")
        estimsim <- arima(simdata,order=c(1,0,0))         #store estimator
        lambdahatsim <- 1-estimsim$coef[1]
        muhatsim <- estimsim$coef[2]/lambdahatsim
        sigmahatsim <- estimsim$sigma2
        thetahatsim[h,1] <- lambdahatsim
        thetahatsim[h,2] <- muhatsim
        thetahatsim[h,3] <- sigmahatsim
      }
      thetatilde <- colMeans(thetahatsim)
      Q <- t(thetahat - thetatilde) %*% W %*% (thetahat - thetatilde)
      return(Q)
    }
    # optimize ove all thetas
    res <- optim(startval,f)
    return(res$par)
  }

  OUIndirect(truedata=truedata,H=10,W=diag(3),startval=c(1.3,9,3))
\end{verbatim}
\end{solution}


\subsection{Time-aggregated observations}

Consider the geometric Brownian motion described by the stochastic
differential equation%
\begin{equation*}
dX_{t}=\mu X_{t}dt+\sigma X_{t}dW_{t}
\end{equation*}%
where $W_{t}$ is a standard Wiener process, $\mu \ $is the drift parameter
and $\sigma >0$ is the volatility parameter, and the starting value is $%
X_{0}=100$. Suppose the process $X_{t}$ is not observed continuously. The
only observations are the time-aggregates%
\begin{equation}
Y_{t}=\int_{t-1}^{t}X_{t}dt  \label{inty}
\end{equation}%
for $t=1,\ldots ,T$. The $Y_{t}$ could be interpreted as average stock
prices over time intervals $[t-1,t]$, that are relevant for Asian option
pricing. This exercise explores how to estimate $\mu $ and $\sigma $ from
observations $Y_{1},\ldots ,Y_{T}$ by indirect inference.

\begin{enumerate}
\item Install and activate the R package \texttt{sde}. It provides commands
to simulate paths of Brownian motion (\texttt{BM}) and geometric Brownian
motions (\texttt{GBM}). Generate and plot a single path of the geometric
Brownian motion $X_{t}$ with $\mu =0.00025$ and $\sigma =0.015$ (these
values are more or less realistic for daily stock returns), and starting
values $X_{0}=1$, on the time interval $[0,T]$ with $T=30$. Let the number
of steps be $N=3000$, i.e. 100 steps per period.

\item The time integrals in (\ref{inty}) can be approximated by the sums%
\begin{equation*}
Y_{t}\approx \sum_{i}X_{i}\cdot \Delta
\end{equation*}%
where the sum is over all $i$ between $t-1$ and $t$, and $\Delta =T/N=0.01$
is the interval length. Create an empty matrix \texttt{Z} of dimensions $%
R\times 4$ with $R=1000.$ Write a loop over $r=1,\ldots ,R$ performing the
following steps for each replication.

\begin{itemize}
\item Generate a path of the geometric Brownian motion $X_{t}$ given in
exercise 1.

\item Calculate the four integrals $Y_{1},Y_{2},Y_{15},Y\,_{30}$ and save
them in row $r$ of the matrix \texttt{Z}.

\item After the loop, calculate the means for each column and the
variance-covariance matrix of \texttt{Z} (using the command \texttt{cov}).
\end{itemize}

\item Load the dataset \texttt{timeaggr.csv}. It contains 30 time-aggregated
observations $Y_{1},\ldots ,Y_{30}$. Estimate $\mu $ and $\sigma $ by
indirect inference. As auxiliary model use an $ARIMA(1,0,1)$ process with
intercept. Also include the error term variance of the $ARIMA$ model in your
estimation.\footnote{%
To estimate an $ARIMA(p,d,q)$ model in R, use the command \texttt{a
<- arima(x,order=c(p,d,q))}. The coefficients can be extracted
by \texttt{a\$coef} and error term variance is \texttt{a\$sigma2}.} Assume
that the starting value is $X_{0}=1$.
\end{enumerate}

\begin{solution}
\textbf{Time-aggregated observations}
\end{solution}


\newpage
\section{Bootstrap}

\subsection{Omitted variables bias does not go away}

This exercise shows that the bootstrap does not help to eliminate omitted
variable bias. Reconsider exercise \ref{omitted}.

\begin{enumerate}
\item Load the dataset \texttt{omitted.csv} and estimate the model without
the relevant variable $X_{4}$ by OLS.

\item Bootstrap the bias and standard error of the coefficients of $X_{2}$
and $X_{3}$. Set the number of bootstrap replications to $B=5000$.
\end{enumerate}

\begin{solution}
\textbf{Omitted variables bias does not go away }

The code might look like this:
\begin{verbatim}
#################################################
#### Omitted variables bias does not go away ####
#################################################
#1)
#Input data and make it accessible
omitted <- read.csv(file.choose(), sep=";", dec=",")
x1 <- omitted$x1
x2 <- omitted$x2
x3 <- omitted$x3
x4 <- omitted$x4
y <- omitted$y
n <- length(y)
#Estimate by OLS without x4
#Reminder: the true model is Y = 1 + 2X1 + 3X2 + 4X3 + 5X4
# X1 is uncorrelated, X2&X3 are correlated and X3&X4, X2&X4 are uncorrelated
mod <- lm(y~x1+x2+x3)
summary(mod) #only the uncorrelated variable x1 is estimated well

#2)
### 1.Alternative: Bootstrap the residuals
uhat <- mod$residuals
ahat <- coefficients(mod)[1]
beta1hat <- coefficients(mod)[2]
beta2hat <- coefficients(mod)[3]
beta3hat <- coefficients(mod)[4]

B <- 1000
betastar <- matrix(NA,B,2)
SEbetastar <- matrix(NA,B,2)
for(b in 1:B) {
  ustar <- sample(uhat,n,replace=TRUE)
  ystar <- ahat+beta1hat*x1+beta2hat*x2+beta3hat*x3+ustar
  bootmod <- lm(ystar~x1+x2+x3)
  betastar[b,1] <- coefficients(bootmod)[3] #coef for x2
  betastar[b,2] <- coefficients(bootmod)[4] #coef for x3
  SEbetastar[b,1] <- sqrt(vcov(bootmod)[3,3]) #SE for x2
  SEbetastar[b,2] <- sqrt(vcov(bootmod)[4,4]) #SE for x3
}

# Calculate the bias
print(colMeans(betastar)-c(beta2hat,beta3hat)) #omitted variable bias does not go away
colMeans(SEbetastar)

### 2. Alternative: Bootstrap of the observations
B <- 1000
betastar <- matrix(NA,B,2)
SEbetastar <- matrix(NA,B,2)
for(b in 1:B) {
  indices <- sample(1:n,n,replace=TRUE)
  x1star <- x1[indices]
  x2star <- x2[indices]
  x3star <- x3[indices]
  ystar <- y[indices]
  bootmod <- lm(ystar~x1star+x2star+x3star)
  betastar[b,1] <- coefficients(bootmod)[3] #coef for x2
  betastar[b,2] <- coefficients(bootmod)[4] #coef for x3
  SEbetastar[b,1] <- sqrt(vcov(bootmod)[3,3]) #SE for x2
  SEbetastar[b,2] <- sqrt(vcov(bootmod)[4,4]) #SE for x3
}

# Calculate the bias
print(colMeans(betastar)-c(beta2hat,beta3hat)) #omitted variable bias does not go away
colMeans(SEbetastar)
\end{verbatim}
\end{solution}

\subsection{Confidence intervals for the Gini index}

Install and activate the package \texttt{ineq}. It provides functions for
inequality measures and concentration measures as well as Lorenz curves.

\begin{enumerate}
\item Load the dataset \texttt{earnings.csv}. It contains earnings of 11648
individuals. Compute the Gini coefficient of earnings.

\item Bootstrap the standard error of the Gini coefficient.

\item Compute the bootstrap 0.95-confidence interval for the Gini
coefficient using the percentile method.
\end{enumerate}

\begin{solution}
\textbf{Confidence intervals for the Gini index}

The program could look like this:
\begin{verbatim}
#################################################
#### Confidence intervals for the Gini index ####
#################################################
#1)
install.packages("ineq")
library(ineq)
earnings <- read.csv(file.choose(), header=T)
x <- earnings$x
?ineq
ineq(x,type="Gini")
Ginihat <- Gini(x)
print(Ginihat)
#2)
n <- length(x)
B <- 1000
Ginistar <- rep(NA,B)

for(b in 1:B) {
  # Draw a resample
  xx <- sample(x,n,replace=TRUE)
  # Compute and save the Gini-coefficient for the resample
  Ginistar[b] <- Gini(xx)
}

# Compute the standard error
print(sd(Ginistar))

#3)
# Sort Ginistar
Ginistar <- sort(Ginistar)

print(paste("Lower limit = ",2*Ginihat-Ginistar[0.975*B]))
print(paste("Upper limit = ",2*Ginihat-Ginistar[0.025*B]))
\end{verbatim}
\end{solution}

\subsection{Confidence intervals for correlation coefficients}

The distribution of the empirical correlation coefficient
\begin{equation*}
\hat{\rho}=\frac{\sum_{i=1}^{n}\left( X_{i}-\bar{X}\right) \left( Y_{i}-\bar{%
Y}\right) }{\sqrt{\sum_{i=1}^{n}\left( X_{i}-\bar{X}\right) ^{2}}\sqrt{%
\sum_{i=1}^{n}\left( Y_{i}-\bar{Y}\right) ^{2}}}
\end{equation*}%
is rather complicated except for some special cases. Of course, being based
on moments, $\hat{\rho}$ is asymptotically normally distributed (if the
relevant moments exist). However, in small samples, confidence intervals for
$\rho $ are not trivial.

\begin{enumerate}
\item Install and activate the package \texttt{copula}. It provides
functions and commands to deal with copulas. Generate a single sample of
size $n=1000$ from $\left( X,Y\right) $ by executing
\begin{equation*}
\text{\texttt{x <- qexp(rcopula(gumbelCopula(1.3),1000)).}}
\end{equation*}%
Row $i$ of the $\left( n\times 2\right) $-matrix \texttt{x} is the pair $%
(X_{i},Y_{i})$. Plot the sample and compute the correlation coefficient.

\item Simulate the distribution of $\hat{\rho}$ for sample size $n=50$ and
show that the distribution is not normal.

\item Draw a single sample of size $n=50$. Compute the bootstrap
0.95-confidence interval for $\rho $ using the percentile method with $%
B=1000 $ bootstrap replications. Check if the true value (about 0.43) is
covered by the interval.
\end{enumerate}

\begin{solution}
\textbf{Confidence intervals for correlation coefficients }

The code might look like this:
\begin{verbatim}
###########################################################
#### Confidence intervals for correlation coefficients ####
###########################################################
#1)
install.packages("copula")
library(copula)
x <- qexp(rcopula(gumbelCopula(1.3),1000))
plot(x)
cor(x)

#2)
library(MASS)
corvec <- rep(NA,1000)
for (k in 1:1000){
  x <- qexp(rcopula(gumbelCopula(1.3),50))
  corvec[k] <- cor(x)[1,2]
}

qqnorm(corvec)
qqline(corvec)
truehist(corvec)
curve(dnorm(x,mean=mean(corvec),sd=sd(corvec)),add=T)

#3)
library(AER)
library(MASS)
n <- 50
x <- qexp(rcopula(gumbelCopula(1.3),n))
corhat <- cor(x)[1,2]
B <- 5000
corstar <- rep(NA,B)

for(b in 1:B) {
  #Draw a resample
  indices <- sample(1:n,n,replace=T)
  xx <- x[indices,]
  corstar[b] <- cor(xx)[1,2]
}
# truehist(corstar)
# curve(dnorm(x,mean=mean(corstar),sd=sd(corstar)),add=T)
# the normal distribution does not fit

corstar <- sort(corstar)

print(paste("Lower limit = ",2*corhat-corstar[0.975*B]))
print(paste("Upper limit = ",2*corhat-corstar[0.025*B]))
#poor confidence intervals!
\end{verbatim}
\end{solution}

\subsection{The t-test}

This exercise shows that the ordinary $t$-test is a special case of the
parametric bootstrap. Consider the simple linear regression model%
\begin{equation*}
y_{t}=\alpha +\beta x_{t}+u_{t},\quad u_{t}\sim N(0,\sigma ^{2})
\end{equation*}%
with $\alpha =1$, $\beta =0$ and $\sigma =2$. Load the dataset \texttt{%
ttestboot.csv}. It contains the exogenous variable $x$ and the endogenous
variable $y$. The number of observations is $n=9$. We want to test $%
H_{0}:\beta =\beta _{0}=0$ against $H_{1}:\beta \neq 0$.

\begin{enumerate}
\item Compute the ordinary OLS test statistic%
\begin{equation}
\frac{\hat{\beta}-\beta _{0}}{SE(\hat{\beta})}  \label{tstat0}
\end{equation}%
and the $p$-value of the test (both are printed by \texttt{summary} of the
\texttt{lm} object).

\item The parametric bootstrap makes use of the fact that the distribution
of $u_{t}$ is known apart from the variance $\sigma ^{2}$. Resamples can be
generated by drawing new error terms from the normal distribution $N(0,\hat{%
\sigma}^{2})$ where $\hat{\sigma}^{2}$ is an unbiased estimate of $\sigma
^{2}$. Compute the estimates $\hat{\alpha}$, $\hat{\beta}$ and%
\begin{equation*}
\hat{\sigma}^{2}=\frac{1}{7}\sum_{t=1}^{9}\hat{u}_{t}^{2}.
\end{equation*}%
Use the function \texttt{residuals} to extract the residuals from the
\texttt{lm} object, and the function \texttt{coefficients} to extract the
parameter estimates $\hat{\alpha}$ and $\hat{\beta}$.

\item Prepare an empty vector \texttt{Z} of length $R=5000$. Write a loop
over $r=1,\ldots ,R$ performing the following steps:

\begin{itemize}
\item Draw a random sample $u_{1}^{\ast },\ldots ,u_{9}^{\ast }$ from $N(0,%
\hat{\sigma}^{2})$.

\item Compute%
\begin{equation*}
y_{t}^{\ast }=\hat{\alpha}+\hat{\beta}x_{t}+u_{t}^{\ast },\quad t=1,\ldots
,9.
\end{equation*}

\item For the resample $\left( x_{1},y_{1}^{\ast }\right) ,\ldots ,\left(
x_{9},y_{9}^{\ast }\right) $, calculate the bootstrap test statistic
\begin{equation*}
\frac{\hat{\beta}_{r}^{\ast }-\hat{\beta}}{SE(\hat{\beta}_{r}^{\ast })}
\end{equation*}
and save it as \texttt{Z[r]}.
\end{itemize}

\item Plot the histogram of \texttt{Z} and add the density function of the $%
t $-distribution with 7 degrees of freedom.

\item Calculate the proportion of \texttt{abs(Z)} that is larger than the
absolute value of the original test statistic (\ref{tstat0}). Compare your
result with the $p$-value computed above.
\end{enumerate}

\begin{solution}
\textbf{The t-test}

\begin{verbatim}
#####################
#### The t-test ####
####################
ttestboot <- read.csv(file.choose())
x <- ttestboot$x
y <- ttestboot$y
n <- length(y)
#Reminder: the data is generated by a=1, b=0, sigma=2

#1)
ols <- lm(y~x)
obj <- summary(ols)
tols <- obj$coefficients[2,3]
pols <- obj$coefficients[2,4]

#2)
alphahat <- coefficients(ols)[1]
betahat <- coefficients(ols)[2]
uhat <- residuals(ols)
sigma2hat <- 1/(n-2)*sum(uhat^2)
# alternativ:
# sigma2hat <- obj$sigma^2

#3)
R <- 5000
Z <- rep(NA,R)

for(r in 1:R){
  ustar <- rnorm(n,sd=sqrt(sigma2hat))
  ystar <- alphahat + betahat*x + ustar
  olsstar <- lm(ystar~x)
  betahatstar <- coefficients(olsstar)[2]
  SEbetahatstar <- summary(olsstar)$coefficients[2,2]
  Z[r] <- (betahatstar - betahat)/(SEbetahatstar)
}

#4)
library(MASS)
library(AER)
truehist(Z)
curve(dt(x,df=7),add=T) #the approximation fits perfectly

#5)
length(Z[abs(Z)>abs(tols)])/length(abs(Z))
pols
#p-value is almost the same!
\end{verbatim}
\end{solution}

\subsection{The percentile-t-method}

In the lecture, we considered the distribution of $\hat{\theta}-\theta $ and
its bootstrap approximation $\theta ^{\ast }-\hat{\theta}$ to determine
confidence intervals. An asymptotically more efficient method is to consider
the distribution of%
\begin{equation*}
\frac{\hat{\theta}-\theta }{SE(\hat{\theta})}
\end{equation*}%
and its bootstrap approximation%
\begin{equation*}
\frac{\theta ^{\ast }-\hat{\theta}}{SE(\theta ^{\ast })}.
\end{equation*}%
Since these quantities look like $t$-statistics, this method is often called
the percentile-t-method.

\begin{enumerate}
\item Start with%
\begin{equation*}
P\left( c_{1}\leq \frac{\hat{\theta}-\theta }{SE(\hat{\theta})}\leq
c_{2}\right) =1-\alpha
\end{equation*}%
and derive an expression for the $\left( 1-\alpha \right) $-confidence
interval.

\item Start with%
\begin{equation*}
P\left( c_{1}\leq \frac{\theta ^{\ast }-\hat{\theta}}{SE(\theta ^{\ast })}%
\leq c_{2}\right) =1-\alpha
\end{equation*}%
and derive an expression for the bootstrap $\left( 1-\alpha \right) $%
-confidence interval.

\item Explain the algorithm to compute the bootstrap confidence interval.

\item Reconsider exercise \ref{mdemand}. Write a program that computes the
bootstrap $0.95$-percentile-t-confidence interval for the coefficient of the
interest rate $r_{t}$ estimated with instruments $r_{t-1}$ and $r_{t-2}$ (as
done in \ref{mdemand}.2).
\end{enumerate}

\begin{solution}
\textbf{The percentile-t-method }

The code might look like this:
\begin{verbatim}
#################################
#### The percentile-t-method ####
#################################
library(AER)
# read data
money <- read.csv(file.choose())
m <- money$m
r <- money$r
y <- money$y
TT <- length(m)

yt <- y[5:TT]
mt <- m[5:TT]; mt1 <- m[4:(TT-1)]; mt2 <- m[3:(TT-2)]
rt <- r[5:TT]; rt1 <- r[4:(TT-1)]; rt2 <- r[3:(TT-2)]

# Generalized IV estimation with confidence interval
IV <- ivreg(mt~rt+yt+mt1+mt2|rt1+rt2+yt+mt1+mt2)
summary(IV)
confint(IV,parm="rt",level=.95)

# Bootstrap 0.95-percentile-t condfidence intervals
# estimate for original data
rhat <- coefficients(IV)[2]
SErhat <- sqrt(vcov(IV)[2,2])

# Draw resamples
B <- 1000
Taustar <- rep(NA,B)
for(b in 1:B) {
  indices <- sample(1:TT,TT,replace=TRUE)
  mtstar <- mt[indices]
  rtstar <- rt[indices]
  ytstar <- yt[indices]
  mt1star <- mt1[indices]
  mt2star <- mt2[indices]
  rt1star <- rt1[indices]
  rt2star <- rt2[indices]
  bootmod <- ivreg(mtstar~rtstar+ytstar+mt1star+mt2star|rt1star+rt2star+ytstar+mt1star+mt2star)
  SErstar <- sqrt(vcov(bootmod)[2,2])
  rstar <- bootmod$coefficients[2]
  Taustar[b] <- (rstar-rhat)/SErstar
}

# Sort
Taustar <- sort(Taustar)

#Compute the interval
low <- rhat-Taustar[0.975*B]*SErhat
high <- rhat-Taustar[0.025*B]*SErhat
names(low)="2.5%"; names(high)="97.5%"
CI <- c(low,high)

print(CI) #CI using bootstrap
confint(IV,parm="rt",level=.95) #CI using GIV estimation
\end{verbatim}
\end{solution}

\subsection{Heavy tails and variance testing}

Let $X$ and $Y$ be independent random variables both having a $t$%
-distribution with 3 degrees of freedom (a plausible model for returns). Let
$\left( X_{1},Y_{1}\right) ,\ldots ,\left( X_{n},Y_{n}\right) $ be a random
sample of size $n=200$.

\begin{enumerate}
\item Use simulations to show that the $F$-test of the hypotheses%
\begin{eqnarray*}
H_{0} &:&Var(X)=Var(Y) \\
H_{1} &:&Var(X)\neq Var(Y)
\end{eqnarray*}%
rejects the null hypothesis far too often at significance level $\alpha
=0.05 $. Hint: Tests for equal variances can be performed by the R command
\texttt{var.test(x,y)\$p.value}.

\item Implement a nonparametric Wald-type bootstrap test for equality of
variances. Use the ordinary $F$-statistic as test statistic (\texttt{%
var.test(x,y)\$statistic}).

\item Implement a nonparametric LM-type bootstrap test for equality of
variances. Use the ordinary $F$-statistic as test statistic (\texttt{%
var.test(x,y)\$statistic}).

\item If your time and computer power allow, do a Monte-Carlo simulation to
show that the bootstrap tests keeps the prescribed level $\alpha $ more
closely than the ordinary $F$-test, i.e. that the proportion of rejections
of the true null hypothesis is closer to 5\% of the replications. Note,
however, that the rejection probability is still substantially too high for
both variants of the bootstrap tests.
\end{enumerate}

\begin{solution}
\begin{verbatim}
##########################################
#### Heavy Tails and Variance Testing ####
##########################################
#1)
simnum <- 1000
rej <- rep(NA,simnum)
alpha <- 0.05
n <- 200

for (k in 1:simnum){
  x <- rt(n,3)
  y <- rt(n,3)
  rej[k] <- as.numeric(var.test(x,y)$p.value<alpha)
}
mean(rej)

#2) Wald type Bootstrap
WaldBootstrap <- function(n,x,y,Fstat,mes){
  R <- 1000
  Fstar <- rep(NA,R)
  H0 <- 1 # The F-Statistic equals 1 under H0
  for(r in 1:R) {
    # Draw resample
    xx <- sample(x,n,replace=TRUE)
    yy <- sample(y,n,replace=TRUE)
    Fstar[r] <- var.test(xx,yy)$statistic/Fstat
  }

  # Sort Fstar and get confidence intervalls
  Fstar <- sort(Fstar)
  critlow <- Fstar[0.025*R]
  crithigh <- Fstar[0.975*R]
  if (mes == 1){
    if(Fstat/H0<critlow | Fstat/H0>crithigh)  print("Reject") else print("Do not reject")
  }
  if(Fstat/H0<critlow | Fstat/H0>crithigh) out<-1 else out<-0
  return(out)
}

#3) LM type Bootstrap:
LMBootstrap <- function(n,x,y,Fstat,mes){
  R <- 1000
  Fsharp <- rep(NA,R)
  H0 <- 1 # The F-Statistic equals 1 under H0
  # Standardize data to equal variances
  x2 <- x/sd(x)
  y2 <- y/sd(y)
  for(r in 1:R) {
    # Draw resample
    xx <- sample(x2,n,replace=TRUE)
    yy <- sample(y2,n,replace=TRUE)
    Fsharp[r] <- var.test(xx,yy)$statistic/H0
  }

  # Sort Tsharp and get confidence intervals
  Fsharp <- sort(Fsharp)
  critlow <- Fsharp[0.025*R]
  crithigh <- Fsharp[0.975*R]
  if (mes == 1){
    if(Fstat/H0<critlow | Fstat/H0>crithigh) print("Reject H0") else print("Do not reject H0")
  }
  if(Fstat/H0<critlow | Fstat/H0>crithigh) out<-1 else out<-0
  return(out)
}

#4)
reps <- 1000
rej <- rep(NA,reps)
rejWald <- rep(NA,reps)
rejLM <- rep(NA,reps)
n <- 200

pb <- txtProgressBar(min=0,max=reps,style=3)
for (k in 1:reps){
  x <- rt(n,3)
  y <- rt(n,3)
  Fstat <- var.test(x,y)$statistic
  rej[k] <- as.numeric(var.test(x,y)$p.value<0.05)
  rejWald[k] <- WaldBootstrap(n,x,y,Fstat,0)
  rejLM[k] <- LMBootstrap(n,x,y,Fstat,0)
  setTxtProgressBar(pb,k)
}
close(pb)
print(mean(rejWald))
print(mean(rejLM))
print(mean(rej))
\end{verbatim}
\end{solution}

\subsection{Time series}

This exercise shows in a simple setting how to bootstrap time series. In
general, this approach works well if there is a parametric time series model
based on an underlying white noise process (e.g. GARCH, ARIMA, VAR).
Consider the simple $AR(1)$ model with intercept%
\begin{equation}
\left( x_{t}-\mu \right) =\rho \left( x_{t-1}-\mu \right) +\varepsilon _{t}
\label{ar1}
\end{equation}%
with $\varepsilon _{t}\sim N(0,\sigma ^{2})$.
\begin{enumerate}\setlength{\itemsep}{0pt}
\item Load the dataset \texttt{ar1bsp.csv}. Estimate the parameters $\mu $
and $\rho $ using the \texttt{arima} command. The standard errors that are
reported are asymptotically valid.

\item Show that bootstrapping the standard errors of $\hat{\mu}$ and $\hat{%
\rho}$ does not work under the ordinary bootstrap resampling scheme, i.e.
drawing $x_{1}^{\ast },\ldots ,x_{T}^{\ast }$ from $x_{1},\ldots ,x_{T}$
with replacement.

\item Program the following time series bootstrap approach. Estimate the
model (\ref{ar1}) for the original sample $x_{1},\ldots ,x_{T}$ and save the
parameter estimates $\hat{\mu}$ and $\hat{\rho}$ and the residuals $\hat{%
\varepsilon}_{1},\ldots ,\hat{\varepsilon}_{T}$. Initialize an empty $\left(
R\times 2\right) $-matrix \texttt{Z} for, say, $R=1000$. For $r=1,\ldots ,R:$

\begin{itemize}\setlength{\itemsep}{0pt}
\item Draw a resample $\varepsilon _{1}^{\ast },\ldots ,\varepsilon
_{T}^{\ast }$ from $\hat{\varepsilon}_{1},\ldots ,\hat{\varepsilon}_{T}$.

\item Set $x_{1}^{\ast }=x_{1}$ and compute $x_{t}^{\ast }=\hat{\mu}+\hat{\rho}\left( x_{t-1}^{\ast }-\hat{\mu}\right) +\varepsilon _{t}^{\ast }$ for $t=2,\ldots ,T$.

\item Estimate (\ref{ar1}) for $x_{1}^{\ast },\ldots ,x_{T}^{\ast }$ and
save the estimates $\mu ^{\ast }$ and $\rho ^{\ast }$ in row $r$ of \texttt{Z%
}.
\end{itemize}
Compute the standard errors for both columns of \texttt{Z}
and compare them to exercise 1.
\end{enumerate}

\begin{solution}
\textbf{Time Series}

The code might look like this:
\begin{verbatim}
#####################
#### Time Series ####
#####################
#1)
ar1bsp <- read.csv(file.choose())
x <- ar1bsp$x
n <- length(x)
estimate <- arima(x,order=c(1,0,0),include.mean=TRUE)
rhohat <- estimate$coef[1]
muhat <- estimate$coef[2]

#2) iid bootstrap
R <- 1000
rhostar1 <- rep(NA,R)
mustar1 <- rep(NA,R)
for(r in 1:R)  {
  xstar1 <- sample(x,n,replace=TRUE)
  estimate1 <- arima(xstar1,order=c(1,0,0),include.mean=TRUE)
  rhostar1[r] <- estimate1$coef[1]
  mustar1[r] <- estimate1$coef[2]
}

rhostar1 <- sort(rhostar1)
mustar1 <- sort(mustar1)

print("iid-Bootstrap")
print(paste("Wahrer Wert rho = 0.9"))
print(paste("rhohat       = ",rhohat))
print(paste("Untergrenze = ",2*rhohat-rhostar1[0.975*R]))
print(paste("Obergrenze  = ",2*rhohat-rhostar1[0.025*R]))

print(paste("Wahrer Wert mu = 5"))
print(paste("muhat       = ",muhat))
print(paste("Untergrenze = ",2*muhat-mustar1[0.975*R]))
print(paste("Obergrenze  = ",2*muhat-mustar1[0.025*R]))

# Wahre Werte liegen nicht im Konfidenzintervall


#3) Zeitreihen Bootstrap
eps_hat <- estimate$residuals #original residuals
R <- 1000
rhostar2 <- rep(NA,R)
mustar2 <- rep(NA,R)

for (r in 1:R){
  indices <- sample(1:n,n,replace=TRUE)
  eps_star <- eps_hat[indices]
  xstar2 <- rep(NA,n)
  xstar2[1] <- x[1]
  for (k in 2:n){
    xstar2[k] <- muhat + rhohat*(xstar2[k-1] - muhat) + eps_star[k]
  }
  estimate2 <- arima(xstar2,order=c(1,0,0),include.mean=TRUE)
  rhostar2[r] <- estimate2$coef[1]
  mustar2[r] <- estimate2$coef[2]
}

rhostar2 <- sort(rhostar2)
mustar2 <- sort(mustar2)

print("Zeitreihen-Bootstrap")
print(paste("Wahrer Wert rho = 0.9"))
print(paste("rhohat       = ",rhohat))
print(paste("Untergrenze = ",2*rhohat-rhostar2[0.975*R]))
print(paste("Obergrenze  = ",2*rhohat-rhostar2[0.025*R]))

print(paste("Wahrer Wert mu = 5"))
print(paste("muhat       = ",muhat))
print(paste("Untergrenze = ",2*muhat-mustar2[0.975*R]))
print(paste("Obergrenze  = ",2*muhat-mustar2[0.025*R]))

# Wahre Werte liegen im Konfidenzintervall
\end{verbatim}
\end{solution}

\subsection{Bootstrap test for the Zipf index of city size distributions}

It is well known that the population size distribution of large cities can
be approximated by the Zipf distribution which is a special case of the
Pareto distribution with tail index $\alpha =1$. Suppose, $X_{1}\geq
X_{2}\geq \ldots \geq X_{n}$ is a descendingly ordered sample of city sizes.
In regional economics, the tail index $\alpha $ is often estimated from the
regression%
\begin{equation*}
\ln \left( i\right) =c-\alpha \ln X_{i}+u_{i}
\end{equation*}%
where $i$ is the rank of the city and $X_{i}$ its size, the intercept
parameter $c$ is of no interest. Since the sample is ordered, the
observations are no longer independent and the optimality properties of OLS
vanish. In particular, the ordinary $t$-test does not work correctly anymore. In this exercise, ordered samples
from the Zipf distribution (i.e. from the Pareto distribution with true tail
index $\alpha =1$) are generated by the command%
\begin{equation*}
\text{\texttt{x <- sort(exp(rexp(n)),decreasing=TRUE)}}
\end{equation*}%
where \texttt{n} is the sample size.

\begin{enumerate}\setlength{\itemsep}{0pt}
\item Simulate and plot the distribution of $\hat{\alpha}$ for sample size $%
n=20$. The regression can be performed by \texttt{obj <-
lm(log(1:n)\symbol{126}log(x))}. The estimates can then be extracted by the
function \texttt{coefficients(obj)}.

\item An important hypothesis is $H_{0}:\alpha =1$ against $H_{1}:\alpha
\neq 1$. Simulate and plot the distribution of the test statistic%
\begin{equation*}
T=\frac{\hat{\alpha}-1}{SE(\hat{\alpha})}
\end{equation*}%
and show that it is not $t_{n-2}$-distributed even though $H_{0}$ is true.

\item Explain why the simulations done in 1. and 2. can be used to find the
critical values of a parametric LM-type bootstrap test of $H_{0}:\alpha =1$.
\end{enumerate}

\begin{solution}
\textbf{Bootstrap test for the Zipf index of city size distributions}

The code might look like this:
\begin{verbatim}
  ######################################################################
  #### Bootstrap test for the Zipf index of city size distributions ####
  ######################################################################
  #1)
  library(AER)
  n <- 20
  R <- 1000
  y <- 1:n
  Z <- rep(NA,R)

  for (r in 1:R) {
    x <- sort(exp(rexp(n)),decreasing=TRUE)
    obj <- lm(log(y) ~ log(x))
    Z[r] <- coefficients(obj)[2]
  }
  truehist(Z)

  #2)
  n <- 20
  R <- 1000
  y <- 1:n
  TT <- rep(NA,R)
  for (r in 1:R) {
    x <- sort(exp(rexp(n)),decreasing=TRUE)
    obj <- lm(log(y) ~ log(x))
   TT[r] <- (coefficients(obj)[2]-1)/ sqrt(vcov(obj)[2,2])
  }
  truehist(TT)
  curve(dt(x,df=n-2),add=T)

  #3)
  # True data
  n <- 20
  alpha <- 1 #true value
  x <- sort(exp(rexp(n,rate=alpha)),decreasing=TRUE)
  y <- 1:n

  # Hypothetical value (here it is also the true value), Nullhypothesis
  alpha0 <- 1

  # Compute the test statistics for the true data
  obj <- lm(log(y)~log(x))
  alphahat <- coefficients(obj)[2]
  SEalphahat <- sqrt(vcov(obj)[2,2])
  Tstat <- (alphahat-alpha0)/SEalphahat #test statistic for true data

  # Approximate distribution through bootstrap
  B <- 1000
  Tsharp <- rep(NA,B)

  for(b in 1:B) {
    #draw a resample taking into account the nullhypothesis, alpha=alpha0=1
    #here it is also the true value
    xx <- sort(exp(rexp(n,rate=alpha0)),decreasing=TRUE)
    obj <- lm(log(y)~log(xx))
    alphasharp <- coefficients(obj)[2]
    SEalphasharp <- sqrt(vcov(obj)[2,2])
    Tsharp[b] <- (alphasharp-alpha0)/SEalphasharp
    }

  # Sort the Tsharp values
  Tsharp <- sort(Tsharp)
  truehist(Tsharp)
  critlow <- Tsharp[0.025*B]
  crithigh <- Tsharp[0.975*B]

  if(Tstat<critlow | Tstat>crithigh)
   print("Reject H0") else
   print("Don't reject H0")
\end{verbatim}

Please note that there are better ways to remedy the failure of standard OLS than using the bootstrap. In particular, extreme value theory provides many useful tools for estimation and testing.
\end{solution}


\Closesolutionfile{ans}
% HIER KANN MAN LÖSUNGEN EINBINDEN
%\newpage
%\appendix
%\section*{Solutions}
%\input{ans}

\end{document} 